<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Seung Yeon Lee" />

<meta name="date" content="2017-06-28" />

<title>Cognitive Diagnosis Model</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="site_libs/highlight/default.css"
      type="text/css" />
<script src="site_libs/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="dina_independent_files/styles.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Cognitive Diagnosis Model</h1>
<h3 class="subtitle"><em>DINA model with independent attributes</em></h3>
<h4 class="author"><em>Seung Yeon Lee</em></h4>
<h4 class="date"><em>June 28, 2017</em></h4>

</div>

<div id="TOC">
<ul>
<li><a href="#dina-model"><span class="toc-section-number">1</span> DINA Model</a><ul>
<li><a href="#overview"><span class="toc-section-number">1.1</span> Overview</a></li>
<li><a href="#modelforstan"><span class="toc-section-number">1.2</span> Model specification for <strong>Stan</strong></a></li>
<li><a href="#prediction-of-respondents-attribute-profiles"><span class="toc-section-number">1.3</span> Prediction of respondents’ attribute profiles</a></li>
<li><a href="#stan_nostructure"><span class="toc-section-number">1.4</span> <strong>Stan</strong> program with no structure for <span class="math inline">\(\nu_c\)</span></a></li>
</ul></li>
<li><a href="#stan_ind"><span class="toc-section-number">2</span> DINA with independent attributes</a><ul>
<li><a href="#stan_ind_code"><span class="toc-section-number">2.1</span> <strong>Stan</strong> program</a></li>
<li><a href="#simulation"><span class="toc-section-number">2.2</span> Simulation</a></li>
</ul></li>
<li><a href="#example-application"><span class="toc-section-number">3</span> Example application</a></li>
<li><a href="#references"><span class="toc-section-number">4</span> References</a></li>
</ul>
</div>

<div id="dina-model" class="section level1">
<h1><span class="header-section-number">1</span> DINA Model</h1>
<p>This case study uses <strong>Stan</strong> to fit the <em>deterministic inputs, noisy “and” gate</em> (DINA) model. Analysis is performed with <strong>R</strong>, making use of the <strong>rstan</strong>, which is the implementation of <strong>Stan</strong> for <strong>R</strong>. The following <strong>R</strong> code loads the necessary packages and then sets some <strong>rstan</strong> options, which causes the compiled <strong>Stan</strong> model to be saved for future use and the MCMC chains to be executed in parallel.</p>
<pre class="r"><code># Load R packages
library(rstan)
library(ggplot2)
library(knitr)
library(CDM)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())</code></pre>
<p>The case study uses <strong>R</strong> version 3.3.3, <strong>rstan</strong> version 2.15.1, <strong>ggplot2</strong> version 2.2.1, and <strong>knitr</strong> version 1.14. Also, the example data are from <strong>CDM</strong> version 5.7.16. Readers may wish to check the versions for their installed packages using the <code>packageVersion()</code> function.</p>
<div id="overview" class="section level2">
<h2><span class="header-section-number">1.1</span> Overview</h2>
<p>In educational measurement, cognitive diagnosis models (CDMs) have been used to evaluate the strengths and weaknesses in a particular content domain by identifying the presence or absence of multiple fine-grained attributes (or skills). The presence and absence of attributes are referred to as <em>“mastery”</em> and <em>“non-mastery”</em> respectively. A respondent’s knowledge is represented by a binary vector, referred to as <em>“attribute profile”</em>, to indicate which attributes have been mastered or have not.</p>
<p>The <em>deterministic inputs, noisy “and”&quot; gate</em> (DINA) model <span class="citation">(Junker and Sijtsma 2001)</span> is a popular conjunctive CDM, which assumes that a respondent must have mastered all required attributes in order to correctly respond to an item on an assessment.</p>
<p>To estimate respondents’ knowledge of attributes, we need information about which attributes are required for each item. For this, we use a Q-matrix which is an <span class="math inline">\(I \times K\)</span> matrix where <span class="math inline">\(q_{ik}\)</span>=1 if item <span class="math inline">\(i\)</span> requires attribute <span class="math inline">\(k\)</span> and 0 if not. <span class="math inline">\(I\)</span> is the number of items and <span class="math inline">\(K\)</span> is the number of attributes in the assessment.</p>
<p>A binary latent variable <span class="math inline">\(\alpha_{jk}\)</span> indicates respondent <span class="math inline">\(j\)</span>’s knowledge of attribute <span class="math inline">\(k\)</span>, where <span class="math inline">\(\alpha_{jk}=1\)</span> if respondent <span class="math inline">\(j\)</span> has mastered attribute <span class="math inline">\(k\)</span> and 0 if he or she has not. Then, an underlying attribute profile of respondent <span class="math inline">\(j\)</span>, <span class="math inline">\(\boldsymbol{\alpha_j}\)</span>, is a binary vector of length <span class="math inline">\(K\)</span> that indicates whether or not the respondent has mastered each of the <span class="math inline">\(K\)</span> attributes.</p>
<p>The deterministic element of the DINA model is a latent variable <span class="math inline">\(\xi_{ij}\)</span> that indicates whether or not respondent <span class="math inline">\(j\)</span> has mastered all attributes required for item <span class="math inline">\(i\)</span>: <span class="math display">\[
\xi_{ij}=\prod_{k=1}^{K}\alpha_{jk}^{q_{ik}}
\]</span> If respondent <span class="math inline">\(j\)</span> has mastered all attributes required for item <span class="math inline">\(i\)</span>, <span class="math inline">\(\xi_{ij}=1\)</span>; if the respondent has not mastered all of the attributes, <span class="math inline">\(\xi_{ij}=0\)</span>.</p>
<p>The model allows for slipping and guessing defined in terms of conditional probabilities of answering items correctly (<span class="math inline">\(Y_{ij}=1\)</span>) and incorrectly (<span class="math inline">\(Y_{ij}=0\)</span>) <span class="math display">\[
s_i=\mathrm{Pr}(Y_{ij}=0\, | \, \xi_{ij}=1)
\]</span> <span class="math display">\[
g_i=\mathrm{Pr}(Y_{ij}=1 \, | \, \xi_{ij}=0).
\]</span></p>
<p>The slip parameter <span class="math inline">\(s_i\)</span> is the probability that respondent <span class="math inline">\(j\)</span> responds incorrectly to item <span class="math inline">\(i\)</span> although he or she has mastered all required attributes. The guess parameter <span class="math inline">\(g_i\)</span> is the probability that respondent <span class="math inline">\(j\)</span> responds correctly to item <span class="math inline">\(i\)</span> although he or she has not mastered all the required attributes.</p>
<p>It follows that the probability <span class="math inline">\(\pi_{ij}\)</span> of a correct response of respondent <span class="math inline">\(j\)</span> to item <span class="math inline">\(i\)</span> is <span class="math display">\[
\pi_{ij}=\mathrm{Pr}(Y_{ij}=1 \, | \, \boldsymbol{\alpha_j}, s_i, g_i)=(1-s_{i})^{\xi_{ij}}g_{i}^{1-\xi_{ij}}.
\]</span></p>
</div>
<div id="modelforstan" class="section level2">
<h2><span class="header-section-number">1.2</span> Model specification for <strong>Stan</strong></h2>
<p>In <a href="#overview">Section 1.1</a>, respondents’ knowledge was defined in terms of <span class="math inline">\(\alpha_{jk}\)</span> and <span class="math inline">\(\xi_{ij}\)</span> which are discrete latent variables. However, <strong>Stan</strong> does not support sampling discrete parameters. Instead, such models that involve bounded discrete parameters can be coded by marginalizing out the discrete parameters (See Chapter 14 in Stan reference 2.15.0. for more information on latent discrete parameters).</p>
<p>The purpose of the DINA model is to estimate an attribute profile of each respondent. In the framework of latent class models, respondents are viewed as belonging to latent classes that determine the attribute profiles. In this sense, <span class="math inline">\(\alpha_{jk}\)</span> and <span class="math inline">\(\xi_{ij}\)</span> can alternatively be expressed at the level of the latent class subscripted by <span class="math inline">\(c\)</span>. Each possible attribute profile corresponds to a latent class and the corresponding attribute profiles are labeled <span class="math inline">\(\boldsymbol{\alpha_c}\)</span> with elements <span class="math inline">\(\alpha_{ck}\)</span>. The global attribute mastery indicator for respondents in latent class <span class="math inline">\(c\)</span> is defined by <span class="math display">\[
\xi_{ic}=\prod_{k=1}^{K}\alpha_{ck}^{q_{ik}}
\]</span> where <span class="math inline">\(\alpha_{ck}\)</span> represents the attribute variable for respondents in latent class <span class="math inline">\(c\)</span> that indicates whether respondents in this class have mastered attribute <span class="math inline">\(k\)</span> <span class="math inline">\((\alpha_{ck}=1)\)</span> or not <span class="math inline">\((\alpha_{ck}=0)\)</span>, and <span class="math inline">\(q_{ik}\)</span> represents the binary entry in the Q-matrix for item <span class="math inline">\(i\)</span> and attribute <span class="math inline">\(k\)</span>. Although <span class="math inline">\(\xi_{ij}\)</span> for respondent <span class="math inline">\(j\)</span> is latent, <span class="math inline">\(\xi_{ic}\)</span> is determined and known for each possible attribute profile as a type of characteristic of each latent class.</p>
<p>Then, the probability of a correct response to item <span class="math inline">\(i\)</span> for a respondent in latent class <span class="math inline">\(c\)</span> is represented as follows: <span class="math display">\[
\pi_{ic}=\mathrm{Pr}(Y_{ic}=1 \, | \, \boldsymbol{\alpha_c}, s_i, g_i)=(1-s_{i})^{\xi_{ic}}g_{i}^{1-\xi_{ic}}
\]</span> where <span class="math inline">\(Y_{ic}\)</span> is the observed response to item <span class="math inline">\(i\)</span> of a respondent in latent class <span class="math inline">\(c\)</span>.</p>
<p>The marginal probability of a respondent’s observed responses across all items becomes a finite mixture model as follows: <span class="math display">\[
\begin{aligned}
\mathrm{Pr}({Y}_j=\boldsymbol{y}_j) &amp;= \sum_{c=1}^{C}\nu_c\prod_{i=1}^I\mathrm{Pr}(Y_{ij}=y_{ij} \, | \, \boldsymbol{\alpha_c}, s_i, g_i) \\
&amp;=\sum_{c=1}^{C}\nu_c\prod_{i=1}^I\pi_{ic}^{y_{ij}}(1-\pi_{ic})^{1-y_{ij}} \\
&amp;= \sum_{c=1}^{C}\nu_c\prod_{i=1}^I{[}(1-s_{i})^{\xi_{ic}}g_{i}^{1-\xi_{ic}}{]}^{y_{ij}}{[}1-\{(1-s_{i})^{\xi_{ic}}g_{i}^{1-\xi_{ic}}\}{]}^{1-y_{ij}}
\end{aligned}
\]</span> where <span class="math inline">\(\boldsymbol{y}_j\)</span> is the vector of observed responses <span class="math inline">\(y_{ij} (i=1,...,I)\)</span>, <span class="math inline">\(\nu_c\)</span> is the probability of membership in latent class <span class="math inline">\(c\)</span>, and <span class="math inline">\(\pi_{ic}\)</span> is the probability of a correct response to item <span class="math inline">\(i\)</span> by a respondent in latent class <span class="math inline">\(c\)</span>. In <strong>Stan</strong>, such mixture distributions can be specified using the function <code>target +=</code>.</p>
<p>The probability <span class="math inline">\(\nu_c\)</span> of membership in latent class <span class="math inline">\(c\)</span> is the joint probability of the components of <span class="math inline">\(\boldsymbol{\alpha_c}=(\alpha_{c1},\alpha_{c2},...,\alpha_{cK})&#39;\)</span> and can be structured in different ways. The simplest approach is the independence model which assumes that the attributes are independent of each other. Then <span class="math inline">\(\nu_c\)</span> is simply a product of probabilities for individual attributes (See <a href="#stan_ind_code">Section 2.1</a>). However, the independence assumption is usually implausible in practice. For example, the attributes can often be viewed as specific aspects of a more broadly-defined continuous latent trait. In this case, we can model the joint distribution of <span class="math inline">\(\boldsymbol{\alpha_c}\)</span> as depending on a higher-order latent trait so that the attributes are independent conditional on the higher-order latent trait. In some situations, the attributes can have prerequisite relationships where some attributes cannot be mastered before other attributes are mastered. The attribute hierarchy assumption reduces the number of possible latent classes and <span class="math inline">\(\nu_c\)</span> can be structured in different ways.</p>
</div>
<div id="prediction-of-respondents-attribute-profiles" class="section level2">
<h2><span class="header-section-number">1.3</span> Prediction of respondents’ attribute profiles</h2>
<p>As we have seen, estimation of finite mixture models in Stan does not involve drawing realizations of the respondents’ class membership (i.e., attribute profiles) from the posterior distribution. Therefore, additional Stan code is necessary for obtaining the posterior probabilities of the respondents’ class membership.</p>
<p>We will begin by conditioning on the parameters <span class="math inline">\(\nu_c\)</span>, (<span class="math inline">\(c=1,...,C\)</span>), <span class="math inline">\(s_i\)</span> and <span class="math inline">\(g_i\)</span>, (<span class="math inline">\(i=1,...,I\)</span>). The parameter <span class="math inline">\(\nu_c\)</span> represents the ‘prior’ probability that respondent <span class="math inline">\(j\)</span> belongs to class <span class="math inline">\(c\)</span>, not conditioning on the respondent’s response vector <span class="math inline">\(\boldsymbol{y}_j\)</span>. Since classes are defined by the response vectors <span class="math inline">\(\boldsymbol{\alpha_c}\)</span>, we can write this probability as <span class="math display">\[
\mathrm{Pr}(\boldsymbol{\alpha_j}=\boldsymbol{\alpha_c})=\nu_c.
\]</span> The corresponding posterior probability of respondent <span class="math inline">\(j\)</span>’s class membership, given the response vector <span class="math inline">\(\boldsymbol{y}_j\)</span> , becomes <span class="math display">\[
\mathrm{Pr}(\boldsymbol{\alpha_j}=\boldsymbol{\alpha_c} \, | \, \boldsymbol{y}_j)=\frac{\mathrm{Pr}(\boldsymbol{\alpha_j}=\boldsymbol{\alpha_c})\mathrm{Pr}(\boldsymbol{Y}_j=\boldsymbol{y}_j \, | \, \boldsymbol{\alpha_c})}{\mathrm{Pr}(\boldsymbol{Y}_j=\boldsymbol{y}_j)}=\frac{\nu_c\prod_{i=1}^I\pi_{ic}^{y_{ij}}(1-\pi_{ic})^{1-y_{ij}}}{\sum_{c=1}^{C}\nu_c\prod_{i=1}^I\pi_{ic}^{y_{ij}}(1-\pi_{ic})^{1-y_{ij}}}.
\]</span></p>
<p>From these joint posterior probabilities of the attribute vectors, we can also derive the posterior probabilities of mastery of the individual attributes as <span class="math display">\[
\mathrm{Pr}(\alpha_{jk}=1 \, | \, \boldsymbol{y}_j)=\sum_{c=1}^{C}\mathrm{Pr}(\boldsymbol{\alpha_j}=\boldsymbol{\alpha_c} \, | \, \boldsymbol{y}_j)\times\alpha_{ck}.
\]</span></p>
<p>Instead of conditioning on the parameters <span class="math inline">\(\nu_c,s_i,g_i\)</span> to obtain <span class="math inline">\(\mathrm{Pr}(\boldsymbol{\alpha_j}=\boldsymbol{\alpha_c}|\boldsymbol{Y}_j=\boldsymbol{y}_j)\)</span>, we want to derive the posterior probabilities, averaged over the posterior distribution of the parameters. This is achieved by evaluating the expressions above for posterior draws of the parameters and averaging these over the MCMC iterations. Let the vector of all parameters be denoted <span class="math inline">\(\boldsymbol{\theta}\)</span> and let the posterior draw in iteration <span class="math inline">\(s\)</span> be denoted <span class="math inline">\(\boldsymbol{\theta}^{(s)}_{.}\)</span> Then we estimate the posterior probability, not conditioning on the parameters, as <span class="math display">\[
\frac{1}{S}\sum_{s=1}^{S}\mathrm{Pr}(\boldsymbol{\alpha_j}=\boldsymbol{\alpha_c} \, | \, \boldsymbol{y}_j,\boldsymbol{\theta}^{(s)}).
\]</span></p>
<p>In <a href="#stan_nostructure">Section 1.4</a>, we introduce the <strong>Stan</strong> program with no structure for <span class="math inline">\(\nu_c\)</span>. <a href="#stan_ind">Section 2</a> describes modification of this <strong>Stan</strong> program to specify the independence model for <span class="math inline">\(\nu_c\)</span> and presents simulation results.</p>
</div>
<div id="stan_nostructure" class="section level2">
<h2><span class="header-section-number">1.4</span> <strong>Stan</strong> program with no structure for <span class="math inline">\(\nu_c\)</span></h2>
<p>The <strong>Stan</strong> program without any structure for <span class="math inline">\(\nu_c\)</span> is given in <em>dina_nostructure.stan</em>.</p>
<p>As described in <a href="#modelforstan">Section 1.2</a>, the marginal probability of a respondent’s observed responses across all items, <span class="math inline">\(\mathrm{Pr}(\boldsymbol{Y}_j=\boldsymbol{y}_j)\)</span>, is a mixture of the <span class="math inline">\(C\)</span> mixture components which are conditional probabilities given latent class <span class="math inline">\(c\)</span> , <span class="math inline">\(\prod_{i=1}^I\mathrm{Pr}(Y_{ij}=y_{ij}\, | \,\boldsymbol{\alpha_c}, s_i, g_i)\)</span>, with mixing proportions <span class="math inline">\(\nu_c\)</span>. The mixing proportion parameter <code>nu</code> is declared to be a unit <span class="math inline">\(C\)</span>-simplex in the <code>parameters</code> block. In the <code>model</code> block, we declare a local array variable <code>ps</code> to be size <span class="math inline">\(C\)</span> and use it to accumulate the contributions from the mixture components. In the loop over respondents <span class="math inline">\(J\)</span>, for each respondent, the log of <span class="math inline">\(\nu_c\prod_{i=1}^I\mathrm{Pr}(Y_{ij}=y_{ij}\, | \,\boldsymbol{\alpha_c}, s_i, g_i)\)</span> is calculated using the expression <span class="math inline">\(\mathrm{log}\nu_c+\sum_{i=1}^{I}\{y_{ij}\mathrm{log}\pi_{ic}+(1-y_{ij})\mathrm{log}(1-\pi_{ic})\}\)</span> and added to <code>ps</code>. Then the log probability is incremented with the log sum of exponentials of the values of <code>ps</code> by <code>target += log_sum_exp(ps)</code>.</p>
<p>For slip and guess parameters, we assign beta priors. Beta(5,25) corresponds to the prior expectation that respondents who have the required attributes answer an item incorrectly by slipping 1/6 (=5/(5+25)) of the time and respondents who do not have the required attributes answer correctly to an item by guessing 1/6 of the time.</p>
<p>In the <code>generated quantities</code> block, we predict respondents’ attribute profiles. <code>prob_resp_class</code> is defined to be a <span class="math inline">\(J \times C\)</span> matrix for posterior probabilities of respondent <span class="math inline">\(j\)</span> being in latent class <span class="math inline">\(c\)</span>, <span class="math inline">\(\mathrm{Pr}(\boldsymbol{\alpha_j}=\boldsymbol{\alpha_c}\, | \,\boldsymbol{y}_j)\)</span>, and a <span class="math inline">\(J \times K\)</span> matrix <code>prob_resp_attr</code> is then defined to calculate posterior probabilities of respondent <span class="math inline">\(j\)</span> being a master of attribute <span class="math inline">\(k\)</span>, <span class="math inline">\(\mathrm{Pr}(\alpha_{jk}=1\, | \,\boldsymbol{y}_j)\)</span>.</p>
<pre><code>data {
  int&lt;lower=1&gt; I;           // # of items
  int&lt;lower=1&gt; J;           // # of respondents 
  int&lt;lower=1&gt; K;           // # of attributes
  int&lt;lower=1&gt; C;           // # of attribute profiles (latent classes) 
  matrix[J,I] y;            // response matrix
  matrix[C,K] alpha;        // attribute profile matrix
  matrix[I,C] xi;           // the global attribute mastery indicator (product of alpha^q-element)
}

parameters {
  simplex[C] nu;                        // probabilities of latent class membership
  real&lt;lower=0,upper=1&gt; slip[I];        // slip parameter   
  real&lt;lower=0,upper=1&gt; guess[I];       // guess parameter
}

transformed parameters {
  vector[C] log_nu = log(nu);
}

model {
  real ps[C];               // temp for log component densities
  matrix[I,C] pi;
  real log_items[I];
  slip ~ beta(5,25);
  guess ~ beta(5,25);
  for (c in 1:C){
    for (i in 1:I){
      pi[i,c] = (1 - slip[i])^xi[i,c] * guess[i]^(1 - xi[i,c]);
    }
  }
  for (j in 1:J){
    for (c in 1:C){
      for (i in 1:I){
        log_items[i] = y[j,i] * log(pi[i,c]) + (1 - y[j,i]) * log(1 - pi[i,c]);
      }
      ps[c] = log_nu[c] + sum(log_items);   
    }
    target += log_sum_exp(ps);
  }
}

generated quantities {
  matrix[J,C] prob_resp_class;      // posterior probabilities of respondent j being in latent class c 
  matrix[J,K] prob_resp_attr;       // posterior probabilities of respondent j being a master of attribute k 
  matrix[I,C] pi;
  real log_items[I];
  row_vector[C] prob_joint;
  real prob_attr_class[C];
  for (c in 1:C){
    for (i in 1:I){
      pi[i,c] = (1 - slip[i])^xi[i,c] * guess[i]^(1 - xi[i,c]);
    }
  }
  for (j in 1:J){
    for (c in 1:C){
      for (i in 1:I){
        log_items[i] = y[j,i] * log(pi[i,c]) + (1 - y[j,i]) * log(1 - pi[i,c]);
      }
      prob_joint[c] = nu[c] * exp(sum(log_items));
    }
    prob_resp_class[j] = prob_joint/sum(prob_joint);
  }
  for (j in 1:J){
    for (k in 1:K){
      for (c in 1:C){
        prob_attr_class[c] = prob_resp_class[j,c] * alpha[c,k];
      }     
      prob_resp_attr[j,k] = sum(prob_attr_class);
    }
  } 
}</code></pre>
</div>
</div>
<div id="stan_ind" class="section level1">
<h1><span class="header-section-number">2</span> DINA with independent attributes</h1>
<div id="stan_ind_code" class="section level2">
<h2><span class="header-section-number">2.1</span> <strong>Stan</strong> program</h2>
<p>When the attributes are independent of each other, <span class="math inline">\(\nu_c\)</span> is a function of the probabilities of mastery of each attribute, <span class="math inline">\(\eta_k=\mathrm{Pr}(\alpha_k=1)\)</span>. The probability of each attribute profile, <span class="math inline">\(\nu_c\)</span> for latent class <span class="math inline">\(c\)</span>, is then constructed by multiplying the corresponding probabilities: we multiply <span class="math inline">\(\eta_k\)</span> if attribute <span class="math inline">\(k\)</span> has been mastered and <span class="math inline">\(1-\eta_k\)</span> if not mastered, <span class="math inline">\(\nu_c=\prod_{k=1}^{K}\eta_k^{\alpha_{ck}}(1-\eta_k)^{(1-\alpha_{ck})}\)</span>. For example, if attributes <span class="math inline">\(A_1\)</span> and <span class="math inline">\(A_2\)</span> are independent and the probabilities of each attribute mastery are <span class="math inline">\(0.3\)</span> and <span class="math inline">\(0.6\)</span> respectively. Then the probability of attribute profile <span class="math inline">\((A_1,A_2)=(1,0)\)</span> is <span class="math inline">\(0.12 (= 0.3 \times (1-0.6))\)</span>.</p>
<p>The <strong>Stan</strong> program for the independence model is given in <em>dina_independent.stan</em>.</p>
<p>When the attributes are independent of each other, we consider <span class="math inline">\(C=2^K\)</span> attribute profiles. In the <code>parameters</code> block, <code>eta</code> is defined to be a row vector of length <span class="math inline">\(K\)</span> for the probabilities of mastery of each attribute. Then the <code>transformed parameters</code> block defines <code>nu</code> as a function of <code>eta</code> and <code>alpha.</code></p>
<pre><code>data {
  int&lt;lower=1&gt; I;           // # of items
  int&lt;lower=1&gt; J;           // # of respondents     
  int&lt;lower=1&gt; K;           // # of attributes
  int&lt;lower=1&gt; C;           // # of attribute profiles (latent classes) 
  matrix[J,I] y;            // response matrix
  matrix[C,K] alpha;        // attribute profile matrix
  matrix[I,C] xi;           // the global attribute mastery indicator (product of alpha^q-element)
}

parameters {
  row_vector&lt;lower=0,upper=1&gt;[K] eta;   // probabilities of each attribute mastery
  real&lt;lower=0,upper=1&gt; slip[I];        // slip parameter   
  real&lt;lower=0,upper=1&gt; guess[I];       // guess parameter
}

transformed parameters {
  simplex[C] nu;                    // probabilities of latent class membership
  vector[C] log_nu;
  for (c in 1:C){
    nu[c] = 1;
    for (k in 1:K){
      nu[c] = nu[c] * eta[k]^alpha[c,k] * (1 - eta[k])^(1 - alpha[c,k]);        
    }   
  }
  log_nu = log(nu);
}

model {
  real ps[C];               // temp for log component densities
  matrix[I,C] pi;
  real log_items[I];
  slip ~ beta(5,25);
  guess ~ beta(5,25);
  for (c in 1:C){
    for (i in 1:I){
      pi[i,c] = (1 - slip[i])^xi[i,c] * guess[i]^(1 - xi[i,c]);
    }
  }
  for (j in 1:J){
    for (c in 1:C){
      for (i in 1:I){
        log_items[i] = y[j,i] * log(pi[i,c]) + (1 - y[j,i]) * log(1 - pi[i,c]);     
      }
      ps[c] = log_nu[c] + sum(log_items);   
    }
    target += log_sum_exp(ps);
  }
}

generated quantities {
  matrix[J,C] prob_resp_class;      // posterior probabilities of respondent j being in latent class c 
  matrix[J,K] prob_resp_attr;       // posterior probabilities of respondent j being a master of attribute k 
  matrix[I,C] pi;
  real log_items[I];
  row_vector[C] prob_joint;
  real prob_attr_class[C];
  for (c in 1:C){
    for (i in 1:I){
      pi[i,c] = (1 - slip[i])^xi[i,c] * guess[i]^(1 - xi[i,c]);
    }
  }
  for (j in 1:J){
    for (c in 1:C){
      for (i in 1:I){
        log_items[i] = y[j,i] * log(pi[i,c]) + (1 - y[j,i]) * log(1 - pi[i,c]);
      }
      prob_joint[c] = nu[c] * exp(sum(log_items));
    }
    prob_resp_class[j] = prob_joint/sum(prob_joint);
  }
  for (j in 1:J){
    for (k in 1:K){
      for (c in 1:C){
        prob_attr_class[c] = prob_resp_class[j,c] * alpha[c,k];
      }     
      prob_resp_attr[j,k] = sum(prob_attr_class);
    }
  } 
}</code></pre>
</div>
<div id="simulation" class="section level2">
<h2><span class="header-section-number">2.2</span> Simulation</h2>
<p>In this simulation, we consider 20 items and 5 attributes that are independent of each other. The Q-matrix for the simulated data is as follows:</p>
<pre class="r"><code>Q &lt;- matrix(0, 20, 5)
Q[c(2, 5, 6, 7, 15, 19), 1] &lt;- 1
Q[c(4, 7, 9, 10, 11, 13, 14, 16:20), 2] &lt;- 1
Q[c(1:3, 5, 6, 13), 3] &lt;- 1
Q[c(3, 4, 8, 10, 11, 17:20), 4] &lt;- 1
Q[c(2, 4, 5, 10, 12, 19, 20), 5] &lt;- 1
rownames(Q) &lt;- paste0(&quot;Item&quot;, 1:20)
colnames(Q) &lt;- paste0(&quot;A&quot;, 1:5)
Q &lt;- as.matrix(Q)
Q</code></pre>
<pre><code>##        A1 A2 A3 A4 A5
## Item1   0  0  1  0  0
## Item2   1  0  1  0  1
## Item3   0  0  1  1  0
## Item4   0  1  0  1  1
## Item5   1  0  1  0  1
## Item6   1  0  1  0  0
## Item7   1  1  0  0  0
## Item8   0  0  0  1  0
## Item9   0  1  0  0  0
## Item10  0  1  0  1  1
## Item11  0  1  0  1  0
## Item12  0  0  0  0  1
## Item13  0  1  1  0  0
## Item14  0  1  0  0  0
## Item15  1  0  0  0  0
## Item16  0  1  0  0  0
## Item17  0  1  0  1  0
## Item18  0  1  0  1  0
## Item19  1  1  0  1  1
## Item20  0  1  0  1  1</code></pre>
<p>We consider <span class="math inline">\(32 (=2^5)\)</span> attribute profiles <span class="math inline">\(\boldsymbol{\alpha_c},~ c=1,...,32.\)</span></p>
<pre class="r"><code>alpha_patt &lt;- expand.grid(c(0, 1), c(0, 1), c(0, 1), c(0, 1), c(0, 1))
colnames(alpha_patt) &lt;- paste0(&quot;A&quot;, 1:5)
alpha_patt</code></pre>
<pre><code>##    A1 A2 A3 A4 A5
## 1   0  0  0  0  0
## 2   1  0  0  0  0
## 3   0  1  0  0  0
## 4   1  1  0  0  0
## 5   0  0  1  0  0
## 6   1  0  1  0  0
## 7   0  1  1  0  0
## 8   1  1  1  0  0
## 9   0  0  0  1  0
## 10  1  0  0  1  0
## 11  0  1  0  1  0
## 12  1  1  0  1  0
## 13  0  0  1  1  0
## 14  1  0  1  1  0
## 15  0  1  1  1  0
## 16  1  1  1  1  0
## 17  0  0  0  0  1
## 18  1  0  0  0  1
## 19  0  1  0  0  1
## 20  1  1  0  0  1
## 21  0  0  1  0  1
## 22  1  0  1  0  1
## 23  0  1  1  0  1
## 24  1  1  1  0  1
## 25  0  0  0  1  1
## 26  1  0  0  1  1
## 27  0  1  0  1  1
## 28  1  1  0  1  1
## 29  0  0  1  1  1
## 30  1  0  1  1  1
## 31  0  1  1  1  1
## 32  1  1  1  1  1</code></pre>
<p>The following code defines probabilities <span class="math inline">\(\eta_k\)</span> that respondents master each skill <span class="math inline">\(k\)</span>.</p>
<pre class="r"><code>eta &lt;- c()
eta[1] &lt;- 0.3
eta[2] &lt;- 0.6
eta[3] &lt;- 0.8
eta[4] &lt;- 0.2
eta[5] &lt;- 0.7
eta</code></pre>
<pre><code>## [1] 0.3 0.6 0.8 0.2 0.7</code></pre>
<p>We then define the probabilities <span class="math inline">\(\nu_c\)</span> for the 32 attribute profiles as follows:</p>
<pre class="r"><code>alpha_prob &lt;- rep(1, nrow(alpha_patt))
for (i in 1:nrow(alpha_patt)) {
    for (j in 1:ncol(alpha_patt)) {
        alpha_prob[i] &lt;- alpha_prob[i] * eta[j]^alpha_patt[i, j] * (1 - eta[j])^(1 - 
            alpha_patt[i, j])
    }
}
alpha_prob</code></pre>
<pre><code>##  [1] 0.01344 0.00576 0.02016 0.00864 0.05376 0.02304 0.08064 0.03456
##  [9] 0.00336 0.00144 0.00504 0.00216 0.01344 0.00576 0.02016 0.00864
## [17] 0.03136 0.01344 0.04704 0.02016 0.12544 0.05376 0.18816 0.08064
## [25] 0.00784 0.00336 0.01176 0.00504 0.03136 0.01344 0.04704 0.02016</code></pre>
<p>Slip and guess parameters <span class="math inline">\(s_i,~g_i,~i=1,...,20\)</span>, are randomly generated from a uniform distribution on 0.05 to 0.3.</p>
<pre class="r"><code># Generate slip and guess (the values were generated by using &#39;runif(20,
# 0.05, 0.3)&#39; and fixed for repeatability)
slip &lt;- c(15, 16, 9, 14, 19, 12, 22, 16, 14, 26, 12, 18, 20, 13, 9, 29, 30, 
    24, 9, 27) * 0.01
guess &lt;- c(9, 15, 12, 20, 8, 10, 17, 25, 28, 15, 7, 27, 10, 5, 25, 13, 25, 17, 
    11, 16) * 0.01
slip</code></pre>
<pre><code>##  [1] 0.15 0.16 0.09 0.14 0.19 0.12 0.22 0.16 0.14 0.26 0.12 0.18 0.20 0.13
## [15] 0.09 0.29 0.30 0.24 0.09 0.27</code></pre>
<pre class="r"><code>guess</code></pre>
<pre><code>##  [1] 0.09 0.15 0.12 0.20 0.08 0.10 0.17 0.25 0.28 0.15 0.07 0.27 0.10 0.05
## [15] 0.25 0.13 0.25 0.17 0.11 0.16</code></pre>
<p>We simulate the true attribute profiles <span class="math inline">\(\boldsymbol{\alpha_j}\)</span> of 500 respondents, then generate the probabilities of correct responses <span class="math inline">\(\pi_{ij}\)</span> and finally the sample responses <span class="math inline">\(y_{ij}\)</span> for all items <span class="math inline">\(i\)</span> and respondents <span class="math inline">\(j\)</span>.</p>
<pre class="r"><code>J &lt;- 500  # Number of respondents
I &lt;- 20  # Number of items
K &lt;- 5  # Number of attributes
C &lt;- nrow(alpha_patt)  # Number of attribute profiles

# Generate a respondent&#39;s true latent attribute profile
ind &lt;- sample(x = 1:C, size = J, replace = TRUE, prob = alpha_prob)
A &lt;- alpha_patt[ind, ]  # true attribute profiles

# Calculate an indicator whether respondents have all attributes needed for
# each item
xi_ind &lt;- matrix(0, J, I)
for (j in 1:J) {
    for (i in 1:I) {
        xi_ind[j, i] &lt;- prod(A[j, ]^Q[i, ])
    }
}

# Generate probability correct and sample responses
prob_correct &lt;- matrix(0, J, I)
y &lt;- matrix(0, J, I)
for (j in 1:J) {
    for (i in 1:I) {
        prob_correct[j, i] &lt;- ((1 - slip[i])^xi_ind[j, i]) * (guess[i]^(1 - 
            xi_ind[j, i]))
        y[j, i] &lt;- rbinom(1, 1, prob_correct[j, i])
    }
}</code></pre>
<p>We then prepare data for <strong>Stan</strong> as follows:</p>
<pre class="r"><code># The global attribute mastery indicator for respondents in latent class c
xi &lt;- matrix(0, I, C)
for (i in 1:I) {
    for (c in 1:C) {
        xi[i, c] &lt;- prod(alpha_patt[c, ]^Q[i, ])
    }
}

dina_data_ind &lt;- list(I = I, J = J, K = K, C = C, y = y, alpha = alpha_patt, 
    xi = xi)</code></pre>
<p>The simulated dataset is fit with <strong>Stan</strong> by <em>dina_independent.stan</em></p>
<pre class="r"><code># Specify initial values for the four chains
stan_inits &lt;- list(list(guess = runif(20, 0.1, 0.3), slip = runif(20, 0.1, 0.3)), 
    list(guess = runif(20, 0.2, 0.4), slip = runif(20, 0.2, 0.4)), list(guess = runif(20, 
        0.3, 0.5), slip = runif(20, 0.3, 0.5)), list(guess = runif(20, 0.4, 
        0.6), slip = runif(20, 0.4, 0.6)))

# Fit model to simulated data
dina_independent &lt;- stan(file = &quot;dina_independent.stan&quot;, data = dina_data_ind, 
    chains = 4, iter = 500, init = stan_inits)</code></pre>
<p>We specify initial values for <code>slip</code> and <code>guess</code> parameters in each of four chains. Random values from uniform distributions between 0.1 and 0.3, between 0.2 and 0.4, between 0.3 and 0.5, between 0.4 and 0.6 are used for chain 1 through chain 4, respectively, so that each chain can be initiated at different places. We specify the initial values in order to avoid the situation where a large value close to 1 is used as a starting point for <code>slip</code> or <code>guess</code>. For example, if <code>guess</code> and <code>slip</code> are close to 1, we cannot learn whether respondents have the required attributes. Thus, a chain with such large initial value is stuck near that value and fails to converge.</p>
<p>A summary of the parameter posteriors generated by <em>dina_independent.stan</em> is as follows:</p>
<pre class="r"><code># View table of parameter posteriors
print(dina_independent, pars = c(&quot;eta&quot;, &quot;guess&quot;, &quot;slip&quot;, &quot;nu&quot;))</code></pre>
<pre><code>## Inference for Stan model: dina_independent.
## 4 chains, each with iter=500; warmup=250; thin=1; 
## post-warmup draws per chain=250, total post-warmup draws=1000.
## 
##           mean se_mean   sd 2.5%  25%  50%  75% 97.5% n_eff Rhat
## eta[1]    0.33       0 0.03 0.27 0.31 0.33 0.35  0.38  1000 1.00
## eta[2]    0.60       0 0.03 0.55 0.59 0.60 0.62  0.66  1000 1.00
## eta[3]    0.78       0 0.03 0.72 0.76 0.78 0.80  0.84   830 1.00
## eta[4]    0.21       0 0.02 0.16 0.19 0.21 0.22  0.25  1000 1.00
## eta[5]    0.68       0 0.04 0.60 0.66 0.68 0.71  0.77   811 1.00
## guess[1]  0.18       0 0.05 0.08 0.14 0.18 0.22  0.29  1000 1.00
## guess[2]  0.15       0 0.02 0.12 0.14 0.15 0.16  0.19   815 1.00
## guess[3]  0.14       0 0.02 0.10 0.12 0.14 0.15  0.18  1000 1.00
## guess[4]  0.19       0 0.02 0.16 0.18 0.19 0.21  0.23  1000 1.00
## guess[5]  0.09       0 0.02 0.06 0.08 0.09 0.10  0.12  1000 1.00
## guess[6]  0.13       0 0.02 0.09 0.12 0.13 0.14  0.17   971 1.00
## guess[7]  0.22       0 0.02 0.18 0.21 0.22 0.23  0.26  1000 1.00
## guess[8]  0.27       0 0.02 0.22 0.25 0.27 0.28  0.31  1000 1.00
## guess[9]  0.22       0 0.03 0.16 0.20 0.22 0.24  0.28  1000 1.00
## guess[10] 0.16       0 0.02 0.12 0.15 0.16 0.17  0.19  1000 1.00
## guess[11] 0.07       0 0.01 0.04 0.06 0.07 0.08  0.10  1000 1.00
## guess[12] 0.18       0 0.06 0.08 0.14 0.17 0.21  0.29   879 1.00
## guess[13] 0.14       0 0.03 0.09 0.12 0.14 0.16  0.20  1000 1.00
## guess[14] 0.08       0 0.02 0.04 0.06 0.08 0.10  0.13  1000 1.00
## guess[15] 0.28       0 0.03 0.22 0.26 0.28 0.30  0.34  1000 1.00
## guess[16] 0.17       0 0.03 0.12 0.15 0.17 0.19  0.22  1000 1.00
## guess[17] 0.24       0 0.02 0.20 0.23 0.24 0.26  0.28  1000 1.00
## guess[18] 0.15       0 0.02 0.12 0.14 0.15 0.16  0.18   913 1.00
## guess[19] 0.11       0 0.01 0.08 0.10 0.11 0.12  0.14  1000 1.00
## guess[20] 0.17       0 0.02 0.14 0.16 0.17 0.18  0.21  1000 1.01
## slip[1]   0.13       0 0.02 0.09 0.12 0.13 0.14  0.17  1000 1.00
## slip[2]   0.21       0 0.04 0.13 0.18 0.21 0.24  0.30  1000 1.01
## slip[3]   0.12       0 0.04 0.05 0.09 0.12 0.14  0.20  1000 1.00
## slip[4]   0.19       0 0.05 0.10 0.15 0.19 0.22  0.29  1000 1.00
## slip[5]   0.19       0 0.04 0.12 0.16 0.19 0.22  0.29  1000 1.00
## slip[6]   0.16       0 0.04 0.09 0.13 0.16 0.18  0.24  1000 1.00
## slip[7]   0.21       0 0.05 0.13 0.18 0.21 0.24  0.30  1000 1.00
## slip[8]   0.23       0 0.04 0.15 0.19 0.22 0.26  0.32  1000 1.00
## slip[9]   0.17       0 0.02 0.12 0.15 0.16 0.18  0.21  1000 1.00
## slip[10]  0.21       0 0.05 0.11 0.17 0.21 0.24  0.32  1000 1.00
## slip[11]  0.12       0 0.04 0.05 0.09 0.12 0.14  0.21  1000 1.00
## slip[12]  0.17       0 0.03 0.11 0.15 0.17 0.19  0.24   817 1.00
## slip[13]  0.19       0 0.03 0.13 0.17 0.19 0.21  0.25  1000 1.00
## slip[14]  0.14       0 0.02 0.10 0.13 0.14 0.16  0.19   711 1.00
## slip[15]  0.12       0 0.03 0.07 0.10 0.12 0.14  0.19  1000 1.00
## slip[16]  0.30       0 0.03 0.25 0.28 0.30 0.31  0.35  1000 1.00
## slip[17]  0.35       0 0.05 0.25 0.32 0.35 0.39  0.46  1000 1.00
## slip[18]  0.20       0 0.04 0.11 0.17 0.20 0.23  0.28  1000 1.00
## slip[19]  0.17       0 0.05 0.08 0.13 0.17 0.21  0.28  1000 1.00
## slip[20]  0.21       0 0.05 0.13 0.18 0.21 0.24  0.32  1000 1.00
## nu[1]     0.01       0 0.00 0.01 0.01 0.01 0.02  0.02   901 1.00
## nu[2]     0.01       0 0.00 0.00 0.01 0.01 0.01  0.01   747 1.00
## nu[3]     0.02       0 0.00 0.01 0.02 0.02 0.02  0.03   894 1.00
## nu[4]     0.01       0 0.00 0.01 0.01 0.01 0.01  0.02   737 1.00
## nu[5]     0.05       0 0.01 0.04 0.05 0.05 0.06  0.07   926 1.00
## nu[6]     0.03       0 0.00 0.02 0.02 0.03 0.03  0.04   770 1.00
## nu[7]     0.08       0 0.01 0.06 0.07 0.08 0.09  0.11   831 1.00
## nu[8]     0.04       0 0.01 0.03 0.03 0.04 0.04  0.05   710 1.00
## nu[9]     0.00       0 0.00 0.00 0.00 0.00 0.00  0.01   876 1.00
## nu[10]    0.00       0 0.00 0.00 0.00 0.00 0.00  0.00   714 1.00
## nu[11]    0.01       0 0.00 0.00 0.00 0.01 0.01  0.01   896 1.00
## nu[12]    0.00       0 0.00 0.00 0.00 0.00 0.00  0.00   715 1.00
## nu[13]    0.01       0 0.00 0.01 0.01 0.01 0.02  0.02   996 1.00
## nu[14]    0.01       0 0.00 0.00 0.01 0.01 0.01  0.01   824 1.00
## nu[15]    0.02       0 0.00 0.01 0.02 0.02 0.02  0.03  1000 1.00
## nu[16]    0.01       0 0.00 0.01 0.01 0.01 0.01  0.01   796 1.00
## nu[17]    0.03       0 0.01 0.02 0.03 0.03 0.04  0.04  1000 1.00
## nu[18]    0.02       0 0.00 0.01 0.01 0.02 0.02  0.02   832 1.00
## nu[19]    0.05       0 0.01 0.03 0.04 0.05 0.05  0.06  1000 1.00
## nu[20]    0.02       0 0.00 0.02 0.02 0.02 0.03  0.03   855 1.00
## nu[21]    0.11       0 0.01 0.09 0.11 0.11 0.12  0.14   787 1.00
## nu[22]    0.06       0 0.01 0.04 0.05 0.06 0.06  0.07  1000 1.00
## nu[23]    0.17       0 0.02 0.14 0.16 0.17 0.18  0.21   696 1.00
## nu[24]    0.08       0 0.01 0.07 0.08 0.08 0.09  0.10  1000 1.00
## nu[25]    0.01       0 0.00 0.01 0.01 0.01 0.01  0.01   829 1.00
## nu[26]    0.00       0 0.00 0.00 0.00 0.00 0.00  0.01   762 1.00
## nu[27]    0.01       0 0.00 0.01 0.01 0.01 0.01  0.02  1000 1.00
## nu[28]    0.01       0 0.00 0.00 0.01 0.01 0.01  0.01   801 1.00
## nu[29]    0.03       0 0.00 0.02 0.03 0.03 0.03  0.04  1000 1.00
## nu[30]    0.01       0 0.00 0.01 0.01 0.01 0.02  0.02   891 1.00
## nu[31]    0.04       0 0.01 0.03 0.04 0.04 0.05  0.06  1000 1.00
## nu[32]    0.02       0 0.00 0.02 0.02 0.02 0.02  0.03  1000 1.00
## 
## Samples were drawn using NUTS(diag_e) at Tue Jun 27 22:34:32 2017.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
<p>The <strong>Stan</strong> model is evaluated in terms of its ability to recover the generating values of the parameters. The <strong>R</strong> code below prepares a plot in which the points indicate the difference between the posterior means and generating values for the parameters of main interest (<span class="math inline">\(\eta_k, g_i, s_i\)</span>). This difference is referred to as discrepancy. The lines indicate the 95% posterior intervals for the difference. Ideally, (nearly) all the 95% posterior intervals would include zero.</p>
<pre class="r"><code># Make vector of wanted parameter names
wanted_pars &lt;- c(paste0(&quot;eta[&quot;, 1:dina_data_ind$K, &quot;]&quot;), paste0(&quot;guess[&quot;, 1:dina_data_ind$I, 
    &quot;]&quot;), paste0(&quot;slip[&quot;, 1:dina_data_ind$I, &quot;]&quot;))

# Get estimated and generating values for wanted parameters
generating_values = c(eta, guess, slip)
sim_summary &lt;- as.data.frame(summary(dina_independent)[[1]])
estimated_values &lt;- sim_summary[wanted_pars, c(&quot;mean&quot;, &quot;2.5%&quot;, &quot;97.5%&quot;)]

# Assesmble a data frame to pass to ggplot()
sim_df &lt;- data.frame(parameter = factor(wanted_pars, rev(wanted_pars)), row.names = NULL)
sim_df$middle &lt;- estimated_values[, &quot;mean&quot;] - generating_values
sim_df$lower &lt;- estimated_values[, &quot;2.5%&quot;] - generating_values
sim_df$upper &lt;- estimated_values[, &quot;97.5%&quot;] - generating_values

# Plot the discrepancy
ggplot(sim_df) + aes(x = parameter, y = middle, ymin = lower, ymax = upper) + 
    scale_x_discrete() + labs(y = &quot;Discrepancy&quot;, x = NULL) + geom_abline(intercept = 0, 
    slope = 0, color = &quot;white&quot;) + geom_linerange() + geom_point(size = 2) + 
    theme(panel.grid = element_blank()) + coord_flip()</code></pre>
<div class="figure">
<img src="dina_independent_files/figure-html/sim_plot_indep-1.png" alt="Discrepancies between estimated and generating parameters for the simulation. Points indicate the difference between the posterior means and generating values for a parameter, and horizontal lines indicate 95% posterior intervals for the difference. Most intervals contain 0, indicating that **Stan** successfully recovers the true parameters." width="672" />
<p class="caption">
Discrepancies between estimated and generating parameters for the simulation. Points indicate the difference between the posterior means and generating values for a parameter, and horizontal lines indicate 95% posterior intervals for the difference. Most intervals contain 0, indicating that <strong>Stan</strong> successfully recovers the true parameters.
</p>
</div>
<p>Next, we evaluate the ability of the <strong>Stan</strong> model to predict respondents’ attribute mastery. First, for each attribute, we calculate the mean of the predicted posterior probabilities of attribute mastery for the group of respondents who have mastered the corresponding attribute in their observed profiles (Group 1) and for the group of respondents who have not mastered the corresponding attribute in their observed profiles (Group 2), respectively. Ideally, Group 1 should have greater mean probabilities than Group 2 across the attributes.</p>
<pre class="r"><code># Make a table for mean predicted probabilities of individual attribute
# mastery
table_mean &lt;- as.data.frame(matrix(0, K, 2))
rownames(table_mean) &lt;- paste0(&quot;attribute &quot;, 1:K)
colnames(table_mean) &lt;- c(&quot;Group 1&quot;, &quot;Group 2&quot;)
for (k in 1:K) {
    # Make vector of wanted parameter names
    wanted_pars &lt;- c(paste0(&quot;prob_resp_attr[&quot;, 1:dina_data_ind$J, &quot;,&quot;, k, &quot;]&quot;))
    # Get predicted posterior probabilities of each attribute mastery for all
    # respondents
    posterior_prob_attr &lt;- sim_summary[wanted_pars, c(&quot;mean&quot;)]
    dim(posterior_prob_attr)
    # Calculate mean of the probabilities for respondents who have mastered the
    # attributes and for those who do not
    table_mean[k, &quot;Group 1&quot;] &lt;- mean(posterior_prob_attr[A[, k] == 1])
    table_mean[k, &quot;Group 2&quot;] &lt;- mean(posterior_prob_attr[A[, k] == 0])
}
kable(table_mean, digits = 2, caption = &quot;Table 1: Mean of predicted posterior probabilities of attribute mastery for the group of respondents who have mastered the corresponding attribute in their observed profiles (Group 1) and for the group of respondents who have not mastered the corresponding attribute in their observed profiles (Group 2)&quot;)</code></pre>
<table>
<caption>Table 1: Mean of predicted posterior probabilities of attribute mastery for the group of respondents who have mastered the corresponding attribute in their observed profiles (Group 1) and for the group of respondents who have not mastered the corresponding attribute in their observed profiles (Group 2)</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">Group 1</th>
<th align="right">Group 2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">attribute 1</td>
<td align="right">0.79</td>
<td align="right">0.10</td>
</tr>
<tr class="even">
<td align="left">attribute 2</td>
<td align="right">0.92</td>
<td align="right">0.11</td>
</tr>
<tr class="odd">
<td align="left">attribute 3</td>
<td align="right">0.91</td>
<td align="right">0.25</td>
</tr>
<tr class="even">
<td align="left">attribute 4</td>
<td align="right">0.79</td>
<td align="right">0.06</td>
</tr>
<tr class="odd">
<td align="left">attribute 5</td>
<td align="right">0.82</td>
<td align="right">0.38</td>
</tr>
</tbody>
</table>
<p>Table 1 shows that the group of respondents who have mastered the attributes (Group 1) has greater mean predicted probabilities of attribute mastery than the group of respondents who have not mastered (Group 2) across attributes. Also, the mean probabilities look reasonable as they are greater than 0.5 for Group 1 and less than 0.5 for Group 2.</p>
<p>We further verify the quality of predictions in terms of how accurately <strong>Stan</strong> classifies respondents into mastery and non-mastery. Respondents are classified as mastery if their predicted probabilities are greater than 0.5 and non-mastery if not. The <strong>R</strong> code below calculates how many respondents in Group 1 are actually classified as mastery (True positive rate or Sensitivity) and how many respondents in Group 2 are classified as non-mastery (True negative rate or Specificity).</p>
<pre class="r"><code>classification_table &lt;- as.data.frame(matrix(0, K, 2))
rownames(classification_table) &lt;- paste0(&quot;attribute &quot;, 1:K)
colnames(classification_table) &lt;- c(&quot;Sensitivity&quot;, &quot;Specificity&quot;)
for (k in 1:K) {
    # Make vector of wanted parameter names
    wanted_pars &lt;- c(paste0(&quot;prob_resp_attr[&quot;, 1:dina_data_ind$J, &quot;,&quot;, k, &quot;]&quot;))
    # Get predicted posterior probabilities of each attribute mastery for all
    # respondents
    posterior_prob_attr &lt;- sim_summary[wanted_pars, c(&quot;mean&quot;)]
    # Calculate &#39;sensitivity&#39; and &#39;specificity&#39;
    classification_table[k, &quot;Sensitivity&quot;] &lt;- sum(round(posterior_prob_attr[A[, 
        k] == 1]))/sum(A[, k] == 1)
    classification_table[k, &quot;Specificity&quot;] &lt;- sum(1 - round(posterior_prob_attr[A[, 
        k] == 0]))/sum(A[, k] == 0)
}
kable(classification_table, digits = 2, caption = &quot;Table 2: Sensitivity and specificity&quot;)</code></pre>
<table>
<caption>Table 2: Sensitivity and specificity</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">Sensitivity</th>
<th align="right">Specificity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">attribute 1</td>
<td align="right">0.85</td>
<td align="right">0.94</td>
</tr>
<tr class="even">
<td align="left">attribute 2</td>
<td align="right">0.94</td>
<td align="right">0.93</td>
</tr>
<tr class="odd">
<td align="left">attribute 3</td>
<td align="right">0.95</td>
<td align="right">0.85</td>
</tr>
<tr class="even">
<td align="left">attribute 4</td>
<td align="right">0.82</td>
<td align="right">0.98</td>
</tr>
<tr class="odd">
<td align="left">attribute 5</td>
<td align="right">0.83</td>
<td align="right">0.78</td>
</tr>
</tbody>
</table>
<p>Table 2 presents sensitivity and specificity for each attribute. Overall, both sensitivity and specificity are quite high (greater than 0.7) suggesting that <strong>Stan</strong> reasonably predicts attribute mastery.</p>
<p>In particular, attribute 2 shows the greatest classification accuracy among the attributes. This can be partly explained by the fact that, based on the Q-matrix, there are 3 items that measure only attribute 2 (item 9, 14, 16) while the other attributes have only 1 item that measure these attributes exclusively (item 15 for attribute 1; item 1 for attribute 3; item 8 for attribute 4; item 12 for attribute 5). Including items that measure multiple attributes, 12 items require attribute 2 whereas only 6 require attributes 1 and attribute 3, 9 items require attribute 4 and 7 require attribute 5. Thus, <span class="math inline">\(\boldsymbol{y}_j\)</span> appears to be particularly informative about attribute 2 compared with the other attributes.</p>
<p>We can also directly estimate the probabilities of each attribute profile without specifying the structure for <span class="math inline">\(\nu_c\)</span> by using <em>dina_nostructure.stan</em> described in <a href="#stan_nostructure">Section 1.4</a>.</p>
<pre class="r"><code># Fit model to simulated data
dina_independent_nostructure &lt;- stan(file = &quot;dina_nostructure.stan&quot;, data = dina_data_ind, 
    chains = 4, iter = 500, init = stan_inits)</code></pre>
<p>The results from <em>dina_nostructure.stan</em> can be summarized as follows:</p>
<pre class="r"><code># View table of parameter posteriors
print(dina_independent_nostructure, pars = c(&quot;guess&quot;, &quot;slip&quot;, &quot;nu&quot;))</code></pre>
<pre><code>## Inference for Stan model: dina_nostructure.
## 4 chains, each with iter=500; warmup=250; thin=1; 
## post-warmup draws per chain=250, total post-warmup draws=1000.
## 
##           mean se_mean   sd 2.5%  25%  50%  75% 97.5% n_eff Rhat
## guess[1]  0.20       0 0.06 0.10 0.16 0.19 0.24  0.31  1000 1.00
## guess[2]  0.15       0 0.02 0.12 0.14 0.15 0.16  0.19  1000 1.00
## guess[3]  0.14       0 0.02 0.10 0.12 0.14 0.15  0.18  1000 1.00
## guess[4]  0.20       0 0.02 0.16 0.18 0.20 0.21  0.23  1000 1.00
## guess[5]  0.09       0 0.02 0.06 0.08 0.09 0.10  0.12  1000 1.00
## guess[6]  0.14       0 0.02 0.09 0.12 0.14 0.15  0.18  1000 1.00
## guess[7]  0.22       0 0.02 0.18 0.21 0.22 0.24  0.27  1000 1.00
## guess[8]  0.26       0 0.02 0.22 0.24 0.26 0.28  0.31  1000 1.00
## guess[9]  0.22       0 0.03 0.16 0.20 0.22 0.24  0.29  1000 1.00
## guess[10] 0.15       0 0.02 0.13 0.14 0.16 0.16  0.19  1000 1.00
## guess[11] 0.07       0 0.01 0.04 0.06 0.07 0.08  0.09  1000 1.00
## guess[12] 0.22       0 0.07 0.10 0.18 0.22 0.27  0.35   754 1.00
## guess[13] 0.15       0 0.03 0.10 0.13 0.15 0.16  0.20  1000 1.00
## guess[14] 0.08       0 0.02 0.04 0.07 0.08 0.10  0.14  1000 1.00
## guess[15] 0.27       0 0.03 0.21 0.25 0.27 0.29  0.33  1000 1.00
## guess[16] 0.17       0 0.03 0.12 0.15 0.17 0.19  0.23  1000 1.00
## guess[17] 0.24       0 0.02 0.20 0.23 0.24 0.25  0.28  1000 1.00
## guess[18] 0.15       0 0.02 0.11 0.14 0.15 0.16  0.18  1000 1.00
## guess[19] 0.10       0 0.01 0.08 0.09 0.10 0.11  0.13  1000 1.00
## guess[20] 0.17       0 0.02 0.14 0.16 0.17 0.18  0.21  1000 1.00
## slip[1]   0.12       0 0.02 0.09 0.11 0.12 0.14  0.17  1000 1.00
## slip[2]   0.21       0 0.04 0.13 0.18 0.21 0.24  0.30  1000 1.00
## slip[3]   0.12       0 0.04 0.05 0.09 0.12 0.15  0.21  1000 1.00
## slip[4]   0.19       0 0.05 0.10 0.16 0.19 0.23  0.29  1000 1.00
## slip[5]   0.20       0 0.04 0.12 0.16 0.19 0.22  0.29  1000 1.00
## slip[6]   0.16       0 0.04 0.09 0.13 0.16 0.19  0.25  1000 1.00
## slip[7]   0.23       0 0.05 0.14 0.19 0.22 0.26  0.32  1000 1.00
## slip[8]   0.24       0 0.05 0.15 0.20 0.24 0.27  0.34  1000 1.00
## slip[9]   0.16       0 0.02 0.12 0.15 0.16 0.18  0.21  1000 1.00
## slip[10]  0.20       0 0.06 0.11 0.16 0.20 0.24  0.32  1000 1.00
## slip[11]  0.12       0 0.04 0.06 0.09 0.12 0.15  0.21  1000 1.00
## slip[12]  0.17       0 0.03 0.10 0.14 0.17 0.19  0.24   841 1.00
## slip[13]  0.19       0 0.03 0.14 0.17 0.19 0.22  0.25  1000 1.00
## slip[14]  0.14       0 0.02 0.09 0.12 0.14 0.16  0.19  1000 1.00
## slip[15]  0.13       0 0.03 0.07 0.11 0.13 0.15  0.19  1000 1.01
## slip[16]  0.29       0 0.02 0.25 0.28 0.29 0.31  0.34  1000 1.00
## slip[17]  0.35       0 0.05 0.26 0.32 0.35 0.39  0.44  1000 1.00
## slip[18]  0.21       0 0.04 0.13 0.18 0.21 0.24  0.30  1000 1.00
## slip[19]  0.18       0 0.06 0.08 0.14 0.18 0.22  0.30  1000 1.00
## slip[20]  0.21       0 0.05 0.11 0.18 0.21 0.24  0.32  1000 1.00
## nu[1]     0.03       0 0.02 0.00 0.01 0.02 0.04  0.07  1000 1.00
## nu[2]     0.02       0 0.01 0.00 0.01 0.01 0.02  0.04  1000 1.00
## nu[3]     0.03       0 0.01 0.01 0.02 0.03 0.04  0.06  1000 1.00
## nu[4]     0.01       0 0.01 0.00 0.01 0.01 0.02  0.04  1000 1.00
## nu[5]     0.04       0 0.02 0.00 0.02 0.04 0.05  0.08   653 1.00
## nu[6]     0.02       0 0.01 0.00 0.01 0.02 0.03  0.04  1000 1.00
## nu[7]     0.09       0 0.03 0.04 0.07 0.09 0.11  0.15   877 1.00
## nu[8]     0.03       0 0.01 0.01 0.02 0.03 0.04  0.06  1000 1.00
## nu[9]     0.01       0 0.01 0.00 0.00 0.01 0.01  0.03  1000 1.01
## nu[10]    0.01       0 0.01 0.00 0.00 0.01 0.01  0.03  1000 1.00
## nu[11]    0.01       0 0.01 0.00 0.00 0.01 0.01  0.02  1000 1.00
## nu[12]    0.00       0 0.00 0.00 0.00 0.00 0.00  0.01  1000 1.00
## nu[13]    0.01       0 0.01 0.00 0.00 0.01 0.01  0.03  1000 1.00
## nu[14]    0.01       0 0.01 0.00 0.00 0.01 0.01  0.02  1000 1.00
## nu[15]    0.02       0 0.01 0.01 0.02 0.02 0.03  0.04  1000 1.00
## nu[16]    0.02       0 0.01 0.00 0.01 0.01 0.02  0.03  1000 1.00
## nu[17]    0.03       0 0.02 0.00 0.01 0.03 0.04  0.07   696 1.00
## nu[18]    0.01       0 0.01 0.00 0.01 0.01 0.02  0.04  1000 1.00
## nu[19]    0.02       0 0.01 0.00 0.01 0.02 0.03  0.06  1000 1.00
## nu[20]    0.02       0 0.01 0.00 0.01 0.02 0.03  0.05  1000 1.00
## nu[21]    0.11       0 0.03 0.06 0.09 0.11 0.13  0.17   816 1.00
## nu[22]    0.06       0 0.01 0.03 0.05 0.05 0.06  0.08  1000 1.00
## nu[23]    0.16       0 0.03 0.11 0.14 0.16 0.18  0.23   749 1.00
## nu[24]    0.07       0 0.01 0.05 0.06 0.07 0.08  0.10  1000 1.00
## nu[25]    0.01       0 0.01 0.00 0.00 0.01 0.02  0.04  1000 1.00
## nu[26]    0.01       0 0.01 0.00 0.00 0.00 0.01  0.02  1000 1.00
## nu[27]    0.01       0 0.00 0.00 0.00 0.01 0.01  0.02  1000 1.00
## nu[28]    0.02       0 0.01 0.01 0.01 0.02 0.02  0.03  1000 1.00
## nu[29]    0.03       0 0.01 0.01 0.02 0.02 0.03  0.05  1000 1.00
## nu[30]    0.02       0 0.01 0.01 0.01 0.02 0.03  0.04  1000 1.00
## nu[31]    0.04       0 0.01 0.02 0.03 0.04 0.04  0.06  1000 1.00
## nu[32]    0.03       0 0.01 0.02 0.02 0.03 0.03  0.04  1000 1.00
## 
## Samples were drawn using NUTS(diag_e) at Tue Jun 27 23:06:59 2017.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
</div>
</div>
<div id="example-application" class="section level1">
<h1><span class="header-section-number">3</span> Example application</h1>
<p>The example data are from <span class="citation">Tatsuoka (1984)</span>’s fraction subtraction data. The original data set is comprised of responses to 20 fraction subtraction test items that measure 8 attributes: (1) Convert a whole number to a fraction, (2) Separate a whole number from a fraction, (3) Simplify before subtracting, (4) Find a common denominator, (5) Borrow from whole number part, (6) Column borrow to subtract the second numerator from the first, (7) Subtract numerators, and (8) Reduce answers to simplest form.</p>
<p>We use a subset of the data that includes 536 middle school students’ responses to 15 of the items. The items are associated with only 5 attributes and the Q-matrix was defined in <span class="citation">de la Torre (2009)</span>. The Q-matrix and response data are available in the <strong>CDM</strong> package.</p>
<pre class="r"><code>Q &lt;- data.fraction1$q.matrix
y &lt;- data.fraction1$data

# Create possible attribute patterns
alpha_patt &lt;- expand.grid(c(0, 1), c(0, 1), c(0, 1), c(0, 1), c(0, 1))
colnames(alpha_patt) &lt;- paste0(&quot;A&quot;, 1:5)
alpha_patt</code></pre>
<pre><code>   A1 A2 A3 A4 A5
1   0  0  0  0  0
2   1  0  0  0  0
3   0  1  0  0  0
4   1  1  0  0  0
5   0  0  1  0  0
6   1  0  1  0  0
7   0  1  1  0  0
8   1  1  1  0  0
9   0  0  0  1  0
10  1  0  0  1  0
11  0  1  0  1  0
12  1  1  0  1  0
13  0  0  1  1  0
14  1  0  1  1  0
15  0  1  1  1  0
16  1  1  1  1  0
17  0  0  0  0  1
18  1  0  0  0  1
19  0  1  0  0  1
20  1  1  0  0  1
21  0  0  1  0  1
22  1  0  1  0  1
23  0  1  1  0  1
24  1  1  1  0  1
25  0  0  0  1  1
26  1  0  0  1  1
27  0  1  0  1  1
28  1  1  0  1  1
29  0  0  1  1  1
30  1  0  1  1  1
31  0  1  1  1  1
32  1  1  1  1  1</code></pre>
<pre class="r"><code># Assemble data list for Stan
I = ncol(y)
J = nrow(y)
K = ncol(Q)
C = nrow(alpha_patt)

xi &lt;- matrix(0, I, C)
for (i in 1:I) {
    for (c in 1:C) {
        xi[i, c] &lt;- prod(alpha_patt[c, ]^Q[i, ])
    }
}

dina_data_fraction &lt;- list(I = I, J = J, K = K, C = C, y = y, alpha = alpha_patt, 
    xi = xi)</code></pre>
<p>The data are now formatted into a list and fit with <strong>Stan</strong>. Here, we use the <strong>Stan</strong> code <em>dina_nostructure.stan</em> for the purpose of comparison with the maximum likelihood estimates from the <strong>CDM</strong> package later, since the independence model is not available in the package.</p>
<pre class="r"><code># Specify initial values for the four chains
stan_inits &lt;- list(list(guess = runif(15, 0.1, 0.3), slip = runif(15, 0.1, 0.3)), 
    list(guess = runif(15, 0.2, 0.4), slip = runif(15, 0.2, 0.4)), list(guess = runif(15, 
        0.3, 0.5), slip = runif(15, 0.3, 0.5)), list(guess = runif(15, 0.4, 
        0.6), slip = runif(15, 0.4, 0.6)))

# Run Stan model
dina_fraction &lt;- stan(file = &quot;dina_nostructure.stan&quot;, data = dina_data_fraction, 
    chains = 4, iter = 500, init = stan_inits)</code></pre>
<p>A summary of the parameter posteriors is as follows:</p>
<pre class="r"><code># View table of parameter posteriors
print(dina_fraction, pars = c(&quot;guess&quot;, &quot;slip&quot;, &quot;nu&quot;))</code></pre>
<pre><code>## Inference for Stan model: dina_nostructure.
## 4 chains, each with iter=500; warmup=250; thin=1; 
## post-warmup draws per chain=250, total post-warmup draws=1000.
## 
##           mean se_mean   sd 2.5%  25%  50%  75% 97.5% n_eff Rhat
## guess[1]  0.04       0 0.02 0.01 0.03 0.04 0.05  0.08  1000 1.00
## guess[2]  0.21       0 0.02 0.17 0.19 0.21 0.22  0.26  1000 1.00
## guess[3]  0.15       0 0.04 0.08 0.12 0.15 0.18  0.24   814 1.00
## guess[4]  0.13       0 0.02 0.09 0.12 0.13 0.14  0.17  1000 1.00
## guess[5]  0.18       0 0.05 0.09 0.15 0.18 0.22  0.29  1000 1.00
## guess[6]  0.05       0 0.01 0.03 0.04 0.05 0.05  0.07  1000 1.00
## guess[7]  0.08       0 0.02 0.05 0.07 0.08 0.09  0.12  1000 1.00
## guess[8]  0.17       0 0.04 0.10 0.14 0.16 0.19  0.25  1000 1.00
## guess[9]  0.12       0 0.03 0.07 0.10 0.12 0.14  0.18  1000 1.00
## guess[10] 0.17       0 0.02 0.13 0.16 0.17 0.19  0.22  1000 1.00
## guess[11] 0.13       0 0.03 0.07 0.11 0.12 0.15  0.19  1000 1.00
## guess[12] 0.05       0 0.01 0.03 0.04 0.05 0.06  0.08  1000 1.00
## guess[13] 0.14       0 0.02 0.10 0.12 0.14 0.15  0.18  1000 1.00
## guess[14] 0.04       0 0.01 0.02 0.03 0.04 0.04  0.06  1000 1.00
## guess[15] 0.03       0 0.01 0.01 0.02 0.03 0.03  0.05  1000 1.00
## slip[1]   0.27       0 0.02 0.23 0.25 0.27 0.28  0.31  1000 1.00
## slip[2]   0.12       0 0.02 0.09 0.11 0.12 0.14  0.17  1000 1.00
## slip[3]   0.05       0 0.01 0.03 0.04 0.04 0.05  0.07  1000 1.00
## slip[4]   0.13       0 0.03 0.08 0.11 0.13 0.15  0.19  1000 1.00
## slip[5]   0.24       0 0.02 0.20 0.23 0.24 0.25  0.28  1000 1.00
## slip[6]   0.22       0 0.02 0.17 0.20 0.21 0.23  0.27  1000 1.00
## slip[7]   0.08       0 0.02 0.05 0.07 0.08 0.09  0.12  1000 1.00
## slip[8]   0.06       0 0.01 0.03 0.05 0.06 0.07  0.09  1000 1.00
## slip[9]   0.07       0 0.01 0.05 0.06 0.07 0.08  0.09  1000 1.00
## slip[10]  0.08       0 0.02 0.05 0.07 0.08 0.10  0.13  1000 1.00
## slip[11]  0.10       0 0.02 0.07 0.09 0.10 0.11  0.14   970 1.00
## slip[12]  0.14       0 0.02 0.10 0.12 0.14 0.15  0.18  1000 1.00
## slip[13]  0.16       0 0.02 0.11 0.14 0.15 0.17  0.20  1000 1.00
## slip[14]  0.19       0 0.03 0.14 0.17 0.19 0.21  0.25  1000 1.00
## slip[15]  0.18       0 0.02 0.13 0.16 0.18 0.19  0.23  1000 1.00
## nu[1]     0.02       0 0.02 0.00 0.01 0.01 0.03  0.06  1000 1.00
## nu[2]     0.01       0 0.01 0.00 0.00 0.01 0.01  0.03  1000 1.00
## nu[3]     0.02       0 0.02 0.00 0.01 0.01 0.02  0.06  1000 1.00
## nu[4]     0.01       0 0.01 0.00 0.00 0.01 0.01  0.03  1000 1.00
## nu[5]     0.01       0 0.01 0.00 0.00 0.01 0.01  0.03  1000 1.00
## nu[6]     0.02       0 0.01 0.00 0.01 0.02 0.03  0.05   653 1.00
## nu[7]     0.01       0 0.01 0.00 0.00 0.01 0.01  0.03  1000 1.00
## nu[8]     0.11       0 0.06 0.01 0.06 0.11 0.17  0.23   553 1.00
## nu[9]     0.02       0 0.02 0.00 0.01 0.01 0.03  0.06  1000 1.00
## nu[10]    0.01       0 0.01 0.00 0.00 0.01 0.01  0.03  1000 1.01
## nu[11]    0.02       0 0.02 0.00 0.01 0.01 0.02  0.06  1000 1.00
## nu[12]    0.01       0 0.01 0.00 0.00 0.01 0.01  0.02  1000 1.00
## nu[13]    0.01       0 0.01 0.00 0.00 0.01 0.01  0.04  1000 1.00
## nu[14]    0.01       0 0.00 0.00 0.00 0.01 0.01  0.02  1000 1.00
## nu[15]    0.01       0 0.01 0.00 0.00 0.01 0.01  0.04  1000 1.00
## nu[16]    0.10       0 0.02 0.07 0.09 0.10 0.11  0.13  1000 1.00
## nu[17]    0.02       0 0.02 0.00 0.01 0.01 0.03  0.06  1000 1.00
## nu[18]    0.01       0 0.01 0.00 0.00 0.01 0.01  0.03  1000 1.00
## nu[19]    0.02       0 0.02 0.00 0.01 0.01 0.02  0.06  1000 1.00
## nu[20]    0.01       0 0.01 0.00 0.00 0.01 0.01  0.03  1000 1.00
## nu[21]    0.01       0 0.01 0.00 0.00 0.01 0.01  0.04  1000 1.00
## nu[22]    0.02       0 0.01 0.00 0.01 0.02 0.03  0.05   581 1.00
## nu[23]    0.01       0 0.01 0.00 0.00 0.01 0.02  0.04  1000 1.00
## nu[24]    0.10       0 0.06 0.01 0.05 0.10 0.16  0.22   599 1.00
## nu[25]    0.02       0 0.02 0.00 0.01 0.01 0.03  0.06  1000 1.00
## nu[26]    0.01       0 0.01 0.00 0.00 0.01 0.01  0.03  1000 1.00
## nu[27]    0.02       0 0.02 0.00 0.01 0.01 0.03  0.06  1000 1.00
## nu[28]    0.01       0 0.01 0.00 0.00 0.01 0.01  0.03  1000 1.00
## nu[29]    0.01       0 0.01 0.00 0.00 0.01 0.01  0.03  1000 1.00
## nu[30]    0.00       0 0.00 0.00 0.00 0.00 0.01  0.01  1000 1.00
## nu[31]    0.01       0 0.01 0.00 0.00 0.01 0.01  0.03  1000 1.00
## nu[32]    0.35       0 0.02 0.31 0.33 0.35 0.36  0.39  1000 1.00
## 
## Samples were drawn using NUTS(diag_e) at Wed Jun 28 00:51:11 2017.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
<p>We can compare the <strong>Stan</strong> estimates with maximum likelihood estimates obtained by the <strong>CDM</strong> package. The <strong>R</strong> code below calculates ML point estimates by using the <code>din()</code> function of the <strong>CDM</strong> package and prepares a scatter plot of the posterior means versus the ML estimates for <span class="math inline">\(g_i, s_i\)</span> and <span class="math inline">\(\nu_c\)</span> (<span class="math inline">\(i=1,...,15, ~ c=1,...,32).\)</span></p>
<pre class="r"><code># Fit the DINA model
result_dina_cdm &lt;- din(y, Q, rule = &quot;DINA&quot;)</code></pre>
<pre class="r"><code># Extract the 32 attribute profile patterns
attr_prof_mle &lt;- rownames(result_dina_cdm$attribute.patt)
# Sort the estimated probabilities of attribute profiles (latent class) by
# the attribute profile pattern
class_prob_mle &lt;- result_dina_cdm$attribute.patt[order(attr_prof_mle), ]

# Get MLEs of guess, slip and probabilities of attribute profiles (nu)
mle_pars &lt;- as.vector(rbind(as.matrix(result_dina_cdm$guess[1]), as.matrix(result_dina_cdm$slip[1]), 
    as.matrix(class_prob_mle$class.prob)))</code></pre>
<pre class="r"><code># Create a vector of the 32 attribute profile patterns used for Stan
attr_prof_stan &lt;- do.call(paste0, alpha_patt[1:5])
# Assign ID for each attribute profile (In the Stan estimates, nu[1]
# indicates the estimated probability of having attribute profile
# (0,0,0,0,0))
attr_prof_stan &lt;- cbind(attr_prof_stan, 1:32)
# Sort the assigned ID by the attribute profile pattern
attr_prof_stan &lt;- attr_prof_stan[order(attr_prof_stan[, 1]), -1]

# Make vector of wanted parameter names
wanted_pars &lt;- c(paste0(&quot;guess[&quot;, 1:dina_data_fraction$I, &quot;]&quot;), paste0(&quot;slip[&quot;, 
    1:dina_data_fraction$I, &quot;]&quot;), paste0(&quot;nu[&quot;, attr_prof_stan, &quot;]&quot;))

# Get posterior means
ex_summary &lt;- as.data.frame(summary(dina_fraction)[[1]])
posterior_means &lt;- ex_summary[wanted_pars, c(&quot;mean&quot;)]

# Create a data frame that combines posterior means and mle, and generate a
# scatter plot with 45-degree line
estimates &lt;- data.frame(post.means = posterior_means, mle = mle_pars)
estimates$pars &lt;- c(rep(&quot;guess&quot;, dina_data_fraction$I), rep(&quot;slip&quot;, dina_data_fraction$I), 
    rep(&quot;nu&quot;, 2^K))
ggplot(data = estimates, aes(x = mle, y = post.means, shape = pars)) + geom_point() + 
    geom_abline(intercept = 0, slope = 1, colour = &quot;gray&quot;) + labs(x = &quot;MLE&quot;, 
    y = &quot;Posterior means&quot;) + scale_shape_discrete(name = &quot;Parameters&quot;)</code></pre>
<div class="figure">
<img src="dina_independent_files/figure-html/plot_ex-1.png" alt="Scatter plot of the **Stan** estimates vs. ML estimates from the **CDM** package for the subset of fraction subtraction data. Most points lie near the 45-degree line, indicating that the **Stan** estimates are similar to the ML estimates. The `guess` estimate of Item 5 differs between Stan (0.18) and CDM estimate (0.27). The reason could be that the Stan estimate has more shrunk towards the prior mean of 0.16 due to the imprecision of the ML estimate for the item; it has the largest standard error (0.04) for `guess` among all items" width="672" />
<p class="caption">
Scatter plot of the <strong>Stan</strong> estimates vs. ML estimates from the <strong>CDM</strong> package for the subset of fraction subtraction data. Most points lie near the 45-degree line, indicating that the <strong>Stan</strong> estimates are similar to the ML estimates. The <code>guess</code> estimate of Item 5 differs between Stan (0.18) and CDM estimate (0.27). The reason could be that the Stan estimate has more shrunk towards the prior mean of 0.16 due to the imprecision of the ML estimate for the item; it has the largest standard error (0.04) for <code>guess</code> among all items
</p>
</div>
</div>
<div id="references" class="section level1">
<h1><span class="header-section-number">4</span> References</h1>
<!-- This comment causes section to be numbered -->
<div id="refs" class="references">
<div id="ref-de2009dina">
<p>de la Torre, Jimmy. 2009. “DINA Model and Parameter Estimation: A Didactic.” <em>Journal of Educational and Behavioral Statistics</em> 34 (1): 115–30.</p>
</div>
<div id="ref-Junker2001">
<p>Junker, Brian, and Klaas Sijtsma. 2001. “Cognitive Assessment Models with Few Assumptions, and Connections with Nonparametric Item Response Theory.” <em>Applied Psychological Measurement</em> 25: 258–72.</p>
</div>
<div id="ref-tatsuoka1984">
<p>Tatsuoka, Kikumi K. 1984. <em>Analysis of Errors in Fraction Addition and Subtraction Problems</em>. Computer-based Education Research Laboratory, University of Illinois.</p>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
