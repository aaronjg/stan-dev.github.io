<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Bob Carpenter" />

<meta name="date" content="2017-04-11" />

<title>Typical Sets and the Curse of Dimensionality</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="site_libs/highlight/default.css"
      type="text/css" />
<script src="site_libs/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Typical Sets and the Curse of Dimensionality</h1>
<h4 class="author"><em>Bob Carpenter</em></h4>
<h4 class="date"><em>11 April 2017</em></h4>

</div>


<hr />
<div id="abstract" class="section level3">
<h3>Abstract</h3>
<p>This case study illustrates the so-called “curse of dimensionality,” starting from scratch and using simple examples based on simulation. For example, generating points uniformly at random in a unit hypercube, it is easy to see how the average distance between random points increases with dimensionality. Similarly, such points tend to increasingly fall outside the hypersphere inscribed within the unit hypercube, showing that in higher dimensions, almost all of the volume is in the corners of the hypercube.</p>
<p>Similarly, generating standard normal variates (zero mean, unit standard deviation) leads to draws that concentrate in a thin shell of increasing distance from the mean as dimensionality increases. The squared distance of a standard normal draw from the mode follows a standard chi-square distribution with degrees of freedom equal to the dimensionality. This allows the the precise bounds of the thin shell to be calculated using tail statistics, showing just how unlikely it is to get near the mode with a random draw from a standard multivariate normal.</p>
<p>These properties of the standard normal distribution provides a segue into discussing the important information-theoretic concept of the typical set for a probability distribution, which is roughly defined as the central log density band into which almost all random draws from that distribution will fall. The rather surprising result is that for a normal distribution in high dimensions, the typical set does <em>not</em> include the volume around the mode. The density is highest around the mode, but the volume is low, and the probability mass is determined as the product of the density and the volume (or more precisely, the integral of the density over the volume).</p>
<p>The standard normal log density is just negative squared Euclidean distance, which provides a straightforward demonstration of why the maximum likelihood estimate for a normal regression is identical to the least squares solution.</p>
<hr />
</div>
<div id="euclidean-length-and-distance" class="section level2">
<h2>1. Euclidean Length and Distance</h2>
<div id="euclidean-length" class="section level3">
<h3>Euclidean Length</h3>
<p>Consider a vector <span class="math inline">\(y = (y_1, \ldots, y_N)\)</span> with <span class="math inline">\(N\)</span> elements (we call such vectors <span class="math inline">\(N\)</span>-vectors). The Euclidean length of a vector <span class="math inline">\(y\)</span> is written as <span class="math inline">\(|\!| y |\!|\)</span>, and defined by a generalizing the Pythagorean theorem,</p>
<p><span class="math display">\[
|\!| y |\!|
\ = \ \sqrt{y_1^2 + y_2^2 + \cdots + y_N^2}.
\]</span></p>
<p>In words, the length of a vector is the square root of the sum of the squares of its elements.</p>
<p>If we take <span class="math inline">\(y = (y_1, \ldots, y_N)\)</span> to be a row vector, then we see that the dot product of a vector with itself is its squared length, so that</p>
<p><span class="math display">\[
|\!| y |\!| = \sqrt{y \, y^{\top}}.
\]</span></p>
<div id="calculating-vector-length-in-r" class="section level4">
<h4>Calculating Vector Length in R</h4>
<p>The following function computes vector length in R.</p>
<pre class="r"><code>euclidean_length &lt;- function(u) sqrt(sum(u * u));</code></pre>
<p>In R, the operator <code>*</code> operates elementwise rather than as vector multiplication. To test the function on a simple case, we can verify the first example of the Pythagorean theorem everyone learns in school, namely <span class="math inline">\(|\!| (3, 4) |\!| = 5\)</span>.</p>
<pre class="r"><code>euclidean_length(c(3, 4));</code></pre>
<pre><code>[1] 5</code></pre>
<p>In R, the function <code>length()</code> returns the number of elements in a vector rather than the vector’s Euclidean length.</p>
<pre class="r"><code>length(c(3, 4));</code></pre>
<pre><code>[1] 2</code></pre>
</div>
</div>
<div id="euclidean-distance" class="section level3">
<h3>Euclidean Distance</h3>
<p>The Euclidean distance between two <span class="math inline">\(N\)</span>-vectors, <span class="math inline">\(x = (x_1, \ldots, x_N)\)</span> and <span class="math inline">\(y = (y_1, \ldots, y_N)\)</span>, written <span class="math inline">\(\mathrm{d}(x,y)\)</span>, is the Euclidean length of the vector <span class="math inline">\(x - y\)</span> connecting them,</p>
<p><span class="math display">\[
\mathrm{d}(x, y)
\ = \
|\!| x - y |\!|
\ = \
\sqrt{(x_1 - y_1)^2 + \cdots + (x_N - y_N)^2}.
\]</span></p>
</div>
</div>
<div id="all-of-the-volume-is-in-the-corners" class="section level2">
<h2>2. All of the Volume is in the Corners</h2>
<p>Suppose we have a square and inscribe a circle in it, or that we have a cube and a sphere inscribed in it. When we extend this construction to higher dimensions, we get hyperspheres inscribed in hypercubes. This section illustrates the curious fact that as the dimensionality grows, most of the points in the hypercube lie outside the inscribed hypersphere.</p>
<p>Suppose we have an <span class="math inline">\(N\)</span>-dimensional hypercube, with unit-length sides centered around the origin <span class="math inline">\(\mathbf{0} = (0, \ldots, 0)\)</span>. The hypercube will have <span class="math inline">\(2^N\)</span> corners at the points <span class="math inline">\(\left( \pm \frac{1}{2}, \ldots, \pm \frac{1}{2} \right)\)</span>. Because its sides are length 1, it will have also have unit volume, because <span class="math inline">\(1^N = 1\)</span>.</p>
<p>If <span class="math inline">\(N=1\)</span>, the hypercube is a line from <span class="math inline">\(-\frac{1}{2}\)</span> to <span class="math inline">\(\frac{1}{2}\)</span> of unit length (i.e., length 1). If <span class="math inline">\(N=2\)</span>, the hypercube is a square of unit area with opposite corners at <span class="math inline">\(\left( -\frac{1}{2}, -\frac{1}{2} \right)\)</span> and <span class="math inline">\(\left( \frac{1}{2}, \frac{1}{2} \right)\)</span>. With <span class="math inline">\(N=3\)</span>, we have a cube of unit volume, with opposite corners at <span class="math inline">\(\left( -\frac{1}{2}, -\frac{1}{2}, -\frac{1}{2} \right)\)</span> and <span class="math inline">\(\left( \frac{1}{2}, \frac{1}{2}, \frac{1}{2} \right)\)</span>, and unit volume. And so on up the dimensions.</p>
<p>Now consider the biggest hypersphere you can inscribe in the hypercube. It will be centered at the origin and have a radius of <span class="math inline">\(\frac{1}{2}\)</span> so that it extends to the sides of the hypercube. A point <span class="math inline">\(y\)</span> is within this hypersphere if the distance to the origin is less than the radius, or in symbols, if <span class="math inline">\(|\!|y|\!| &lt; \frac{1}{2}\)</span>. Topologically speaking, we have defined what is known as an open ball, i.e., the set of points within a hypersphere excluding the limit points at distance exactly <span class="math inline">\(\frac{1}{2}\)</span> (we could’ve worked with closed balls which include the limit points making up the surface of the ball because this surface (a hypersphere) has zero volume).</p>
<p>In one dimension, the hypersphere is just the line from <span class="math inline">\(-\frac{1}{2}\)</span> to <span class="math inline">\(\frac{1}{2}\)</span> and contains the entire hypercube. In two dimensions, the hypersphere is a circle of radius <span class="math inline">\(\frac{1}{2}\)</span> centered at the origin and extending to the center of all four sides. In three dimensions, it’s a sphere that just fits in the cube, extending to the middle of all six sides.</p>
<div style="width:3in; float:right; padding: 1em; border: 2">
<img src="img/Monte_Carlo_Casino.jpg" alt="Monte Carlo Casino" /> <em><small>Monte Carlo Casino. From Wikipedia, with license CC BY 2.5.</small></em>
</div>
<div id="the-monte-carlo-method-for-integration" class="section level4">
<h4>The Monte Carlo Method for Integration</h4>
<p>We know the volume of the unit hypercube is one, but what is the volume of the ball in the inscribed hypersphere? You may have learned how to define an integral to calculate the answer for a ball of radius <span class="math inline">\(r\)</span> in two dimensions as <span class="math inline">\(\pi r^2\)</span>, and may even recall that in three dimensions it’s <span class="math inline">\(\frac{4}{3}\pi r^3\)</span>. But what about higher dimensions?</p>
<p>We could evaluate harder and harder multiple integrals, but we’ll instead use the need to solve these integrals as an opportunity to introduce the fundamental machinery of using sampling to calculate integrals. Such methods are called “Monte Carlo methods” because they involve random quantities and there is a famous casino in Monte Carlo. They are at the very heart of modern statistical computing (Bayesian and otherwise).</p>
<p>Monte Carlo integration allows us to calculate the value of a definite integral by averaging draws from a random number generator. (Technically, the random number generators we have on computers, like used in R, are pseudorandom number generators in the sense that they’re underlyingingly deterministic; for the sake of argument, we assume they are random enough for our purposes in much the way we assume the functions we’re dealing with are smooth enough).</p>
<p>To use Monte Carlo methods to calculate the volume within a hypersphere inscribed in a hypercube, we need only generate draws at random in the hypercube and count the number of draws that fall in the hypersphere—our answer is the the proportion of draws that fall in the hypersphere. That’s because we deliberately constructed a hypercube of unit volume; in general, we need to multiply by the volume of the set over which we are generating uniform random draws.</p>
<p>As the number of draws increases, the estimated volume converges to the true volume. Because the draws are i.i.d., it follows from the central limit theorem that the error goes down at a rate of <span class="math inline">\(\mathcal{O}\left( 1 / \sqrt{n} \right)\)</span>. That means each additional decimal place of accuracy requires multiplying the sample size by one hundred. We can get rough answers with Monte Carlo methods, but many decimal places of accuracy requires a prohibitive number of simulation draws.</p>
<p>We can draw the points at random from the hypercube by drawing each component independently according to</p>
<p><span class="math display">\[
y_n \sim  \mathsf{Uniform}\left(-\frac{1}{2}, \frac{1}{2}\right).
\]</span> Then we count the proportion of draws that lie within the hypersphere. Recall that a point <span class="math inline">\(y\)</span> lies in the hypersphere if <span class="math inline">\(|\!| y |\!| &lt; \frac{1}{2}\)</span>.</p>
</div>
<div id="using-monte-carlo-methods-to-compute-" class="section level4">
<h4>Using Monte Carlo methods to compute π</h4>
<p>We’ll first look at the case where <span class="math inline">\(N = 2\)</span>, just to make sure we get the right answer. We know the area inside the inscribed circle is <span class="math inline">\(\pi r^2\)</span>, so with <span class="math inline">\(r = \frac{1}{2}\)</span>, that’s <span class="math inline">\(\frac{\pi}{4}\)</span>. Let’s see if we get the right result.</p>
<pre class="r"><code>N &lt;- 2;
M &lt;- 1e6;
y &lt;- matrix(runif(M * N, -0.5, 0.5), M, N);
p &lt;- sum(sqrt(y[ , 1]^2 + y[ , 2]^2) &lt; 0.5) / M;
print(4 * p, digits=3);</code></pre>
<pre><code>[1] 3.14</code></pre>
<pre class="r"><code>print(pi, digits=3);</code></pre>
<pre><code>[1] 3.14</code></pre>
<p>Now, let’s generalize and calculate the volume of the hypersphere inscribed in the unit hypercube (which has unit volume by construction) for increasing dimensions.</p>
<pre class="r"><code>M &lt;- 1e5;
N_MAX = 10;
Pr_inside &lt;- rep(NA, N_MAX);
for (N in 1:N_MAX) {
  y &lt;- matrix(runif(M * N, -0.5, 0.5), M, N);
  inside &lt;- 0;
  for (m in 1:M) {
    if (euclidean_length(y[m,]) &lt; 0.5) {
      inside &lt;- inside + 1;
    }
  }
  Pr_inside[N] &lt;- inside / M;
}
df = data.frame(dims = 1:N_MAX, volume = Pr_inside);
print(df, digits=1);</code></pre>
<pre><code>   dims volume
1     1  1.000
2     2  0.785
3     3  0.525
4     4  0.310
5     5  0.163
6     6  0.080
7     7  0.038
8     8  0.016
9     9  0.006
10   10  0.003</code></pre>
<p>Although we actually calculate the probability that a point drawn at random is inside the hyperphere inscribed in the unit hypercube, this quantity gives the volume inside the inscribed hypersphere.</p>
<p>Here’s the result as a plot.</p>
<pre class="r"><code>library(ggplot2);
plot_corners &lt;-
  ggplot(df, aes(x = dims, y = Pr_inside)) +
  scale_x_continuous(breaks=c(1, 3, 5, 7, 9)) +
  geom_line(colour=&quot;gray&quot;) +
  geom_point() +
  ylab(&quot;volume of inscribed hyperball&quot;) + 
  xlab(&quot;dimensions&quot;) +
  ggtitle(&quot;Volume of Hyperball Inscribed in Unit Hypercube&quot;)
plot_corners;</code></pre>
<p><img src="curse-dims_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
<div id="all-points-are-far-away-from-each-other" class="section level3">
<h3>All points are far away from each other</h3>
<p>Another way of looking at this is that in higher dimensions, the points on average become further and further away from the center of the hypercube. They also become further and further away from each other (see the exercises). The volume of the hypersphere is the proportion of points in the hypercube that are within distance <span class="math inline">\(\frac{1}{2}\)</span> from the center of the hypercube; with inreasing dimension, vanishingly few points are within distance <span class="math inline">\(\frac{1}{2}\)</span> of the center of the hypercube.</p>
<p>As Bernhard Schölkopf <a
href="https://twitter.com/bschoelkopf/status/503554842829549568">said on Twitter</a>, “a high-dimensional space is a lonely place.” What he meant by this is that not only are the points increasingly far from the mean on average, they are also increasingly far from each other. As Michael Betancourt noted in a comment on the pull request for this case study, “Take two points that are translated from each other by 1 unit in each direction — the distance between them grows as <span class="math inline">\(\sqrt{N}\)</span> so as the dimension grows the points appear further from each other, with more and more volume filling up the space between them.”</p>
</div>
</div>
<div id="typical-sets" class="section level2">
<h2>3. Typical Sets</h2>
<p>Roughly speaking, the typical set is where draws from a given distribution tend to lie. That is, it covers most of the mass of the distribution. This particular choice of volume not only covers most of the mass of a distribution, its members reflect the properties of draws from that distribution. What we will see in this section is that the typical set is usually nowhere near the mode of the distribution, even for a multivariate normal (where the mode is the same as the mean).</p>
<div id="a-discrete-example-of-typicality" class="section level4">
<h4>A Discrete Example of Typicality</h4>
<p>Typicality is easiest to illustrate with a binomial example. Consider a binary trial with an eighty percent chance of success (i.e., draws from a <span class="math inline">\(\mathsf{Bernoulli}(0.80)\)</span> distribution). Now consider repeating such a trial one hundred times, drawing <span class="math inline">\(y_1, \ldots, y_{100}\)</span> independently according to</p>
<p><span class="math display">\[
y_n \sim \mathsf{Bernoulli}(0.8).
\]</span></p>
<p>Two questions frame the discussion:</p>
<ol style="list-style-type: decimal">
<li><p>What is the most likely value for <span class="math inline">\(y_1, \ldots, y_{100}\)</span>?</p></li>
<li><p>How many <span class="math inline">\(y_n\)</span> do you expect to be 1?</p></li>
</ol>
<p>Let’s answer the second question first. If you have an 80% chance of success and make 100 attemps, the expected number of successes is just 80 (in general, it’s the probability of success times the number of attempts).</p>
<p>Then why is the most likely outcome 100 straight successes? The probability of 100 straight successes is <span class="math inline">\(0.8^{100}\)</span> or about <span class="math inline">\(2 \times 10^{-10}\)</span>.</p>
<p>In contrast, the probability of any given sequence with 80 successes is only <span class="math inline">\(0.8^{80} \, 0.2^{20}\)</span>, or about <span class="math inline">\(2 \times 10^{-22}\)</span>. The chance of 100 straight successes is a whopping <span class="math inline">\(10^{12}\)</span> times more probable than any specific sequence with 80 successes and 20 failures!</p>
<p>On the other hand, there are a lot of sequences with 80 successes and 20 failures—a total of <span class="math inline">\(\binom{100}{80}\)</span> of them to be exact (around <span class="math inline">\(10^{20}\)</span>). So even though any given sequence of 80 success and 20 failures is relatively improbable, there are so many such sequences that their overall mass is higher than that of the unique sequence with 100 success, because <span class="math inline">\(10^{20} \times 2 \times 10^{-22}\)</span> is a lot bigger than <span class="math inline">\(1 \times 2 \times 10^{-10}\)</span>. Same goes for sequences with 81 successes; each such sequence is more probably than any sequence with 80 successes, but <span class="math inline">\(\binom{100}{81}\)</span> is smaller than <span class="math inline">\(\binom{100}{80}\)</span>.</p>
<p>The binomial distribution is the distribution of counts, so that if <span class="math inline">\(y_1, \ldots, y_N\)</span> is such that each <span class="math inline">\(y_n \sim \mathsf{Bernoulli}(\theta)\)</span>, then</p>
<p><span class="math display">\[
(y_1 + \cdots + y_N) \sim \mathsf{Binomial}(N, \theta).
\]</span></p>
<p>The binomial aggregates the multiple trials by multiplying through by the possible ways in which <span class="math inline">\(y\)</span> successes can arise in <span class="math inline">\(N\)</span> trials, namely</p>
<p><span class="math display">\[
\mathsf{Binomial}(y \mid N, \theta)
= \binom{N}{y} \, \theta^y \, (1 - \theta)^{(N - y)},
\]</span></p>
<p>where the binomial coefficient that normalizes the distribution is defined as the number of binary sequences of length <span class="math inline">\(N\)</span> that contain exactly <span class="math inline">\(y\)</span> elements equal to 1,</p>
<p><span class="math display">\[
\binom{N}{y} = \frac{N!}{y! \, (N - y)!}.
\]</span></p>
<p>In words, <span class="math inline">\(\mathsf{Binomial}(y \mid N, \theta)\)</span> is the probability of <span class="math inline">\(y\)</span> successes in <span class="math inline">\(N\)</span> independent Bernoulli trials if each trial has a chance of success <span class="math inline">\(\theta\)</span>.</p>
<p>To make sure we’re right in expecting around 80 succeses, we can simulate a million binomial outcomes from 100 trials with an 80% chance of success as</p>
<pre class="r"><code>z &lt;- rbinom(1e6, 100, 0.8);</code></pre>
<p>with a summary</p>
<pre class="r"><code>summary(z);</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##      58      77      80      80      83      96</code></pre>
<p>and 99% interval</p>
<pre class="r"><code>quantile(z, probs=c(0.005, 0.995));</code></pre>
<pre><code>##  0.5% 99.5% 
##    69    90</code></pre>
<p>The most probable outcome as a sequence of Bernoulli trials, namely <span class="math inline">\((1, 1, \ldots, 1)\)</span>, has a very improbable number of successes, namely <span class="math inline">\(N\)</span>. A much more typical number of successes is 80. In fact, 100 isn’t even in the 99% interval of 100 trials with an 80% success rate. We can see that analytically using the quantile (inverse cumulative distribution function). Suppose we have a random variable <span class="math inline">\(Y\)</span> with mass or density function <span class="math inline">\(p_Y(y)\)</span>. Then the cumulative distribution function (CDF) is defined by</p>
<p><span class="math display">\[
F_Y(u)
\ = \
\mbox{Pr}[Y \leq u].
\]</span></p>
<p>For discrete probability (mass) functions, this works out to</p>
<p><span class="math display">\[
F_Y(u) \ = \ \sum_{y \, = \, -\infty}^u p_Y(y),
\]</span></p>
<p>and for continuous probability (density) functions,</p>
<p><span class="math display">\[
F_Y(u) = \int_{-\infty}^u p_Y(y) \, \mbox{d}y.
\]</span></p>
<p>What we are going to need is the inverse CDF, <span class="math inline">\(F_Y^{-1}\)</span>, or quantile function, which maps a quantile <span class="math inline">\(\alpha \in (0, 1)\)</span> to the value <span class="math inline">\(y\)</span> such that <span class="math inline">\(\mbox{Pr}[Y \leq y] = \alpha\)</span> (this needs to be rounded in the discrete case to deal with the the fact that the inverse CDF is only technically defined for a countable number of quantiles).</p>
<p>Luckily, this is all built into R, and we can calculate the quantiles that bound the central 99.9999% interval,</p>
<pre class="r"><code>qbinom(c(0.0000005, 0.9999995), 100, 0.8)</code></pre>
<pre><code>## [1] 59 97</code></pre>
<p>This tells us that 99.9999% of the draws (i.e, 999,999 out of 1,000,000 draws) from <span class="math inline">\(\mathsf{Binomial}(100, 0.8)\)</span> lie in the range <span class="math inline">\((59, 97)\)</span>. This demonstrates just how atypical the most probable sequences are.</p>
<p>We next reproduce a graphical illustration from David J. C. MacKay’s <em>Information Theory, Inference, and Learning Algorithms</em> (Cambridge, 2003, section 4.4) of how the typical set of the binomial arises as a product of</p>
<ol style="list-style-type: decimal">
<li><p>the Bernoulli trial probability of a sequence with a given number of successes, and</p></li>
<li><p>the number of sequences with that many successes.</p></li>
</ol>
<p>Suppose we have a binary sequence of <span class="math inline">\(N\)</span> elements, <span class="math inline">\(z = z_1, \ldots, z_N\)</span>, with <span class="math inline">\(z_n \in \{ 0, 1 \}\)</span> and let</p>
<p><span class="math display">\[
y = \sum_{n=1}^N z_n
\]</span></p>
<p>be the total number of successes. The repeated Bernoulli trial probability of <span class="math inline">\(z\)</span> with a chance of success <span class="math inline">\(\theta \in [0, 1]\)</span> is given by the probability mass function</p>
<p><span class="math display">\[
p(z \mid \theta)
\ = \
\prod_{n=1}^N \mathsf{Bernoulli}(z_n \mid \theta)
\ = \
\theta^y \, (1 - \theta)^{(N - y)}.
\]</span></p>
<p>The number of ways in which a binary sequence with a total of <span class="math inline">\(y\)</span> successes out of <span class="math inline">\(N\)</span> trials can arise is the total number of ways a subset of <span class="math inline">\(y\)</span> elements may be chosen out of a set of <span class="math inline">\(N\)</span> elements, which is given by the binomial coefficient,</p>
<p><span class="math display">\[
\binom{N}{y} = \frac{N!}{y! \, (N - y)!}.
\]</span></p>
<p>The following plots show how the binomial probability mass function arises as a product of the binomial coefficient and the probability of a single sequence with a given number of successess. The left column contains plots on the linear scale and the right column plots on the log scale. The top plots are of the binomial coefficient, <span class="math inline">\(\binom{N}{y}\)</span>. Below that is the plot the probability of a single sequence with <span class="math inline">\(y\)</span> successes, namely <span class="math inline">\(\theta^y \, (1 - \theta)^{N-y}\)</span>. Below both of these plots is their product, the binomial probability mass function <span class="math inline">\(\mathsf{Binomial}(y \mid N, \theta)\)</span>. There are three sequences of plots, for <span class="math inline">\(N = 25\)</span>, <span class="math inline">\(N = 100\)</span>, and <span class="math inline">\(N=400\)</span>.</p>
<p>Just to get a sense of scale, there are <span class="math inline">\(2^{25}\)</span> (order <span class="math inline">\(10^7\)</span>) binary sequences of length 25, <span class="math inline">\(2^{100}\)</span> (order <span class="math inline">\(10^{30}\)</span>) binary sequences of length 100, and <span class="math inline">\(2^{400}\)</span> (order <span class="math inline">\(10^{120}\)</span>) binary sequences of length 400.</p>
<pre class="r"><code>choose_df &lt;- function(Ys, theta=0.2) {
  N &lt;- max(Ys);
  Ns &lt;- rep(N, length(Ys));
  Cs &lt;- choose(N, Ys);
  Ls &lt;- theta^Ys * (1 - theta)^(N - Ys);
  Ps &lt;- Cs * Ls;
  data.frame(list(y = Ys, N = Ns, combos = Cs, L = Ls, P = Ps));
}

choose_plot &lt;- function(df, logy = FALSE) {
  p &lt;-
    ggplot(df, aes(x = y, y = combos)) +
    geom_line(size=0.2, color=&quot;darkgray&quot;) +
    geom_point(size=0.4) +
    scale_x_continuous() +
    xlab(&quot;y&quot;);
  if (logy) {
    p &lt;- p + scale_y_log10() +
      ylab(&quot;log (N choose y)&quot;) +
      theme(axis.title.y = element_text(size=8),
            axis.text.y=element_blank(),
            axis.ticks.y=element_blank())
      ggtitle(&quot;sequence log permutations ...&quot;);
  } else {
    p &lt;- p + scale_y_continuous() +
      ylab(&quot;(N choose y)&quot;) +
      theme(axis.title.y = element_text(size=8),
            axis.text.y=element_blank(),
            axis.ticks.y=element_blank())
      ggtitle(&quot;sequence permutations ...&quot;);
  }
  return(p);
}
seq_plot &lt;- function(df, logy = FALSE) {
  p &lt;-
    ggplot(df, aes(x = y, y = L)) +
    geom_line(size=0.2, color=&quot;darkgray&quot;) +
    geom_point(size=0.4) +
    scale_x_continuous() +
    xlab(&quot;y&quot;);
    if (logy) {
      p &lt;- p + scale_y_log10() +
           ylab(&quot;log theta^y * (1 - theta)^(N - y)&quot;) +
           theme(axis.title.y = element_text(size=8),
                 axis.text.y=element_blank(),
                 axis.ticks.y=element_blank())
           ggtitle(&quot;plus sequence log probability ...&quot;);
    } else {
      p &lt;- p + scale_y_continuous() +
           ylab(&quot;theta^y * (1 - theta)^(N - y)&quot;) +
           theme(axis.title.y = element_text(size=8),
                 axis.text.y=element_blank(),
                 axis.ticks.y=element_blank())
           ggtitle(&quot;times sequence probability ...&quot;);
  }
  return(p);
}
joint_plot &lt;- function(df, logy = FALSE) {
  p &lt;- ggplot(df, aes(x = y, y = P)) +
    geom_line(size = 0.25, color = &quot;darkgray&quot;) +
    geom_point(size = 0.25) +
    scale_x_continuous() +
    xlab(&quot;y&quot;);
  if (logy) {
    p &lt;- p + scale_y_log10() +
           ylab(&quot;log binom(y | N, theta)&quot;) +
          theme(axis.title.y = element_text(size=8),
                axis.text.y=element_blank(),
                axis.ticks.y=element_blank())
           ggtitle(&quot;equals count log probability&quot;);
  } else {
    p &lt;- p + scale_y_continuous() +
          ylab(&quot;binom(y | N, theta)&quot;) +
          theme(axis.title.y = element_text(size=8),
                axis.text.y=element_blank(),
                axis.ticks.y=element_blank())
          ggtitle(&quot;equals count probability&quot;);
  }
  return(p);
}
library(&quot;grid&quot;)
library(&quot;gridExtra&quot;);
plot_all &lt;- function(df) {
  cp &lt;- choose_plot(df, logy = FALSE);
  pp &lt;- seq_plot(df, logy = FALSE);
  jp &lt;- joint_plot(df, logy = FALSE);
  lcp &lt;- choose_plot(df, logy = TRUE);
  lpp &lt;- seq_plot(df, logy = TRUE);
  ljp &lt;- joint_plot(df, logy = TRUE);
  grid.newpage();
  grid.arrange(ggplotGrob(cp), ggplotGrob(lcp),
               ggplotGrob(pp), ggplotGrob(lpp),
               ggplotGrob(jp), ggplotGrob(ljp),
               ncol = 2);
}

df25 &lt;- choose_df(0:25);
plot_all(df25);</code></pre>
<p><img src="curse-dims_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<pre class="r"><code>df100 &lt;- choose_df(0:100)
plot_all(df100);</code></pre>
<p><img src="curse-dims_files/figure-html/unnamed-chunk-11-2.png" width="672" /></p>
<pre class="r"><code>df400 &lt;- choose_df(0:400);
plot_all(df400);</code></pre>
<p><img src="curse-dims_files/figure-html/unnamed-chunk-11-3.png" width="672" /></p>
<p>Although it appears that way, the points are not getting heavier, there are just more of them in each subsequent plot.</p>
</div>
<div id="a-continuous-example-of-typicality" class="section level4">
<h4>A Continuous Example of Typicality</h4>
<p>All of the computations of interest in Bayesian statistics are formulated as posterior expectations. For example, Bayesian parameter estimates minimizing expected square error are expectations of the value of a parameter <span class="math inline">\(\theta\)</span> in the posterior (conditioned on data <span class="math inline">\(y\)</span>),</p>
<p><span class="math display">\[
\hat{\theta} = \mathbb{E}[\theta \mid y] = \int_{\Theta} \theta \,
p(\theta \mid y) \, \mathrm{d}\theta,
\]</span></p>
<p>where we integrate over the legal values of <span class="math inline">\(\theta \in \Theta\)</span>.</p>
<p>Posterior event probabilities are expectations of the indicator function <span class="math inline">\(I[\theta \in E]\)</span> that indicates whether a parameter value <span class="math inline">\(\theta\)</span> is in the event <span class="math inline">\(E\)</span>,</p>
<p><span class="math display">\[
\mbox{Pr}[E \mid y]
\ = \
\mathbb{E}[\, I[\theta \in E] \mid y \, ]
\ = \
\int_{\Theta} I[\theta \in E] \ p(\theta \mid y) \, \mathrm{d}\theta.
\]</span></p>
<p>Rather than integrating over the support <span class="math inline">\(\Theta\)</span> of <span class="math inline">\(\theta\)</span> in the posterior, it suffices to integrate over the typical set (the typical set is not the only such set; see the exercises for a discussion of the highest probability set). That is, if <span class="math inline">\(\Theta\)</span> is the support for a density <span class="math inline">\(p(\theta)\)</span>, then the typical set <span class="math inline">\(T \subseteq \Theta\)</span> has the property that for “well-behaved” functions,</p>
<p><span class="math display">\[
\mathbb{E}[f(\theta) \mid y]
\ = \
\int_{\Theta} f(\theta) \, p(\theta \mid y) \, \mathrm{d}\theta
\ \approx \
\int_{T}  f(\theta) \, p(\theta \mid y) \, \mathrm{d}\theta
\]</span></p>
<p>That is, we can restrict attention to the typical set when computing expectations. The fact that typical sets exist is why we are able to compute posterior expectations using Monte Carlo methods—the draws from the posterior distribution we make with Monte Carlo methods will almost all fall in the typical set and we know that will be sufficient.</p>
<p>We saw in the first plot in section 4 (draws are nowhere near the mode) that if we have <span class="math inline">\(D\)</span> i.i.d. draws from a standard normal, that the typical set was well bounded away from the posterior mode, which is at the origin. There’s no built-in R function to compute the quantiles directly, but the next section shows that the distance from the mode reduces to a well-known distribution.</p>
</div>
<div id="definition-of-the-typical-set" class="section level4">
<h4>Definition of the Typical Set</h4>
<p>The formal definition of typicality is for sequences <span class="math inline">\(x_1, \ldots, x_N\)</span> of i.i.d. draws with probability function <span class="math inline">\(p(x)\)</span> (density function for continuous variables, mass function for discrete variables). Such a sequence is said to be “typical” if its average probability is near the (differential) entropy of the distribution. A sequence <span class="math inline">\((x_1, \ldots, x_N)\)</span> is in the typical set <span class="math inline">\(A^N_{\epsilon}\)</span> for a distribution with probability function <span class="math inline">\(p(x)\)</span> if</p>
<p><span class="math display">\[
\left| \frac{1}{N} \sum_{n=1}^N \log p(x_n)  - \mathrm{H}[ X ] \right|
\leq \epsilon,
\]</span></p>
<p>where <span class="math inline">\(\mathrm{H}[X]\)</span> is the discrete entropy if <span class="math inline">\(X\)</span> is discrete and the differential entropy if <span class="math inline">\(X\)</span> is continuous. It can be established that such a set covers most such sequences, in that</p>
<p><span class="math display">\[
\mathrm{Pr}\left[
(X_1, \ldots, X_N) \in A_\epsilon^N
\right] &gt; 1 - \epsilon.
\]</span></p>
<p>assuming the <span class="math inline">\(X_n\)</span> are independent and each has probability function <span class="math inline">\(p(x)\)</span>.</p>
</div>
</div>
<div id="the-normal-distribution" class="section level2">
<h2>3. The Normal Distribution</h2>
<p>The normal log density for a variate <span class="math inline">\(y \in \mathbb{R}\)</span> with location <span class="math inline">\(\mu \in \mathbb{R}\)</span> and scale <span class="math inline">\(\sigma \in \mathbb{R}^+\)</span> is defined by</p>
<p><span class="math display">\[
\mathsf{Normal}(y \mid \mu, \sigma)
= \frac{1}{\sqrt{2 \pi}}
  \, \frac{1}{\sigma}
  \, \exp
       \left(
         -\frac{1}{2}
         \, \left( \frac{y - \mu}{\sigma} \right)^2
       \right).
\]</span></p>
<p>For the standard normal, <span class="math inline">\(\mathsf{Normal}(0, 1)\)</span>, many terms drop out, and we are left with</p>
<p><span class="math display">\[
\mathsf{Normal}(y \mid 0, 1)
= \frac{1}{\sqrt{2 \pi}}
  \, \exp \left( -\frac{1}{2} \ y^2 \right).
\]</span></p>
<p>Converting to the log scale, we have</p>
<p><span class="math display">\[
\log \mathsf{Normal}(y \mid 0, 1)
\ = \ -\frac{1}{2} \ y^2 + \mathrm{constant}
\]</span></p>
<p>where the constant does not depend on <span class="math inline">\(y\)</span>. In this form, it’s easy to see the relation between the standard normal and distance. For multiple i.i.d. standard normal draws <span class="math inline">\(y = (y_1, \ldots, y_N)\)</span>, then <span class="math display">\[
\log p(y) 
\ = \
\sum_{n=1}^N -\frac{1}{2} y_n^2
\ = \
-\frac{1}{2} yy^{\top}
\ = \
- \frac{1}{2} |\!| y |\!|^2
\]</span></p>
<p>In words, the log density of a standard normal distribution is proportional to half the squared distaince from the origin. This is why the maximum likelihood estimate is equal to the least squares estimate for normal distributions.</p>
</div>
<div id="vectors-of-random-unit-normals" class="section level2">
<h2>4. Vectors of Random Unit Normals</h2>
<p>We are now going to generate a random vector <span class="math inline">\(y = (y_1, \ldots, y_D) \in \mathbb{R}^D\)</span> by generating each dimension independently as</p>
<p><span class="math display">\[
y_d \sim \mathsf{Normal}(0, 1).
\]</span></p>
<p>The density over vectors is then defined by</p>
<p><span class="math display">\[
p(y)
\ = \
\prod_{d=1}^D p(y_d)
\ = \
\prod_{d=1}^D \mathsf{Normal}(y_d \mid 0, 1).
\]</span></p>
<p>On the log scale, that’s</p>
<p><span class="math display">\[
\log p(y)
\ = \
\sum_{d=1}^D -\frac{1}{2} \, y_d^2 + \mathrm{const}
\ = \
-\frac{1}{2} \, {|\!| y |\!|}^2 + \mathrm{const}.
\]</span></p>
<p>Equivalently, we could generate the vector <span class="math inline">\(y\)</span> all at once from a multivariate normal with unit covariance matrix,</p>
<p><span class="math display">\[
y \sim \mathsf{MultiNormal}(\mathbf{0}, \mathbf{I}),
\]</span></p>
<p>where <span class="math inline">\(\mathbf{0}\)</span> is a <span class="math inline">\(D\)</span>-vector of zero values, and <span class="math inline">\(\mathbf{I}\)</span> is the <span class="math inline">\(D \times D\)</span> unit covariance matrix with unit values on the diagonal and zero values off diagonal (i.e., unit scale and no correlation).</p>
<p>To generate a random vector <span class="math inline">\(y\)</span> in R, we can use the <code>rnorm()</code> function, which generates univariate random draws from a normal distribution.</p>
<pre class="r"><code>D &lt;- 10;
u &lt;- rnorm(D, 0, 1);
print(u, digits=2);</code></pre>
<pre><code> [1]  0.11  0.64 -0.12 -1.16  1.78 -0.18 -0.10 -0.77  0.66  2.57</code></pre>
<p>It is equally straightforward to compute the Euclidean length of <code>u</code> given the function we defined above:</p>
<pre class="r"><code>print(euclidean_length(u), digits=3);</code></pre>
<pre><code>[1] 3.55</code></pre>
<p>What is the distribution of the lengths of vectors generated this way as a function of the dimensionality? We answer the question analytically in the next section, but for now, we’ll get a handle on what’s going on through simulation as the dimensionality <span class="math inline">\(D\)</span> grows. The following code draws from normal distributions of dimensionality 1 to 256, then plots them with 99% intervals using ggplot.</p>
<pre class="r"><code>log_sum_exp &lt;- function(u) max(u) + log(sum(exp(u - max(u))));
N &lt;- 1e4;
dim &lt;- c(1, 2, 4, 5, 8, 11, 16, 22, 32, 45, 64, 90, 128, 181, 256);
D &lt;- length(dim);
lower &lt;- rep(NA, D);
middle &lt;- rep(NA, D);
upper &lt;- rep(NA, D);
lower_ll &lt;- rep(NA, D);
middle_ll &lt;- rep(NA, D);
upper_ll &lt;- rep(NA, D);

mean_ll &lt;- rep(NA, D);
max_ll &lt;- rep(NA, D);
for (k in 1:D) {
  d &lt;- dim[k];
  y &lt;- rep(NA, N);
  for (n in 1:N) y[n] &lt;- euclidean_length(rnorm(d, 0, 1));

  qs &lt;- quantile(y, probs=c(0.005, 0.500, 0.995));
  lower[k] &lt;- qs[[1]];
  middle[k] &lt;- qs[[2]];
  upper[k] &lt;- qs[[3]];

  ll &lt;- rep(NA, N);
  for (n in 1:N) ll[n] &lt;- sum(dnorm(rnorm(d, 0, 1), 0, 1, log=TRUE));

  qs_ll &lt;- quantile(ll, probs=c(0.005, 0.500, 0.995));
  lower_ll[k] &lt;- qs_ll[[1]];
  middle_ll[k] &lt;- qs_ll[[2]];
  upper_ll[k] &lt;- qs_ll[[3]];

  mean_ll[k] &lt;- log_sum_exp(ll) - log(N);
  max_ll[k] &lt;- sum(dnorm(rep(0, d), 0, 1, log=TRUE));
}
df &lt;- data.frame(list(dim = dim, lb = lower, mid = middle, ub = upper,
                      lb_ll = lower_ll, mid_ll = middle_ll,
                      ub_ll = upper_ll, mean_ll = mean_ll, max_ll = max_ll));
print(df, digits = 1);</code></pre>
<pre><code>   dim    lb  mid ub lb_ll mid_ll  ub_ll mean_ll max_ll
1    1 7e-03  0.7  3    -5     -1   -0.9      -1   -0.9
2    2 1e-01  1.2  3    -7     -3   -1.8      -3   -1.8
3    4 5e-01  1.9  4   -11     -5   -3.8      -5   -3.7
4    5 6e-01  2.1  4   -13     -7   -4.8      -6   -4.6
5    8 1e+00  2.7  5   -18    -11   -8.0     -10   -7.4
6   11 2e+00  3.2  5   -23    -15  -11.4     -14  -10.1
7   16 2e+00  3.9  6   -31    -22  -17.3     -20  -14.7
8   22 3e+00  4.6  7   -41    -31  -24.4     -28  -20.2
9   32 4e+00  5.6  7   -57    -45  -37.1     -41  -29.4
10  45 5e+00  6.7  9   -78    -64  -53.5     -57  -41.4
11  64 6e+00  7.9 10  -108    -90  -78.1     -82  -58.8
12  90 8e+00  9.4 11  -148   -127 -112.3    -114  -82.7
13 128 9e+00 11.3 13  -205   -181 -162.8    -163 -117.6
14 181 1e+01 13.4 15  -284   -256 -234.1    -234 -166.3
15 256 1e+01 16.0 18  -394   -363 -335.4    -335 -235.2</code></pre>
<p>Then we can plot the 99% intervals of the draws using ggplot.</p>
<pre class="r"><code>library(ggplot2);

plot1 &lt;-
  ggplot(df, aes(dim)) +
  geom_ribbon(aes(ymin = lb, ymax = ub), fill=&quot;lightyellow&quot;) +
  geom_line(aes(y = mid)) +
  geom_point(aes(y = mid)) +
  scale_x_log10(breaks=2^(0:D)) +
  ylab(&quot;Euclidean distance from origin (mode)&quot;) +
  xlab(&quot;number of dimensions&quot;) +
  ggtitle(&quot;Draws are Nowhere Near the Mode\n(median draw with 99% intervals)&quot;);
plot1;</code></pre>
<p><img src="curse-dims_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>Even in 16 dimensions, the 99% intervals are far away from zero, which is the mode (highest density point) of the 16-dimensional unit normal distribution.</p>
<div id="concentration-of-measure" class="section level5">
<h5>Concentration of measure</h5>
<p>Not only are the intervals moving away from the mode, they are concentrating into a narrow band in higher dimensions. This result follows here from the central limit theorem in this example and in more generality from concentration of measure (Terry Tao blogged a <a
href="https://terrytao.wordpress.com/2010/01/03/254a-notes-1-concentration-of-measure/">nice introduction</a>).</p>
<p>How does the log density of random draws compare to the log density at the mode (i.e., the location at which density is maximized)? The following plot of the median log density and 99% intervals along with the density at the mode illustrates.</p>
<pre class="r"><code>plot2 &lt;-
  ggplot(df, aes(dim)) +
  geom_ribbon(aes(ymin = lb_ll, ymax = ub_ll), fill=&quot;lightyellow&quot;) +
  geom_line(aes(y = mid_ll)) +
  geom_point(aes(y = mid_ll)) +
  geom_line(aes(y = max_ll), color=&quot;red&quot;) +
  geom_point(aes(y = max_ll), color=&quot;red&quot;) +
  scale_x_log10(breaks=c(2^(0:D))) +
  scale_y_continuous() +
  ylab(&quot;log density&quot;) +
  xlab(&quot;number of dimensions&quot;) +
  ggtitle(&quot;Draws have Much Lower Density than the Mode
(median and 99% intervals of random draws; mode in red)&quot;);
plot2;</code></pre>
<p><img src="curse-dims_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
</div>
<div id="no-individual-is-average" class="section level4">
<h4>No individual is average</h4>
<p>Somewhat counterintuitively, although easy to see in retrospect given the above graphs, the average member of a population is an outlier. How could an item with every feature being average be unusual? Precisely because it is unusual to have so many features that close to average.</p>
<p>In comments on an earlier draft, Michael Betancourt mentioned that physicists like to use an average of a population as a representative, calling such a representative an “Asimov data set” (the name derives from Isaac Asimov’s 1955 short story <a href="https://en.wikipedia.org/wiki/Franchise_(short_story)">Franchise</a>, in which a single elector is chosen to represent a population). Because of the atypicality of the average member of the population, this technique is ripe for misuse. As we saw above, the average member of a population might be located at the mode, whereas the average distance of a population member to the mode is much greater.</p>
</div>
</div>
<div id="squared-distance-of-normal-draws-is-chi-square" class="section level2">
<h2>5. Squared Distance of Normal Draws is Chi-Square</h2>
<p>Suppose <span class="math inline">\(y = (y_1, \ldots, y_D) \in \mathbb{R}^D\)</span> and that for each <span class="math inline">\(d \in 1{:}D\)</span>,</p>
<p><span class="math display">\[
y_d \sim \mathsf{Normal}(0, 1),
\]</span></p>
<p>is drawn independently. The distribution of the sum of the squared elements of <span class="math inline">\(y\)</span> is well known to have a chi-square distribution,</p>
<p><span class="math display">\[
(y_1^2 + \cdots + y_D^2) \sim \mathsf{ChiSquare}(D).
\]</span></p>
<p>In other words, the squared length of a unit normal draw has a chi-square distribution,</p>
<p><span class="math display">\[
{|\!| y |\!|}^2 \sim \mathsf{ChiSquare}(D).
\]</span></p>
<p>This means we could have drawn the plot out analytically rather than using Monte Carlo methods by using the inverse cumulative distribution function for the chi-square distibution (see the exercises).</p>
</div>
<div id="maximum-likelihood-and-least-squares" class="section level2">
<h2>6. Maximum Likelihood and Least Squares</h2>
<p>Gauss recognized the intimate relation between (squared Euclidean) distance and the normal distribution. The connection led him to develop the maximum likelihood principle and show that the maximum likelihood estimate of the mean of a normal distribution was the average, and that such an estimate also minimized the sum of square distances to the data points.</p>
<p>Suppose we observe <span class="math inline">\(y = (y_1, \ldots, y_N) \in \mathbb{R}^N\)</span> and we assume they come from a normal distribution with unit scale <span class="math inline">\(\sigma = 1\)</span> and unknown location <span class="math inline">\(\mu\)</span>. Then the log density is</p>
<p><span class="math display">\[
\log p(y \mid \mu, 1)
\ \propto \
\sum_{n=1}^N \log \mathsf{Normal}(y_n \mid \mu, 1)
\ \propto \
-\frac{1}{2} \sum_{n=1}^N \left( y_n - \mu \right)^2
\ \propto \
-\frac{1}{2} \sum_{n=1}^N \mathrm{d}(y_n, \mu)^2.
\]</span></p>
<p>The maximum likelihood estimate for the location <span class="math inline">\(\mu\)</span> is just the value that maximizes the likelihood function,</p>
<p><span class="math display">\[
\mu^*
\ = \
\mathrm{arg \, max}_{\mu} \ p(y \mid \mu)
\ = \
\mathrm{arg\, max}_{\mu} \ \log p(y \mid \mu)
\ = \
\mathrm{arg \, max}_{\mu} - \frac{1}{2} \, \sum_{n=1}^N \mathrm{d}(y_n, \mu)^2
\ = \
\mathrm{arg \, min}_{\mu} \sum_{n=1}^N \mathrm{d}(y_n, \mu)^2.
\]</span></p>
<p>The last step drops the negative constant multiplier, thus converting maximization to minimization and showing that the estimate that maximizes the likelihood is also the estimate that minimizes the sum of the square distances from the observations to the estimate of <span class="math inline">\(\mu\)</span>.</p>
<p>The estimate that minimizes squares is just the average of the observations,</p>
<p><span class="math display">\[
\mu^*
\ = \
\frac{1}{N} \sum_{n=1}^N y_n.
\]</span></p>
<p>The <a href="https://en.wikipedia.org/wiki/Gauss–Markov_theorem">Gauss-Markov theorem</a> further establishes that the mean has the lowest variance among unbiased estimators (it also generalizes the result to linear regressions).</p>
</div>
<div id="acknowledgements" class="section level2">
<h2>Acknowledgements</h2>
<p>Thanks to Michael Betancourt for comments on an earlier draft of this paper and to the reader’s of Andrew Gelman’s blog for a long series of comments on typical sets in response to my <a href="http://andrewgelman.com/2017/03/15/ensemble-methods-doomed-fail-high-dimensions/">original post on ensemble methods</a>.</p>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<p>Cover and Thomas (2006) is the standard reference for information theory and provides full definitions of entropy, differential entropy, highest probability sets, typical sets, and the connection of the typical set to the asymptotic equipartition property (AEP). Robert and Casella (2005) cover a range of statistical applications of Monte Carlo methods; they also published a companion version with R code. MacKay (2003) provides a more computationally oriented introduction to both Monte Carlo methods and information theory, with straightforward implementations in MATLAB/Octave.</p>
<ul>
<li><p>Cover, T. M., and Thomas, J. A. 2006. <em>Elements of Information Theory</em>. 2nd Edition. John Wiley &amp; Sons.</p></li>
<li><p>Robert, C. P. and G. Casella. 2005. <em>Monte Carlo Statistical Methods</em>. Springer.</p></li>
<li><p>MacKay, David J. C. 2003. <em>Information Theory, Inference and Learning Algorithms</em>. Cambridge University Press.</p></li>
</ul>
</div>
<div id="exercises" class="section level2">
<h2>Exercises</h2>
<ol style="list-style-type: decimal">
<li><p>Define an R function for the Euclidean distance between two vectors. What happens if you use R’s matrix multiplication, such as <code>u %*% v</code>?</p></li>
<li><p>Given <span class="math inline">\(y_n \in \mathbb{R}\)</span> for <span class="math inline">\(n \in 1{:}N\)</span>, the maximum likelihood estimate of <span class="math inline">\(\mu\)</span> for the model <span class="math display">\[
p(y \mid \mu) = \prod_{n=1}^N \mathsf{Normal}(y_n \mid \mu, 1)
\]</span> is just the average of the <span class="math inline">\(y_n\)</span> (i.e., <span class="math inline">\(\mu^* = \bar{y}\)</span>, where <span class="math inline">\(\bar{y}\)</span> is standard notation for the average of <span class="math inline">\(y\)</span>). Hint: show that the first derivative of the log likelihood with respect to <span class="math inline">\(\mu\)</span> is zero and the second derivative is negative.</p></li>
<li><p>For the model in the previous question, show that the maximum likelihood estimate for <span class="math inline">\(\mu\)</span> is the same no matter what <span class="math inline">\(\sigma\)</span> is. Use this fact to derive the maximum likelihood estimate for <span class="math inline">\(\sigma\)</span>. How does the maximum likelihood estimate differ from the statistic known as the sample standard deviation, defined by <span class="math display">\[
\mathrm{sd}(y) = \sqrt{\frac{1}{N - 1} \sum_{n=1}^N (y_n - \bar{y})^2},
\]</span> with <span class="math inline">\(\bar{y} = \frac{1}{N} \sum_{n=1}^N y_n\)</span> being the average of the <span class="math inline">\(y_n\)</span> values. For a fixed scale <span class="math inline">\(\sigma = 1\)</span>, show that the maximum likelihood estimate for a normal mean parameter is equal to the average of the observed <span class="math inline">\(y\)</span>.</p></li>
<li><p>Show that the Euclidean length of a vector <span class="math inline">\(y\)</span> is its distance to the origin, i.e., <span class="math display">\[
|\!| y |\!| = \mathrm{d}(y, \mathbf{0}),
\]</span> where <span class="math inline">\(\mathbf{0} = (0, \ldots, 0)\)</span> is the zero vector.</p></li>
<li><p>Repeat the computational distance calculations for the <span class="math inline">\(\mathrm{L}_1\)</span> norm, defined by <span class="math display">\[
{|\!| y |\!|}_1 = | y_1 | + | y_2 | + \cdots + | y_D |.
\]</span> and the taxicab distance, defined by <span class="math display">\[
d_1(u, v) = {|\!| u - v |\!|}_1.
\]</span> The taxicab distance (or Manhattan distance) is so-called because it may be thought of as a path in Euclidean distance that follows the axes, going from <span class="math inline">\((0,0)\)</span> to <span class="math inline">\((3, 4)\)</span> by way of <span class="math inline">\((3,0)\)</span> or <span class="math inline">\((0, 4)\)</span> (that is, along the streets and avenues).</p></li>
<li><p>Use random number generation and plotting to show what happens to the distance between two points generated uniformly at random in a unit hypercube as the number of dimensions increases.</p></li>
<li><p>Show how the double exponential distribution (aka Laplace distribution) plays the same role with respect to the <span class="math inline">\(\mathrm{L}_1\)</span> norm and taxicab distance as the normal distribution plays with respect to the <span class="math inline">\(\mathrm{L}_2\)</span> norm and Euclidean distance.</p></li>
<li><p>Repeat the computational distance calcuations for the <span class="math inline">\(\mathrm{L}_{\infty}\)</span> norm and associated distance function, defined by <span class="math display">\[
{|\!|y|\!|}_{\infty} = \max \{ | y_1 |, \ldots, | y_D | \}.
\]</span> If the elements of <span class="math inline">\(y\)</span> are independently drawn from a unit normal distribution, the <span class="math inline">\(\mathrm{L}_{\infty}\)</span> norm has the distribution of the <span class="math inline">\(D\)</span>-th order statistic for the normal.</p></li>
<li><p>Recreate the curves in the first few plots using the inverse cumulative distribution function for the chi-square distribution.</p></li>
<li><p>Show that the log density for a multivariate distribution with unit covariance and mean vector <span class="math inline">\(\mu\)</span> is equal to half squared distance between <span class="math inline">\(y\)</span> and <span class="math inline">\(\mu\)</span> plus a constant. In symbols, <span class="math display">\[
\log \mathsf{MultiNorm}(y \mid \mu, \mathbf{I}) = \frac{1}{2} \,
\mathrm{d}(y, \mu)^2 + \mathrm{constant}.
\]</span></p></li>
<li><p>Show that the log density of a vector <span class="math inline">\(y\)</span> in a multivariate normal distribution with location <span class="math inline">\(\mu\)</span> and covariance matrix <span class="math inline">\(\Sigma\)</span> is half the distance between <span class="math inline">\(y\)</span> and <span class="math inline">\(\mu\)</span> in the Riemannian manifold for which the inverse covariance matrix <span class="math inline">\(\Sigma^{-1}\)</span> defines a metric using the standard quadratic form, <span class="math display">\[
\mathrm{d}(y, \mu) = (y - \mu) \, \Sigma^{-1} \, (y - \mu)^{\top}.
\]</span></p></li>
<li><p>The <span class="math inline">\(\mathrm{L}_p\)</span> norm is defined for a vector <span class="math inline">\(y = (y_1, \ldots, y_N)\)</span> and <span class="math inline">\(p &gt; 0\)</span> by <span class="math display">\[
|\!|y|\!|_p = \left( \sum_{n=1}^N {|y_n|}^p \right)^\frac{1}{p}
\]</span> Show that Euclidean distance is defined by the <span class="math inline">\(\mathrm{L}_2\)</span> norm and the taxicab distance by the <span class="math inline">\(\mathrm{L}_1\)</span> norm. What happens in the limits as <span class="math inline">\(p \rightarrow 0\)</span> and <span class="math inline">\(p \rightarrow \infty\)</span>?</p></li>
<li><p>Use the <code>qchisq</code> function in R to generate the median and 99% (or more extreme) intervals for the distribution of lengths of vectors with dimensions drawn as independent unit normals. Use <code>qbinom</code> to do the same thing for binomial draws with an 80% chance of success to illustrate how atypical the all-success draw is.</p></li>
<li><p>The smallest set by volume that contains <span class="math inline">\(1 - \epsilon\)</span> of the probability mass is called the highest probability set. In a univariate, unimodal density, the highest probabilty set is the highest density interval (HDI). Show that the probability of a random draw falling in the difference between these sets (typical set and highest probability set) quickly converges to zero.</p></li>
<li><p>Use the central limit theorem to show why a chi-square distribution tends toward a normal distribution as the number of degrees of freedom grows.</p></li>
</ol>
<br />  
<hr />
<div id="licenses" class="section level4">
<h4>Licenses</h4>
<p><small> <strong>Code:</strong> Copyright (2017) Columbia University. Released under the <a href="https://opensource.org/licenses/BSD-3-Clause">BSD 3-clause license</a>. </small></p>
<p><small> <strong>Text:</strong> Copyright (2017) Bob Carpenter. Released under the the <a href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 license</a>. </small></p>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
