<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Daniel C. Furr" />

<meta name="date" content="2017-06-28" />

<title>Rasch and two-parameter logistic item response models with latent regression</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="site_libs/highlight/default.css"
      type="text/css" />
<script src="site_libs/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="rasch_and_2pl_files/styles.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Rasch and two-parameter logistic item response models with latent regression</h1>
<h4 class="author"><em>Daniel C. Furr</em></h4>
<h4 class="date"><em>June 28, 2017</em></h4>

</div>

<div id="TOC">
<ul>
<li><a href="#rasch-model-with-latent-regression"><span class="toc-section-number">1</span> Rasch model with latent regression</a><ul>
<li><a href="#overview-of-the-model"><span class="toc-section-number">1.1</span> Overview of the model</a></li>
<li><a href="#stan-code-for-a-simple-rasch-model"><span class="toc-section-number">1.2</span> <strong>Stan</strong> code for a simple Rasch model</a></li>
<li><a href="#stan-code-for-the-rasch-model-with-latent-regression"><span class="toc-section-number">1.3</span> <strong>Stan</strong> code for the Rasch model with latent regression</a></li>
<li><a href="#simulation-for-parameter-recovery"><span class="toc-section-number">1.4</span> Simulation for parameter recovery</a></li>
</ul></li>
<li><a href="#two-parameter-logistic-model-with-latent-regression"><span class="toc-section-number">2</span> Two-parameter logistic model with latent regression</a><ul>
<li><a href="#overview-of-the-model-1"><span class="toc-section-number">2.1</span> Overview of the model</a></li>
<li><a href="#stan-code-for-the-2pl-with-latent-regression"><span class="toc-section-number">2.2</span> <strong>Stan</strong> code for the 2PL with latent regression</a></li>
<li><a href="#simulation-for-parameter-recovery-1"><span class="toc-section-number">2.3</span> Simulation for parameter recovery</a></li>
</ul></li>
<li><a href="#example-application"><span class="toc-section-number">3</span> Example application</a><ul>
<li><a href="#data"><span class="toc-section-number">3.1</span> Data</a></li>
<li><a href="#rasch-model-results"><span class="toc-section-number">3.2</span> Rasch model results</a></li>
<li><a href="#two-parameter-logistic-model-results"><span class="toc-section-number">3.3</span> Two parameter logistic model results</a></li>
</ul></li>
<li><a href="#references"><span class="toc-section-number">4</span> References</a></li>
</ul>
</div>

<p>This case study uses <strong>Stan</strong> to fit the Rasch and two-parameter logistic (2PL) item response theory models, including a latent regression for person ability for both. The Rasch model is some times referred to as the one-parameter logistic model. Analysis is performed with <strong>R</strong>, making use of the <strong>rstan</strong> and <strong>edstan</strong> packages. <strong>rstan</strong> is the implementation of <strong>Stan</strong> for <strong>R</strong>, and <strong>edstan</strong> provides <strong>Stan</strong> models for item response theory and several convenience functions.</p>
<p>The <strong>edstan</strong> package is available on <strong>CRAN</strong>, but a more up to date version may often be found on Github. The following <strong>R</strong> code may be used to install the package from Github.</p>
<pre class="r"><code># Install edstan from Github rather than CRAN
install.packages(&quot;devtools&quot;)
devtools::install_github(&quot;danielcfurr/edstan&quot;)</code></pre>
<p>The following <strong>R</strong> code loads the necessary packages and then sets some <strong>rstan</strong> options, which causes the compiled <strong>Stan</strong> model to be saved for future use and the MCMC chains to be executed in parallel.</p>
<pre class="r"><code># Load R packages
library(rstan)
library(edstan)
library(ggplot2)
library(TAM)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())</code></pre>
<p>The case study uses <strong>R</strong> version 3.3.3, <strong>rstan</strong> version 2.15.1, <strong>ggplot2</strong> version 2.2.1, and <strong>edstan</strong> version 1.0.7. Also, the example data are from <strong>TAM</strong> version 2.3.18. Readers may wish to check the versions for their installed packages using the <code>packageVersion()</code> function.</p>
<div id="rasch-model-with-latent-regression" class="section level1">
<h1><span class="header-section-number">1</span> Rasch model with latent regression</h1>
<div id="overview-of-the-model" class="section level2">
<h2><span class="header-section-number">1.1</span> Overview of the model</h2>
<p>The Rasch model <span class="citation">(Rasch 1960)</span> is an item response theory model for dichotomous items. The version presented includes a latent regression, in which the person abilities are regressed on person characteristics. However, the latent regression part of the model may be restricted to an intercept only, resulting in a regular Rasch model.</p>
<p><span class="math display">\[ 
\mathrm{logit} [ \Pr(y_{ij} = 1 | \theta_j, \beta_i) ] = 
  \theta_j - \beta_i
\]</span> <span class="math display">\[
  \theta_j \sim \mathrm{N}(w_j&#39; \lambda, \sigma^2)
\]</span></p>
<p>Variables:</p>
<ul>
<li><span class="math inline">\(i = 1 \ldots I\)</span> indexes items.</li>
<li><span class="math inline">\(j = 1 \ldots J\)</span> indexes persons.</li>
<li><span class="math inline">\(y_{ij} \in \{ 0, 1 \}\)</span> is the response of person <span class="math inline">\(j\)</span> to item <span class="math inline">\(i\)</span>.</li>
<li><span class="math inline">\(w_{j}\)</span> is the vector of covariates for person <span class="math inline">\(j\)</span>, the first element of which <em>must</em> equal one for a model intercept. <span class="math inline">\(w_{j}\)</span> may be assembled into a <span class="math inline">\(J\)</span>-by-<span class="math inline">\(K\)</span> covariate matrix <span class="math inline">\(W\)</span>, where <span class="math inline">\(K\)</span> is number of elements in <span class="math inline">\(w_j\)</span>.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><span class="math inline">\(\beta_i\)</span> is the difficulty for item <span class="math inline">\(i\)</span>.</li>
<li><span class="math inline">\(\theta_j\)</span> is the ability for person <span class="math inline">\(j\)</span>.</li>
<li><span class="math inline">\(\lambda\)</span> is the vector of latent regression parameters of length <span class="math inline">\(K\)</span>.</li>
<li><span class="math inline">\(\sigma^2\)</span> is the variance for the ability distribution.</li>
</ul>
<p>Constraints:</p>
<ul>
<li>The last item difficulty is constrained to be the negative sum of the other difficulties, <span class="math inline">\(\beta_I = -\sum_{i}^{(I-1)} \beta_i\)</span>, resulting in the average item difficulty being zero.</li>
</ul>
<p>Priors:</p>
<ul>
<li><span class="math inline">\(\sigma \sim \mathrm{Exp}(.1)\)</span> is weakly informative for the person standard deviation.</li>
<li><span class="math inline">\(\beta_i \sim \mathrm{N}(0, 9)\)</span> is also weakly informative.</li>
<li><span class="math inline">\(\lambda \sim t_3(0, 1)\)</span>, where <span class="math inline">\(t_3\)</span> is the Student’s <span class="math inline">\(t\)</span> distribution with three degrees of freedom, <em>and</em> the covariates have been transformed as follows: (1) continuous covariates are mean-centered and then divided by two times their standard deviations, (2) binary covariates are mean-centered and divided their maximum minus minimum values, and (3) no change is made to the constant, set to one, for the model intercept. This approach to setting priors is similar to one that has been suggested for logistic regression <span class="citation">(Gelman et al. 2008)</span>. It is possible to adjust the coefficients back to the scales of the original covariates.</li>
</ul>
</div>
<div id="stan-code-for-a-simple-rasch-model" class="section level2">
<h2><span class="header-section-number">1.2</span> <strong>Stan</strong> code for a simple Rasch model</h2>
<p>A simple <strong>Stan</strong> model is described before discussing the complete model, as the code for the complete model is somewhat cumbersome. The simpler model, printed below, omits the latent regression and so does not require rescaling of the person covariates or <code>lambda</code>. The mean of the person distribution is set to zero and the constraint is removed from the item difficulties, which also differs from the complete model.</p>
<pre class="r"><code># Print the simple Rasch model from the edstan package
simple_rasch_file &lt;- system.file(&quot;extdata/rasch_simple.stan&quot;, 
                                 package = &quot;edstan&quot;)
cat(readLines(simple_rasch_file), sep = &quot;\n&quot;)</code></pre>
<pre><code>data {
  int&lt;lower=1&gt; I;               // # questions
  int&lt;lower=1&gt; J;               // # persons
  int&lt;lower=1&gt; N;               // # observations
  int&lt;lower=1, upper=I&gt; ii[N];  // question for n
  int&lt;lower=1, upper=J&gt; jj[N];  // person for n
  int&lt;lower=0, upper=1&gt; y[N];   // correctness for n
}
parameters {
  vector[I] beta;
  vector[J] theta;
  real&lt;lower=1&gt; sigma;
}
model {
  beta ~ normal(0, 3);
  theta ~ normal(0, sigma);
  sigma ~ exponential(.1);
  y ~ bernoulli_logit(theta[jj] - beta[ii]);
}</code></pre>
<p>Data are fed into the model in vector form. That is, <code>y</code> is a long vector of scored item responses, and <code>ii</code> and <code>jj</code> indicate with which item and person each element in <code>y</code> is associated. These three vectors are of length <code>N</code>, which is equal to <code>I</code> times <code>J</code> if there are no missing responses. Parameters <code>beta</code>, <code>theta</code>, and <code>sigma</code> are declared in the parameters block, and priors for these are set in the model block. The likelihood for <code>y</code> in the last line uses vectorization by indexing <code>theta</code> and <code>beta</code> with <code>jj</code> and <code>ii</code>, which is more efficient than using a loop.</p>
</div>
<div id="stan-code-for-the-rasch-model-with-latent-regression" class="section level2">
<h2><span class="header-section-number">1.3</span> <strong>Stan</strong> code for the Rasch model with latent regression</h2>
<p>The Rasch model with latent regression, which is featured in <strong>edstan</strong>, is printed below. It is more complicated than is typically necessary for a <strong>Stan</strong> model because it is written to apply sensible priors for regression parameters associated with arbitrarily scaled covariates.</p>
<pre class="r"><code># Print the latent regression Rasch model from the edstan package
rasch_latreg_file &lt;- system.file(&quot;extdata/rasch_latent_reg.stan&quot;, 
                                 package = &quot;edstan&quot;)
cat(readLines(rasch_latreg_file), sep = &quot;\n&quot;)</code></pre>
<pre><code>functions {
  matrix obtain_adjustments(matrix W) {
    real min_w;
    real max_w;
    int minmax_count;
    matrix[2, cols(W)] adj;
    adj[1, 1] = 0;
    adj[2, 1] = 1;
    if(cols(W) &gt; 1) {
      for(k in 2:cols(W)) {                       // remaining columns
        min_w = min(W[1:rows(W), k]);
        max_w = max(W[1:rows(W), k]);
        minmax_count = 0;
        for(j in 1:rows(W))
          minmax_count = minmax_count + W[j,k] == min_w || W[j,k] == max_w;
        if(minmax_count == rows(W)) {       // if column takes only 2 values
          adj[1, k] = mean(W[1:rows(W), k]);
          adj[2, k] = (max_w - min_w);
        } else {                            // if column takes &gt; 2 values
          adj[1, k] = mean(W[1:rows(W), k]);
          adj[2, k] = sd(W[1:rows(W), k]) * 2;
        }
      }
    }
    return adj;
  }
}
data {
  int&lt;lower=1&gt; I;               // # questions
  int&lt;lower=1&gt; J;               // # persons
  int&lt;lower=1&gt; N;               // # observations
  int&lt;lower=1, upper=I&gt; ii[N];  // question for n
  int&lt;lower=1, upper=J&gt; jj[N];  // person for n
  int&lt;lower=0, upper=1&gt; y[N];   // correctness for n
  int&lt;lower=1&gt; K;               // # person covariates
  matrix[J,K] W;                // person covariate matrix
}
transformed data {
  matrix[2,K] adj;               // values for centering and scaling covariates
  matrix[J,K] W_adj;             // centered and scaled covariates
  adj = obtain_adjustments(W);
  for(k in 1:K) for(j in 1:J)
      W_adj[j,k] = (W[j,k] - adj[1,k]) / adj[2,k];
}
parameters {
  vector[I-1] beta_free;
  vector[J] theta;
  real&lt;lower=0&gt; sigma;
  vector[K] lambda_adj;
}
transformed parameters {
  vector[I] beta;
  beta[1:(I-1)] = beta_free;
  beta[I] = -1*sum(beta_free);
}
model {
  target += normal_lpdf(beta | 0, 3);
  theta ~ normal(W_adj*lambda_adj, sigma);
  lambda_adj ~ student_t(3, 0, 1);
  sigma ~ exponential(.1);
  y ~ bernoulli_logit(theta[jj] - beta[ii]);
}
generated quantities {
  vector[K] lambda;
  lambda[2:K] = lambda_adj[2:K] ./ to_vector(adj[2,2:K]);
  lambda[1] = W_adj[1, 1:K]*lambda_adj[1:K] - W[1, 2:K]*lambda[2:K];
}</code></pre>
<p>The <strong>Stan</strong> model begins with the creation of the <code>obtain_adjustments()</code> function, which accepts a covariate matrix and returns a matrix that contains values to be used in adjusting the covariates. The returned matrix has one column for each covariate (starting with the constant for the intercept). The first row of it provides the values used to center the covariates, and the second provides the values used to scale them. The function begins by setting the values in the first column to zero and one, which corresponds to no change to the constant. Next the function loops over the remaining columns of the covariate matrix and determines whether the covariate is binary or continuous. This determination is made by counting the number of values that are equal to either the maximum or minimum for a given coavariate; if this count equals <span class="math inline">\(J\)</span>, then the covariate must be binary. Based on this determination the appropriate adjustments for the covariates are calculated and then added to the returned matrix.</p>
<p>In the transformed data block, the <code>obtain_adjustments()</code> function is called and the results are stored in <code>adj</code>. Then a double loop is used to assign adjusted covariate values to <code>W_adj</code> using <code>W</code> and <code>adj</code>. The latent regression is carried out in the model block in the declaration of the prior for <code>theta</code>, based on <code>W_adj</code> and <code>lambda_adj</code>. (This approach is referred to as hierarchical centering and tends to be more efficient when there is a large amount of data. The alternative is a “decentered” approach in which the prior mean for <code>theta</code> would be set to zero, and then <code>W*lambda</code> would be added to <code>theta</code> in the likelihood statement.)</p>
<p>The generated quantities block is used to calculate what the regression coefficients and intercept would have been on the scales of the original covariates. For the coefficients this is determined simply by dividing the coefficients by the same value of <code>spread</code> used to modify the scale the original covariates. The intercept given the original scale is then recovered with some algebra. The <code>obtain_adjustments()</code> function and related code for adjusting the covariates and regression coefficients is used in the same way across <strong>edstan</strong> models.</p>
<p>There are a few other differences from the simple model. In the data block, the number of covariates (plus the intercept) <code>K</code> is now required, as is the matrix of covariates <code>W</code>. The first column of <code>W</code> must have all elements equal to one. Also, the unconstrained item parameters are contained in <code>beta_free</code>, which is why it has a length of <span class="math inline">\(I-1\)</span>. In the transformed parameters block, <code>beta</code> is created by appending the constrained item difficulty to <code>beta_free</code>. The <code>target += ...</code> syntax for the prior on <code>beta</code> is a manual way of incrementing the log posterior used when the prior is placed on a transformed parameter.</p>
</div>
<div id="simulation-for-parameter-recovery" class="section level2">
<h2><span class="header-section-number">1.4</span> Simulation for parameter recovery</h2>
<p>The <strong>Stan</strong> model is fit to a simulated dataset to evaluate it’s ability to recover the generating parameter values. The <strong>R</strong> code that follows simulates a dataset conforming to the model.</p>
<pre class="r"><code># Set parameters for the simulated data
I &lt;- 20
J &lt;- 500
sigma &lt;- .8
lambda &lt;- c(-10*.05, .05, .5, -.025)
w_2 &lt;- rnorm(J, 10, 5)
w_3 &lt;- rbinom(J, 1, .5)
W &lt;- cbind(1, w_2, w_3, w_2*w_3)
beta_free &lt;- seq(from = -1, to = 1, length.out = I-1)

# Calculate or sample remaining variables and parameters
N &lt;- I*J
ii &lt;- rep(1:I, times = J)
jj &lt;- rep(1:J, each = I)
beta &lt;- c(beta_free, -1 * sum(beta_free))
rasch_theta &lt;-  rnorm(J, W %*% matrix(lambda), sigma)
rasch_eta &lt;- (rasch_theta[jj] - beta[ii])
rasch_y &lt;- rbinom(N, size = 1, prob = boot::inv.logit(rasch_eta))

# Assemble the data list using an edstan function
sim_rasch_list &lt;- irt_data(y = rasch_y, ii = ii, jj = jj, 
                           covariates = as.data.frame(W), 
                           formula = NULL)</code></pre>
<p>The simulated data consists of 20 dichotomous items and 500 persons. The person covariate vectors <span class="math inline">\(w_j\)</span> include (1) a value of one for the model intercept, (2) a random draw from a normal distribution with mean of 10 and standard deviation of 5, (3) an indicator variable taking values of zero and one, and (4) an interaction between the two. These are chosen to represent a difficult case for assigning automatic priors for the latent regression coefficients. The generating coefficients <span class="math inline">\(\lambda\)</span> for the latent regression are -0.5, 0.05, 0.5, and -0.025. The generating unconstrained difficulties <span class="math inline">\(\beta_1 \cdots \beta_{19}\)</span> are equidistant values between -1 and 1, the constrained difficulty <span class="math inline">\(\beta_{20}\)</span> is equal to 0, and the abilities <span class="math inline">\(\theta\)</span> are random draws from a normal distribution with a mean generated from the latent regression and a standard deviation <span class="math inline">\(\sigma = 0.8\)</span>.</p>
<pre class="r"><code># Plot mean ability conditional on the covariates
f1 &lt;- function(x) lambda[1] + x*lambda[2]
f2 &lt;- function(x) lambda[1] + lambda[3] + x*(lambda[2] + lambda[4])
ggplot(data.frame(w2 = c(0, 20))) +
  aes(x = w2) +
  stat_function(fun = f1, color = &quot;red&quot;) +
  stat_function(fun = f2, color = &quot;blue&quot;) +
  ylab(&quot;Mean generated ability&quot;) +
  xlab(&quot;Value for continous covariate&quot;)</code></pre>
<div class="figure">
<img src="rasch_and_2pl_files/figure-html/rasch_sim_theta_plot-1.png" alt="Mean of generated abilities as a function of the continuous covariate. A line is shown separately for the two groups identified by the binary variable." width="672" />
<p class="caption">
Mean of generated abilities as a function of the continuous covariate. A line is shown separately for the two groups identified by the binary variable.
</p>
</div>
<p>The simulated dataset is next fit with <strong>Stan</strong> using <code>irt_stan()</code> from the <strong>edstan</strong> package. <code>irt_stan()</code> is merely a wrapper for <code>stan()</code> in <strong>rstan</strong>. Using 1,000 posterior draws per chain may be somewhat excessive as we are mainly interested in the posterior means of the parameters. However, as parameter recovery will be evaulated using the 2.5th and 97.5th percentiles of the posterior, the large number of posterior samples is warranted.</p>
<pre class="r"><code># Fit model to simulated data using an edstan function
sim_rasch_fit &lt;- irt_stan(sim_rasch_list, model = &quot;rasch_latent_reg.stan&quot;,
                          chains = 4, iter = 1000)</code></pre>
<pre><code>## Warning: There were 4 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See
## http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded</code></pre>
<pre><code>## Warning: Examine the pairs() plot to diagnose sampling problems</code></pre>
<p>The highest value for <span class="math inline">\(\hat R\)</span> was 1.007 for all parameters and the log posterior, suggesting that the chains have converged. The <strong>Stan</strong> model is evaluated in terms of its ability to recover the generating values of the parameters. The R code below prepares a plot in which the points indicate the difference between the posterior means and generating values for the parameters of main interest. This difference is referred to as discrepancy. The lines indicate the 95% poster intervals for the difference, defined as the 2.5th and 97.5th percentiles of the posterior draws. Ideally, (nearly) all the 95% intervals would include zero.</p>
<pre class="r"><code># Get estimated and generating values for wanted parameters
rasch_generating_values &lt;- c(beta, lambda, sigma)
rasch_estimated_values &lt;- summary(sim_rasch_fit,  
                                  pars = c(&quot;beta&quot;, &quot;lambda&quot;, &quot;sigma&quot;),
                                  probs = c(.025, .975))
rasch_estimated_values &lt;- rasch_estimated_values[[&quot;summary&quot;]]

# Make a data frame of the discrepancies
rasch_discrep &lt;- data.frame(par = rownames(rasch_estimated_values),
                            mean = rasch_estimated_values[, &quot;mean&quot;],
                            p025 = rasch_estimated_values[, &quot;2.5%&quot;],
                            p975 = rasch_estimated_values[, &quot;97.5%&quot;],
                            gen = rasch_generating_values)
rasch_discrep$par &lt;- with(rasch_discrep, factor(par, rev(par)))
rasch_discrep$lower &lt;- with(rasch_discrep, p025 - gen)
rasch_discrep$middle &lt;- with(rasch_discrep, mean - gen)
rasch_discrep$upper &lt;- with(rasch_discrep, p975 - gen)

# Plot the discrepancies
ggplot(rasch_discrep) +
  aes(x = par, y = middle, ymin = lower, ymax = upper) +
  scale_x_discrete() +
  labs(y = &quot;Discrepancy&quot;, x = NULL) +
  geom_abline(intercept = 0, slope = 0, color = &quot;white&quot;) +
  geom_linerange() +
  geom_point(size = 2) +
  theme(panel.grid = element_blank()) +
  coord_flip()</code></pre>
<div class="figure">
<img src="rasch_and_2pl_files/figure-html/sim_rasch_dif-1.png" alt="Discrepancies between estimated and generating parameters. Points indicate the difference between the posterior means and generating values for a parameter, and horizontal lines indicate 95% posterior intervals for the difference. Most of the discrepancies are about zero, indicating that **Stan** successfully recovers the true parameters." width="672" />
<p class="caption">
Discrepancies between estimated and generating parameters. Points indicate the difference between the posterior means and generating values for a parameter, and horizontal lines indicate 95% posterior intervals for the difference. Most of the discrepancies are about zero, indicating that <strong>Stan</strong> successfully recovers the true parameters.
</p>
</div>
</div>
</div>
<div id="two-parameter-logistic-model-with-latent-regression" class="section level1">
<h1><span class="header-section-number">2</span> Two-parameter logistic model with latent regression</h1>
<div id="overview-of-the-model-1" class="section level2">
<h2><span class="header-section-number">2.1</span> Overview of the model</h2>
<p>The two-parameter logistic model (2PL) <span class="citation">(Swaminathan and Gifford 1985)</span> is an item response theory model that includes parameters for both the difficulty and discrimination of dichotomous items. The version presented includes a latent regression. However, the latent regression part of the model may be restricted to an intercept only, resulting in a regular 2PL.</p>
<p><span class="math display">\[
  \mathrm{logit} [ \Pr(y_{ij} = 1 | \alpha_i, \beta_i, \theta_j) ] =
  \alpha_i \theta_j - \beta_i
\]</span> <span class="math display">\[
  \theta_j \sim \mathrm{N}(w_j&#39; \lambda, 1)
\]</span></p>
<p>Many aspects of the 2PL are similar to the Rasch model described earlier. Parameters <span class="math inline">\(\beta_i\)</span>, <span class="math inline">\(\theta_j\)</span>, and <span class="math inline">\(\lambda\)</span> have the same interpretation, but the 2PL adds a discrimination parameter <span class="math inline">\(\alpha_i\)</span> and constrains the variance of <span class="math inline">\(\theta_j\)</span> to one. The prior <span class="math inline">\(\alpha_i \sim \mathrm{log~N}(1, 1)\)</span> is added, which is weakly informative but assumes positive discriminations. The same priors are placed on <span class="math inline">\(\beta_i\)</span> and <span class="math inline">\(\lambda\)</span>, and the same constraint is placed on <span class="math inline">\(\beta_I\)</span>.</p>
</div>
<div id="stan-code-for-the-2pl-with-latent-regression" class="section level2">
<h2><span class="header-section-number">2.2</span> <strong>Stan</strong> code for the 2PL with latent regression</h2>
<p>The <strong>Stan</strong> code for the 2PL is similar to that for the Rasch model except for the addition of the discrimination parameters.</p>
<pre class="r"><code># Print the latent regression 2PL model from the edstan package
twopl_latreg_file &lt;- system.file(&quot;extdata/2pl_latent_reg.stan&quot;, 
                                 package = &quot;edstan&quot;)
cat(readLines(twopl_latreg_file), sep = &quot;\n&quot;)</code></pre>
<pre><code>functions {
  matrix obtain_adjustments(matrix W) {
    real min_w;
    real max_w;
    int minmax_count;
    matrix[2, cols(W)] adj;
    adj[1, 1] = 0;
    adj[2, 1] = 1;
    if(cols(W) &gt; 1) {
      for(k in 2:cols(W)) {                       // remaining columns
        min_w = min(W[1:rows(W), k]);
        max_w = max(W[1:rows(W), k]);
        minmax_count = 0;
        for(j in 1:rows(W))
          minmax_count = minmax_count + W[j,k] == min_w || W[j,k] == max_w;
        if(minmax_count == rows(W)) {       // if column takes only 2 values
          adj[1, k] = mean(W[1:rows(W), k]);
          adj[2, k] = (max_w - min_w);
        } else {                            // if column takes &gt; 2 values
          adj[1, k] = mean(W[1:rows(W), k]);
          adj[2, k] = sd(W[1:rows(W), k]) * 2;
        }
      }
    }
    return adj;
  }
}
data {
  int&lt;lower=1&gt; I;               // # questions
  int&lt;lower=1&gt; J;               // # persons
  int&lt;lower=1&gt; N;               // # observations
  int&lt;lower=1, upper=I&gt; ii[N];  // question for n
  int&lt;lower=1, upper=J&gt; jj[N];  // person for n
  int&lt;lower=0, upper=1&gt; y[N];   // correctness for n
  int&lt;lower=1&gt; K;               // # person covariates
  matrix[J,K] W;                // person covariate matrix
}
transformed data {
  matrix[2,K] adj;               // values for centering and scaling covariates
  matrix[J,K] W_adj;             // centered and scaled covariates
  adj = obtain_adjustments(W);
  for(k in 1:K) for(j in 1:J)
      W_adj[j,k] = (W[j,k] - adj[1,k]) / adj[2,k];
}
parameters {
  vector&lt;lower=0&gt;[I] alpha;
  vector[I-1] beta_free;
  vector[J] theta;
  vector[K] lambda_adj;
}
transformed parameters {
  vector[I] beta;
  beta[1:(I-1)] = beta_free;
  beta[I] = -1*sum(beta_free);
}
model {
  alpha ~ lognormal(1, 1);
  target += normal_lpdf(beta | 0, 3);
  lambda_adj ~ student_t(3, 0, 1);
  theta ~ normal(W_adj*lambda_adj, 1);
  y ~ bernoulli_logit(alpha[ii].*theta[jj] - beta[ii]);
}
generated quantities {
  vector[K] lambda;
  lambda[2:K] = lambda_adj[2:K] ./ to_vector(adj[2,2:K]);
  lambda[1] = W_adj[1, 1:K]*lambda_adj[1:K] - W[1, 2:K]*lambda[2:K];
}</code></pre>
</div>
<div id="simulation-for-parameter-recovery-1" class="section level2">
<h2><span class="header-section-number">2.3</span> Simulation for parameter recovery</h2>
<p>The <strong>Stan</strong> model is fit to a simulated dataset to evaluate it’s ability to recover the generating parameter values. The <strong>R</strong> code that follows simulates a dataset conforming to the model. The item difficulties and some other elements are borrowed from the Rasch model simulation.</p>
<pre class="r"><code># Set alpha, and otherwise use parameters from the previous simulation
alpha &lt;- rep(c(.8, 1, 1.2, 1.4),  length.out = I)

# Calculate or sample remaining variables and parameters where needed
twopl_theta &lt;-  rnorm(J, W %*% matrix(lambda), 1)
twopl_eta &lt;- alpha[ii]*twopl_theta[jj] - beta[ii]
twopl_y &lt;- rbinom(N, size = 1, prob = boot::inv.logit(twopl_eta))

# Assemble the data list using an edstan function
sim_2pl_list &lt;- irt_data(y = twopl_y, ii = ii, jj = jj, 
                         covariates = as.data.frame(W), 
                         formula = NULL)</code></pre>
<p>The simulated dataset is next fit with <strong>Stan</strong> using <code>irt_stan()</code> from the <strong>edstan</strong> package.</p>
<pre class="r"><code># Fit model to simulated data using an edstan function
sim_2pl_fit &lt;- irt_stan(sim_2pl_list, model = &quot;2pl_latent_reg.stan&quot;,
                        chains = 4, iter = 1000)</code></pre>
<p>The highest value for <span class="math inline">\(\hat R\)</span> was 1.006 for all parameters and the log posterior. The <strong>Stan</strong> model is evaluated in terms of its ability to recover the generating values of the parameters. The R code below prepares a plot in which the points indicate the difference between the posterior means and generating values for the parameters of main interest. This difference is referred to as discrepancy. The lines indicate the 95% poster intervals for the difference, defined as the 2.5th and 97.5th percentiles of the posterior draws. Ideally, (nearly) all the 95% intervals would include zero.</p>
<pre class="r"><code># Get estimated and generating values for wanted parameters
twopl_generating_values &lt;- c(alpha, beta, lambda)
twopl_estimated_values &lt;- summary(sim_2pl_fit,  
                                  pars = c(&quot;alpha&quot;, &quot;beta&quot;, &quot;lambda&quot;),
                                  probs = c(.025, .975))
twopl_estimated_values &lt;- twopl_estimated_values[[&quot;summary&quot;]]

# Make a data frame of the discrepancies
twopl_discrep &lt;- data.frame(par = rownames(twopl_estimated_values),
                            mean = twopl_estimated_values[, &quot;mean&quot;],
                            p025 = twopl_estimated_values[, &quot;2.5%&quot;],
                            p975 = twopl_estimated_values[, &quot;97.5%&quot;],
                            gen = twopl_generating_values)
twopl_discrep$par &lt;- with(twopl_discrep, factor(par, rev(par)))
twopl_discrep$lower &lt;- with(twopl_discrep, p025 - gen)
twopl_discrep$middle &lt;- with(twopl_discrep, mean - gen)
twopl_discrep$upper &lt;- with(twopl_discrep, p975 - gen)

# Plot the discrepancies
ggplot(twopl_discrep) +
  aes(x = par, y = middle, ymin = lower, ymax = upper) +
  scale_x_discrete() +
  labs(y = &quot;Discrepancy&quot;, x = NULL) +
  geom_abline(intercept = 0, slope = 0, color = &quot;white&quot;) +
  geom_linerange() +
  geom_point(size = 2) +
  theme(panel.grid = element_blank()) +
  coord_flip()</code></pre>
<div class="figure">
<img src="rasch_and_2pl_files/figure-html/sim_2pl_dif-1.png" alt="Discrepancies between estimated and generating parameters. Points indicate the difference between the posterior means and generating values for a parameter, and horizontal lines indicate 95% posterior intervals for the difference. Most of the discrepancies are about zero, indicating that **Stan** successfully recovers the true parameters." width="672" />
<p class="caption">
Discrepancies between estimated and generating parameters. Points indicate the difference between the posterior means and generating values for a parameter, and horizontal lines indicate 95% posterior intervals for the difference. Most of the discrepancies are about zero, indicating that <strong>Stan</strong> successfully recovers the true parameters.
</p>
</div>
</div>
</div>
<div id="example-application" class="section level1">
<h1><span class="header-section-number">3</span> Example application</h1>
<div id="data" class="section level2">
<h2><span class="header-section-number">3.1</span> Data</h2>
<p>The example data are from the The First International Mathematics Study <span class="citation">(Husen and others 1967; Postlethwaite 1967)</span>. The data include information about student gender and country (Australia or Japan). For convenience, only a subset of the full data are used.</p>
<pre class="r"><code># Attach the example dataset. The TAM package is required.
data(data.fims.Aus.Jpn.scored, package = &quot;TAM&quot;)

# Subset the full data
select &lt;- floor(seq(from = 1, to = nrow(data.fims.Aus.Jpn.scored),
                    length.out = 500))
subsetted_df &lt;- data.fims.Aus.Jpn.scored[select, ]
str(subsetted_df)</code></pre>
<pre><code>## &#39;data.frame&#39;:    500 obs. of  16 variables:
##  $ SEX    : int  1 1 2 2 1 1 2 2 2 1 ...
##  $ M1PTI1 : num  1 1 1 0 0 1 1 1 1 1 ...
##  $ M1PTI2 : num  0 0 0 0 0 1 1 1 1 1 ...
##  $ M1PTI3 : num  1 1 1 0 1 1 0 1 1 0 ...
##  $ M1PTI6 : num  1 0 0 1 0 0 0 1 1 0 ...
##  $ M1PTI7 : num  0 0 0 0 0 1 0 0 0 0 ...
##  $ M1PTI11: num  1 0 0 0 0 0 1 1 1 0 ...
##  $ M1PTI12: num  0 0 0 0 0 1 0 0 1 1 ...
##  $ M1PTI14: num  0 0 1 0 0 1 0 1 1 1 ...
##  $ M1PTI17: num  1 0 0 0 0 1 0 1 1 0 ...
##  $ M1PTI18: num  0 0 1 1 0 0 1 1 0 1 ...
##  $ M1PTI19: num  0 0 0 0 0 1 0 0 0 0 ...
##  $ M1PTI21: num  0 0 0 0 0 0 0 0 0 0 ...
##  $ M1PTI22: num  0 0 0 0 0 1 0 0 0 0 ...
##  $ M1PTI23: num  1 1 1 1 0 1 1 1 0 0 ...
##  $ country: int  1 1 1 1 1 1 1 1 1 1 ...</code></pre>
<p>The dataset is next divided into an item response matrix and a matrix of student covariates.</p>
<pre class="r"><code># Extract the response matrix
response_matrix &lt;- as.matrix(subsetted_df[, grepl(&quot;^M1&quot;, names(subsetted_df))])
dim(response_matrix)</code></pre>
<pre><code>## [1] 500  14</code></pre>
<pre class="r"><code># Set up a data frame of person covariates
covariates &lt;- data.frame(male = as.numeric(subsetted_df$SEX == 2),
                         japan = as.numeric(subsetted_df$country == 2))
table(covariates)</code></pre>
<pre><code>##     japan
## male   0   1
##    0 181  80
##    1 158  81</code></pre>
<p>500 students responded to 4 dichotomously scored items. The data contain no missing values. The two matrices are converted to a list suitable for the <strong>Stan</strong> model.</p>
<pre class="r"><code># Assemble the data list using an edstan function
ex_list &lt;- irt_data(response_matrix = response_matrix, 
                    covariates = covariates, 
                    formula = ~ male*japan)</code></pre>
</div>
<div id="rasch-model-results" class="section level2">
<h2><span class="header-section-number">3.2</span> Rasch model results</h2>
<p>The Rasch model is fit to the data list.</p>
<pre class="r"><code># Fit the Rasch model model using an edstan function
ex_rasch_fit &lt;- irt_stan(ex_list, model = &quot;rasch_latent_reg.stan&quot;, 
                         chains = 4, iter = 300)</code></pre>
<p>As discussed above, convergence of the chains is assessed for every parameter, and also the log posterior density, using <span class="math inline">\(\hat{R}\)</span>.</p>
<pre class="r"><code># Plot of convergence statistics using an edstan function
stan_columns_plot(ex_rasch_fit)</code></pre>
<div class="figure">
<img src="rasch_and_2pl_files/figure-html/example_rasch_converge-1.png" alt="Convergence statistics ($\hat{R}$) by parameter for the example. All values should be less than 1.1 to infer convergence. Horizontal jitter is applied to the points." width="672" />
<p class="caption">
Convergence statistics (<span class="math inline">\(\hat{R}\)</span>) by parameter for the example. All values should be less than 1.1 to infer convergence. Horizontal jitter is applied to the points.
</p>
</div>
<p>Next we view summaries of the parameter posteriors.</p>
<pre class="r"><code># View table of parameter posteriors using an edstan function
print_irt_stan(ex_rasch_fit)</code></pre>
<pre><code>## Inference for Stan model: rasch_latent_reg.
## 4 chains, each with iter=300; warmup=150; thin=1; 
## post-warmup draws per chain=150, total post-warmup draws=600.
## 
##            mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat
## beta[1]   -1.63    0.00 0.12 -1.84 -1.70 -1.63 -1.56 -1.40   600 1.00
## beta[2]   -1.42    0.00 0.11 -1.62 -1.50 -1.42 -1.35 -1.21   600 1.00
## beta[3]   -2.03    0.01 0.14 -2.29 -2.13 -2.04 -1.94 -1.77   301 1.00
## beta[4]   -0.43    0.00 0.10 -0.64 -0.50 -0.43 -0.36 -0.25   600 1.00
## beta[5]    1.85    0.01 0.12  1.60  1.76  1.84  1.94  2.11   402 1.00
## beta[6]   -1.59    0.01 0.12 -1.82 -1.67 -1.59 -1.51 -1.36   503 1.00
## beta[7]    0.83    0.00 0.10  0.64  0.76  0.82  0.90  1.03   600 1.00
## beta[8]    0.35    0.00 0.09  0.17  0.29  0.35  0.41  0.53   501 1.00
## beta[9]    1.06    0.00 0.11  0.85  0.99  1.06  1.14  1.27   600 1.00
## beta[10]  -0.50    0.00 0.10 -0.68 -0.56 -0.49 -0.43 -0.32   600 1.00
## beta[11]   1.60    0.01 0.12  1.36  1.52  1.60  1.68  1.83   503 1.00
## beta[12]   1.30    0.00 0.10  1.11  1.23  1.30  1.37  1.50   600 1.00
## beta[13]   1.75    0.01 0.12  1.51  1.66  1.76  1.84  2.00   600 1.00
## beta[14]  -1.14    0.00 0.11 -1.35 -1.21 -1.14 -1.07 -0.93   521 1.00
## lambda[1] -0.37    0.00 0.08 -0.53 -0.42 -0.37 -0.31 -0.21   319 1.00
## lambda[2]  0.09    0.01 0.12 -0.14  0.01  0.10  0.17  0.32   439 1.00
## lambda[3]  1.04    0.01 0.15  0.74  0.94  1.04  1.15  1.34   353 1.00
## lambda[4] -0.26    0.01 0.21 -0.67 -0.40 -0.27 -0.12  0.14   394 1.00
## sigma      0.89    0.00 0.04  0.81  0.86  0.89  0.92  0.98   213 1.01
## 
## Samples were drawn using NUTS(diag_e) at Wed Jun 28 14:31:28 2017.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
<p>A Rasch model without the latent regression could be fit by changing the person covariate matrix to include only an intercept term. Shown below is how this may be done for the example data.</p>
<pre class="r"><code># Fit the example data without latent regression
noreg_list &lt;- ex_list
noreg_list$W &lt;- matrix(1, nrow = ex_list$J, ncol = 1)
noreg_list$K &lt;- 1
noreg_fit &lt;- stan(file = &quot;rasch_latent_reg.stan&quot;, 
                  data = noreg_list, chains = 4, iter = 300)</code></pre>
</div>
<div id="two-parameter-logistic-model-results" class="section level2">
<h2><span class="header-section-number">3.3</span> Two parameter logistic model results</h2>
<p>The 2PL is fit to the data list.</p>
<pre class="r"><code># Fit the 2PL using an edstan function
ex_2pl_fit &lt;- irt_stan(ex_list, model = &quot;2pl_latent_reg.stan&quot;, 
                       chains = 4, iter = 300)</code></pre>
<p>As discussed above, convergence of the chains is assessed for every parameter, and also the log posterior density, using <span class="math inline">\(\hat{R}\)</span>.</p>
<pre class="r"><code># Plot of convergence statistics using an edstan function
stan_columns_plot(ex_2pl_fit)</code></pre>
<div class="figure">
<img src="rasch_and_2pl_files/figure-html/example_2pl_converge-1.png" alt="Convergence statistics ($\hat{R}$) by parameter for the example. All values should be less than 1.1 to infer convergence. Horizontal jitter is applied to the points." width="672" />
<p class="caption">
Convergence statistics (<span class="math inline">\(\hat{R}\)</span>) by parameter for the example. All values should be less than 1.1 to infer convergence. Horizontal jitter is applied to the points.
</p>
</div>
<p>Next we view summaries of the parameter posteriors.</p>
<pre class="r"><code># View table of parameter posteriors using an edstan function
print_irt_stan(ex_2pl_fit, ex_list)</code></pre>
<pre><code>## Inference for Stan model: 2pl_latent_reg.
## 4 chains, each with iter=300; warmup=150; thin=1; 
## post-warmup draws per chain=150, total post-warmup draws=600.
##   
##              mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat
## Item 1: M1PTI1
##   alpha[1]   0.71    0.01 0.14  0.46  0.61  0.71  0.80  0.99   600 1.00
##   beta[1]   -1.55    0.01 0.13 -1.83 -1.64 -1.54 -1.46 -1.30   600 1.00
## Item 2: M1PTI2
##   alpha[2]   1.49    0.01 0.24  1.06  1.33  1.48  1.64  1.97   411 1.00
##   beta[2]   -1.79    0.01 0.18 -2.13 -1.91 -1.79 -1.66 -1.47   600 1.00
## Item 3: M1PTI3
##   alpha[3]   1.16    0.01 0.20  0.79  1.03  1.15  1.28  1.57   600 1.00
##   beta[3]   -2.24    0.01 0.19 -2.62 -2.37 -2.22 -2.10 -1.90   600 1.00
## Item 4: M1PTI6
##   alpha[4]   1.24    0.01 0.17  0.94  1.12  1.22  1.34  1.63   600 1.01
##   beta[4]   -0.54    0.00 0.10 -0.75 -0.62 -0.54 -0.47 -0.35   600 1.00
## Item 5: M1PTI7
##   alpha[5]   1.87    0.02 0.27  1.38  1.69  1.84  2.03  2.45   297 1.00
##   beta[5]    2.44    0.01 0.22  2.02  2.29  2.43  2.58  2.90   600 1.00
## Item 6: M1PTI11
##   alpha[6]   1.42    0.01 0.24  0.96  1.26  1.39  1.57  1.92   474 1.00
##   beta[6]   -1.94    0.01 0.20 -2.36 -2.07 -1.93 -1.79 -1.59   600 1.00
## Item 7: M1PTI12
##   alpha[7]   0.48    0.00 0.10  0.29  0.41  0.47  0.54  0.68   600 1.00
##   beta[7]    0.73    0.00 0.09  0.55  0.67  0.73  0.79  0.93   600 1.00
## Item 8: M1PTI14
##   alpha[8]   0.31    0.00 0.09  0.14  0.26  0.31  0.36  0.50   600 1.00
##   beta[8]    0.31    0.00 0.10  0.10  0.24  0.31  0.37  0.52   600 1.00
## Item 9: M1PTI17
##   alpha[9]   1.22    0.01 0.17  0.92  1.10  1.21  1.33  1.57   461 1.00
##   beta[9]    1.13    0.01 0.13  0.87  1.04  1.12  1.22  1.38   600 1.00
## Item 10: M1PTI18
##   alpha[10]  0.66    0.00 0.12  0.44  0.58  0.66  0.74  0.92   600 1.00
##   beta[10]  -0.47    0.00 0.11 -0.68 -0.54 -0.47 -0.40 -0.26   600 1.00
## Item 11: M1PTI19
##   alpha[11]  2.16    0.02 0.36  1.63  1.92  2.12  2.38  3.02   267 1.01
##   beta[11]   2.30    0.01 0.24  1.92  2.13  2.27  2.45  2.85   600 1.00
## Item 12: M1PTI21
##   alpha[12]  0.18    0.00 0.06  0.07  0.13  0.17  0.22  0.31   600 1.00
##   beta[12]   1.11    0.00 0.11  0.90  1.03  1.11  1.19  1.32   600 1.00
## Item 13: M1PTI22
##   alpha[13]  1.28    0.01 0.17  0.96  1.17  1.28  1.40  1.65   600 1.00
##   beta[13]   1.93    0.01 0.16  1.62  1.82  1.92  2.04  2.25   600 1.00
## Item 14: M1PTI23
##   alpha[14]  1.42    0.01 0.21  1.07  1.27  1.41  1.56  1.87   507 1.00
##   beta[14]  -1.42    0.01 0.15 -1.75 -1.52 -1.41 -1.31 -1.14   600 1.00
## Ability distribution
##   lambda[1] -0.41    0.00 0.09 -0.58 -0.48 -0.42 -0.35 -0.24   442 1.00
##   lambda[2]  0.05    0.01 0.12 -0.20 -0.03  0.06  0.14  0.29   600 1.00
##   lambda[3]  1.16    0.01 0.16  0.88  1.06  1.15  1.26  1.47   600 1.00
##   lambda[4] -0.26    0.01 0.22 -0.70 -0.41 -0.26 -0.13  0.17   600 0.99
##   
## Samples were drawn using NUTS(diag_e) at Wed Jun 28 14:31:56 2017.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
</div>
</div>
<div id="references" class="section level1">
<h1><span class="header-section-number">4</span> References</h1>
<!-- This comment causes section to be numbered -->
<div id="refs" class="references">
<div id="ref-gelman2008weakly">
<p>Gelman, Andrew, Aleks Jakulin, Maria Grazia Pittau, and Yu-Sung Su. 2008. “A Weakly Informative Default Prior Distribution for Logistic and Other Regression Models.” <em>The Annals of Applied Statistics</em>. JSTOR, 1360–83.</p>
</div>
<div id="ref-husen1967international">
<p>Husen, Torsten, and others. 1967. “International Study of Achievement in Mathematics, a Comparison of Twelve Countries, Volume I.” ERIC.</p>
</div>
<div id="ref-postlethwaite1967school">
<p>Postlethwaite, Neville. 1967. <em>School Organization and Student Achievement</em>. Stockholm: Almqvist &amp; Wiksell.</p>
</div>
<div id="ref-Rasch1960a">
<p>Rasch, George. 1960. <em>Probabilistic Models for Some Intelligence and Attainment Tests</em>. Copenhagen: Danish Institute for Educational Research.</p>
</div>
<div id="ref-swaminathan1985bayesian">
<p>Swaminathan, Hariharan, and Janice A Gifford. 1985. “Bayesian Estimation in the Two-Parameter Logistic Model.” <em>Psychometrika</em> 50 (3). Springer: 349–64.</p>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
