<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Daniel C. Furr" />

<meta name="date" content="2017-06-28" />

<title>Partial credit and generalized partial credit models with latent regression</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="site_libs/highlight/default.css"
      type="text/css" />
<script src="site_libs/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="pcm_and_gpcm_files/styles.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Partial credit and generalized partial credit models with latent regression</h1>
<h4 class="author"><em>Daniel C. Furr</em></h4>
<h4 class="date"><em>June 28, 2017</em></h4>

</div>

<div id="TOC">
<ul>
<li><a href="#partial-credit-model-with-latent-regression"><span class="toc-section-number">1</span> Partial credit model with latent regression</a><ul>
<li><a href="#overview-of-the-model"><span class="toc-section-number">1.1</span> Overview of the model</a></li>
<li><a href="#stan-code-for-a-simple-partial-credit-model"><span class="toc-section-number">1.2</span> <strong>Stan</strong> code for a simple partial credit model</a></li>
<li><a href="#stan-code-for-the-partial-credit-model-with-latent-regression"><span class="toc-section-number">1.3</span> <strong>Stan</strong> code for the partial credit model with latent regression</a></li>
<li><a href="#simulation-for-parameter-recovery"><span class="toc-section-number">1.4</span> Simulation for parameter recovery</a></li>
</ul></li>
<li><a href="#generalized-partial-credit-model-with-latent-regression"><span class="toc-section-number">2</span> Generalized partial credit model with latent regression</a><ul>
<li><a href="#overview-of-the-model-1"><span class="toc-section-number">2.1</span> Overview of the model</a></li>
<li><a href="#stan-code-for-the-generalized-partial-credit-model-with-latent-regression"><span class="toc-section-number">2.2</span> <strong>Stan</strong> code for the generalized partial credit model with latent regression</a></li>
<li><a href="#simulation-for-parameter-recovery-1"><span class="toc-section-number">2.3</span> Simulation for parameter recovery</a></li>
</ul></li>
<li><a href="#example-application"><span class="toc-section-number">3</span> Example application</a><ul>
<li><a href="#data"><span class="toc-section-number">3.1</span> Data</a></li>
<li><a href="#partial-credit-model-results"><span class="toc-section-number">3.2</span> Partial credit model results</a></li>
<li><a href="#generalized-partial-credit-model-results"><span class="toc-section-number">3.3</span> Generalized partial credit model results</a></li>
</ul></li>
<li><a href="#references"><span class="toc-section-number">4</span> References</a></li>
</ul>
</div>

<p>This case study uses <strong>Stan</strong> to fit the Partial Credit Model (PCM) and Generalized Partial Credit Model (GPCM), including a latent regression for person ability for both. Analysis is performed with <strong>R</strong>, making use of the <strong>rstan</strong> and <strong>edstan</strong> packages. <strong>rstan</strong> is the implementation of <strong>Stan</strong> for <strong>R</strong>, and <strong>edstan</strong> provides <strong>Stan</strong> models for item response theory and several convenience functions.</p>
<p>The <strong>edstan</strong> package is available on <strong>CRAN</strong>, but a more up to date version may often be found on Github. The following <strong>R</strong> code may be used to install the package from Github.</p>
<pre class="r"><code># Install edstan from Github rather than CRAN
install.packages(&quot;devtools&quot;)
devtools::install_github(&quot;danielcfurr/edstan&quot;)</code></pre>
<p>The following <strong>R</strong> code loads the necessary packages and then sets some <strong>rstan</strong> options, which causes the compiled <strong>Stan</strong> model to be saved for future use and the MCMC chains to be executed in parallel.</p>
<pre class="r"><code># Load R packages
library(rstan)
library(ggplot2)
library(edstan)
library(TAM)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())</code></pre>
<p>The case study uses <strong>R</strong> version 3.3.3, <strong>rstan</strong> version 2.15.1, <strong>ggplot2</strong> version 2.2.1, and <strong>edstan</strong> version 1.0.7. Also, the example data are from <strong>TAM</strong> version 2.3.18. Readers may wish to check the versions for their installed packages using the <code>packageVersion()</code> function.</p>
<div id="partial-credit-model-with-latent-regression" class="section level1">
<h1><span class="header-section-number">1</span> Partial credit model with latent regression</h1>
<div id="overview-of-the-model" class="section level2">
<h2><span class="header-section-number">1.1</span> Overview of the model</h2>
<p>The PCM <span class="citation">(Masters 1982)</span> is appropriate for item response data that features more than two <em>ordered</em> response categories for some or all items. The items may have differing numbers of response categories. For dichotomous items (items with exactly two response categories), the partial credit model is equivalent to the Rasch model. The version presented includes a latent regression. However, the latent regression part of the model may be restricted to an intercept only, resulting in the standard partial credit model.</p>
<p><span class="math display">\[
\Pr(Y_{ij} = y,~y &gt; 0 | \theta_j, \beta_i) =
\frac{\exp \sum_{s=1}^y (\theta_j - \beta_{is})}
     {1 + \sum_{k=1}^{m_i} \exp \sum_{s=1}^k (\theta_j - \beta_{is})}
\]</span> <span class="math display">\[
\Pr(Y_{ij} = y,~y = 0 | \theta_j, \beta_i) =
\frac{1}
     {1 + \sum_{k=1}^{m_i} \exp \sum_{s=1}^k (\theta_j - \beta_{is})}
\]</span> <span class="math display">\[
\theta_j \sim \mathrm{N}(w_{j}&#39; \lambda, \sigma^2)
\]</span></p>
<p>Variables:</p>
<ul>
<li><span class="math inline">\(i = 1 \ldots I\)</span> indexes items.</li>
<li><span class="math inline">\(j = 1 \ldots J\)</span> indexes persons.</li>
<li><span class="math inline">\(Y_{ij} \in \{ 0 \ldots m_i \}\)</span> is the response of person <span class="math inline">\(j\)</span> to item <span class="math inline">\(i\)</span></li>
<li><span class="math inline">\(m_i\)</span> is simultaneously the maximum score and number of step difficulty parameters for item <span class="math inline">\(i\)</span>.</li>
<li><span class="math inline">\(w_{j}\)</span> is the vector of covariates for person <span class="math inline">\(j\)</span>, the first element of which <em>must</em> equal one for a model intercept. <span class="math inline">\(w_{j}\)</span> may be assembled into a <span class="math inline">\(J\)</span>-by-<span class="math inline">\(K\)</span> covariate matrix <span class="math inline">\(W\)</span>, where <span class="math inline">\(K\)</span> is number of elements in <span class="math inline">\(w_j\)</span>.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><span class="math inline">\(\beta_{is}\)</span> is the <span class="math inline">\(s\)</span>-th step difficulty for item <span class="math inline">\(i\)</span>.</li>
<li><span class="math inline">\(\theta_j\)</span> is the ability for person <span class="math inline">\(j\)</span>.</li>
<li><span class="math inline">\(\lambda\)</span> is a vector of latent regression parameters of length <span class="math inline">\(K\)</span>.</li>
<li><span class="math inline">\(\sigma^2\)</span> is the variance for the ability distribution.</li>
</ul>
<p>Constraints:</p>
<ul>
<li>The last step difficulty parameter is constrained to be the negative sum of the other difficulties, resulting in the average difficulty parameter being zero.</li>
</ul>
<p>Priors:</p>
<ul>
<li><span class="math inline">\(\sigma \sim \mathrm{Exp}(.1)\)</span> is weakly informative for the person standard deviation.</li>
<li><span class="math inline">\(\beta_{is} \sim \mathrm{N}(0, 9)\)</span> is also weakly informative.</li>
<li><span class="math inline">\(\lambda \sim t_3(0, 1)\)</span>, where <span class="math inline">\(t_3\)</span> is the Student’s <span class="math inline">\(t\)</span> distribution with three degrees of freedom, <em>and</em> the covariates have been transformed as follows: (1) continuous covariates are mean-centered and then divided by two times their standard deviations, (2) binary covariates are mean-centered and divided their maximum minus minimum values, and (3) no change is made to the constant, set to one, for the model intercept. This approach to setting priors is similar to one that has been suggested for logistic regression <span class="citation">(Gelman et al. 2008)</span>. It is possible to adjust the coefficients back to the scales of the original covariates.</li>
</ul>
</div>
<div id="stan-code-for-a-simple-partial-credit-model" class="section level2">
<h2><span class="header-section-number">1.2</span> <strong>Stan</strong> code for a simple partial credit model</h2>
<p>A simple <strong>Stan</strong> model is described before discussing the complete model, as the code for the complete model is somewhat cumbersome. The simpler model, printed below, omits the latent regression and so does not require rescaling of the person covariates or <code>lambda</code>. The mean of the person distribution is set to zero and the constraint is removed from the item difficulties, which also differs from the complete model.</p>
<pre class="r"><code># Print the simple PCM from the edstan package
simple_pcm_file &lt;- system.file(&quot;extdata/pcm_simple.stan&quot;, 
                               package = &quot;edstan&quot;)
cat(readLines(simple_pcm_file), sep = &quot;\n&quot;)</code></pre>
<pre><code>functions {
  real pcm(int y, real theta, vector beta) {
    vector[rows(beta) + 1] unsummed;
    vector[rows(beta) + 1] probs;
    unsummed = append_row(rep_vector(0.0, 1), theta - beta);
    probs = softmax(cumulative_sum(unsummed));
    return categorical_lpmf(y + 1 | probs);
  }
}
data {
  int&lt;lower=1&gt; I;                // # items
  int&lt;lower=1&gt; J;                // # persons
  int&lt;lower=1&gt; N;                // # responses
  int&lt;lower=1,upper=I&gt; ii[N];    // i for n
  int&lt;lower=1,upper=J&gt; jj[N];    // j for n
  int&lt;lower=0&gt; y[N];             // response for n; y = 0, 1 ... m_i
}
transformed data {
  int m;                         // # parameters per item (same for all items)
  m = max(y);
}
parameters {
  vector[m] beta[I];
  vector[J] theta;
  real&lt;lower=0&gt; sigma;
}
model {
  for(i in 1:I)
    beta[i] ~ normal(0, 3);
  theta ~ normal(0, sigma);
  sigma ~ exponential(.1);
  for (n in 1:N)
    target += pcm(y[n], theta[jj[n]], beta[ii[n]]);
}</code></pre>
<p>The functions block includes a user-specified function <code>pcm()</code>, which accepts a response <code>y</code>, a value for <code>theta</code>, and a vector of parameters <code>beta</code> for one item. With these inputs, it returns a vector of model-predicted log probability for the response. Later, in the model block, <code>pcm()</code> is used to get the likelihood of the observed item responses.</p>
<p>Looking to the data block, data are fed into the model in vector form. That is, <code>y</code> is a long vector of scored item responses, and <code>ii</code> and <code>jj</code> indicate with which item and person each element in <code>y</code> is associated. These three vectors are of length <code>N</code>, which is either equal to <code>I</code> times <code>J</code> or less if there are missing responses. In the transformed data block, the variable <code>m</code> is created, which represents the number of steps per item.</p>
<p>In the parameters block, <code>beta</code> is declared to be <code>I</code> vectors of length <code>m</code>. Each vector in <code>beta</code> contains the step difficulties for a given item. In this simplified model, all items must have the same number of response categories. The other parameters are handled in conventional ways, with <code>sigma</code> being assigned a lower bound of zero because it is a standard deviation.</p>
<p>The model block indicates the priors and the likelihood. The prior for <code>beta</code> requires a loop because <code>beta</code> is an array rather than a vector. The likelihood manually increments the log posterior using the <code>target += ...</code> syntax.</p>
</div>
<div id="stan-code-for-the-partial-credit-model-with-latent-regression" class="section level2">
<h2><span class="header-section-number">1.3</span> <strong>Stan</strong> code for the partial credit model with latent regression</h2>
<p>The PCM with latent regression will be discussed in relation to the simpler model, and both models are equivalent when the latent regression is restricted to an intercept only. The model with latent regression, which is featured in <strong>edstan</strong>, is printed below. It is more complicated than is typically necessary for a <strong>Stan</strong> model because it is written to apply sensible priors automatically for parameters associated with arbitrarily scaled covariates.</p>
<pre><code>functions {
  real pcm(int y, real theta, vector beta) {
    vector[rows(beta) + 1] unsummed;
    vector[rows(beta) + 1] probs;
    unsummed = append_row(rep_vector(0.0, 1), theta - beta);
    probs = softmax(cumulative_sum(unsummed));
    return categorical_lpmf(y + 1 | probs);
  }
  matrix obtain_adjustments(matrix W) {
    real min_w;
    real max_w;
    int minmax_count;
    matrix[2, cols(W)] adj;
    adj[1, 1] = 0;
    adj[2, 1] = 1;
    if(cols(W) &gt; 1) {
      for(k in 2:cols(W)) {                       // remaining columns
        min_w = min(W[1:rows(W), k]);
        max_w = max(W[1:rows(W), k]);
        minmax_count = 0;
        for(j in 1:rows(W))
          minmax_count = minmax_count + W[j,k] == min_w || W[j,k] == max_w;
        if(minmax_count == rows(W)) {       // if column takes only 2 values
          adj[1, k] = mean(W[1:rows(W), k]);
          adj[2, k] = (max_w - min_w);
        } else {                            // if column takes &gt; 2 values
          adj[1, k] = mean(W[1:rows(W), k]);
          adj[2, k] = sd(W[1:rows(W), k]) * 2;
        }
      }
    }
    return adj;
  }
}
data {
  int&lt;lower=1&gt; I;                // # items
  int&lt;lower=1&gt; J;                // # persons
  int&lt;lower=1&gt; N;                // # responses
  int&lt;lower=1,upper=I&gt; ii[N];    // i for n
  int&lt;lower=1,upper=J&gt; jj[N];    // j for n
  int&lt;lower=0&gt; y[N];             // response for n; y = 0, 1 ... m_i
  int&lt;lower=1&gt; K;                // # person covariates
  matrix[J,K] W;                 // person covariate matrix
}
transformed data {
  int m[I];                      // # parameters per item
  int pos[I];                    // first position in beta vector for item
  matrix[2,K] adj;               // values for centering and scaling covariates
  matrix[J,K] W_adj;             // centered and scaled covariates
  m = rep_array(0, I);
  for(n in 1:N)
    if(y[n] &gt; m[ii[n]]) m[ii[n]] = y[n];
  pos[1] = 1;
  for(i in 2:(I))
    pos[i] = m[i-1] + pos[i-1];
  adj = obtain_adjustments(W);
  for(k in 1:K) for(j in 1:J)
      W_adj[j,k] = (W[j,k] - adj[1,k]) / adj[2,k];
}
parameters {
  vector[sum(m)-1] beta_free;
  vector[J] theta;
  real&lt;lower=0&gt; sigma;
  vector[K] lambda_adj;
}
transformed parameters {
  vector[sum(m)] beta;
  beta[1:(sum(m)-1)] = beta_free;
  beta[sum(m)] = -1*sum(beta_free);
}
model {
  target += normal_lpdf(beta | 0, 3);
  theta ~ normal(W_adj*lambda_adj, sigma);
  lambda_adj ~ student_t(3, 0, 1);
  sigma ~ exponential(.1);
  for (n in 1:N)
    target += pcm(y[n], theta[jj[n]],  segment(beta, pos[ii[n]], m[ii[n]]));
}
generated quantities {
  vector[K] lambda;
  lambda[2:K] = lambda_adj[2:K] ./ to_vector(adj[2,2:K]);
  lambda[1] = W_adj[1, 1:K]*lambda_adj[1:K] - W[1, 2:K]*lambda[2:K];
}</code></pre>
<p>The complete model adds <code>obtain_adjustments()</code> to the functions block, which is used to adjust the covariate matrix. In brief, the model operates on the adjusted covariate matrix, <code>W_adj</code>, and then in the generated quantities block determines what the latent regression coefficients would be on the original scale of the covariates. For a more in depth discussion of <code>obtain_adjustments()</code> and the transformations related to the latent regression, see the <a href="http://mc-stan.org/documentation/case-studies/rasch_and_2pl.html">Rasch and 2PL case study</a>.</p>
<p>In the data block, the number of covariates (plus the intercept) <code>K</code> is now required, as is the matrix of covariates <code>W</code>. Otherwise this block is the same as before. An import change in this model is that <code>beta</code> is now a single, long vector rather than an array. This set up allows items to have different numbers of steps but requires additional programming. To that end, two variables are created in the transformed data block, and these are used to access the elements in <code>beta</code> relevant to a given item: <code>pos</code> indicates the position in <code>beta</code> of the first parameter for a given item, and <code>m</code> indicates the count of parameters for an item.</p>
<p>The parameters <code>beta_free</code>, <code>theta</code>, <code>sigma</code>, and <code>lambda</code> are declared in the parameters block. The unconstrained item parameters are contained in <code>beta_free</code>. In the transformed parameters block, <code>beta</code> is created by appending the constrained item difficulty to <code>beta_free</code>. The model block contains the priors and the likelihood. The <code>target += ...</code> syntax for the prior on <code>beta</code> is a manual way of incrementing the log posterior used when the prior is placed on a transformed parameter.</p>
</div>
<div id="simulation-for-parameter-recovery" class="section level2">
<h2><span class="header-section-number">1.4</span> Simulation for parameter recovery</h2>
<p>The <strong>Stan</strong> model is fit to a simulated dataset to evaluate it’s ability to recover the generating parameter values. The <strong>R</strong> code that follows simulates a dataset conforming to the model.</p>
<pre class="r"><code># Set parameters for the simulated data
J &lt;- 500
sigma &lt;- 1.2
lambda &lt;- c(-10*.05, .05, .5, -.025)
w_2 &lt;- rnorm(J, 10, 5)
w_3 &lt;- rbinom(J, 1, .5)
W &lt;- cbind(1, w_2, w_3, w_2*w_3)

# Set item parameters
I &lt;- 20
Beta_uncentered &lt;- matrix(NA, nrow = I, ncol = 2)
Beta_uncentered[,1] &lt;- seq(from = -1, to = 0, length.out = I)
Beta_uncentered[,2] &lt;- Beta_uncentered[,1] + rep(c(.2, .4, .6, .8), 
                                                 length.out = I)
Beta_centered &lt;- Beta_uncentered - mean(Beta_uncentered)

# A function to simulate responses from the model
simulate_response &lt;- function(theta, beta) {
  unsummed &lt;- c(0, theta - beta)
  numerators &lt;- exp(cumsum(unsummed))
  denominator &lt;- sum(numerators)
  response_probs &lt;- numerators/denominator
  simulated_y &lt;- sample(1:length(response_probs) - 1, size = 1, 
                        prob = response_probs) 
  return(simulated_y)
}

# Calculate or sample remaining variables and parameters
N &lt;- I*J
ii &lt;- rep(1:I, times = J)
jj &lt;- rep(1:J, each = I)
pcm_theta &lt;-  rnorm(J, W %*% matrix(lambda), sigma)
pcm_y &lt;- numeric(N)
for(n in 1:N) {
  pcm_y[n] &lt;- simulate_response(pcm_theta[jj[n]], Beta_centered[ii[n], ])
}

# Assemble the data list using an edstan function
sim_pcm_list &lt;- irt_data(y = pcm_y, ii = ii, jj = jj, 
                         covariates = as.data.frame(W), 
                         formula = NULL)</code></pre>
<p>The simulated data consists of 20 items having 3 response categories and 500 persons. The person covariate vectors <span class="math inline">\(w_j\)</span> include (1) a value of one for the model intercept, (2) a random draw from a normal distribution with mean of 10 and standard deviation of 5, (3) an indicator variable taking values of zero and one, and (4) an interaction between the two. These are chosen to represent a difficult case for assigning automatic priors for the latent regression coefficients. The generating coefficients <span class="math inline">\(\lambda\)</span> for the latent regression are -0.5, 0.05, 0.5, and -0.025. The abilities <span class="math inline">\(\theta\)</span> are random draws from a normal distribution with a mean generated from the latent regression and a standard deviation <span class="math inline">\(\sigma = 1.2\)</span>.</p>
<pre class="r"><code># Plot mean ability conditional on the covariates
f1 &lt;- function(x) lambda[1] + x*lambda[2]
f2 &lt;- function(x) lambda[1] + lambda[3] + x*(lambda[2] + lambda[4])
ggplot(data.frame(w2 = c(0, 20))) +
  aes(x = w2) +
  stat_function(fun = f1, color = &quot;red&quot;) +
  stat_function(fun = f2, color = &quot;blue&quot;) +
  ylab(&quot;Mean generated ability&quot;) +
  xlab(&quot;Value for continous covariate&quot;)</code></pre>
<div class="figure">
<img src="pcm_and_gpcm_files/figure-html/pcm_sim_theta_plot-1.png" alt="Mean of generated abilities as a function of the continuous covariate. A line is shown separately for the two groups identified by the binary variable." width="672" />
<p class="caption">
Mean of generated abilities as a function of the continuous covariate. A line is shown separately for the two groups identified by the binary variable.
</p>
</div>
<p>The simulated dataset is next fit with <strong>Stan</strong> using <code>irt_stan()</code> from the <strong>edstan</strong> package. <code>irt_stan()</code> is merely a wrapper for <code>stan()</code> in <strong>rstan</strong>. Using 1,000 posterior draws per chain may be somewhat excessive as we are mainly interested in the posterior means of the parameters. However, as parameter recovery will be evaluated using the 2.5th and 97.5th percentiles of the posterior, the large number of posterior samples is warranted.</p>
<pre class="r"><code>#Fit model to simulated data
sim_pcm_fit &lt;- irt_stan(sim_pcm_list, model = &quot;pcm_latent_reg.stan&quot;, 
                        chains = 4, iter = 1000)</code></pre>
<p>The highest value for <span class="math inline">\(\hat R\)</span> was 1.004 for all parameters and the log posterior, suggesting that the chains have converged. The <strong>Stan</strong> model is evaluated in terms of its ability to recover the generating values of the parameters. The R code below prepares a plot in which the points indicate the difference between the posterior means and generating values for the parameters of main interest. This difference is referred to as discrepancy. The lines indicate the 95% poster intervals for the difference, defined as the 2.5th and 97.5th percentiles of the posterior draws. Ideally, (nearly) all the 95% intervals would include zero.</p>
<pre class="r"><code># Get estimated and generating values for wanted parameters
beta &lt;- as.vector(t(Beta_centered))
pcm_generating_values &lt;- c(beta, lambda, sigma)
pcm_estimated_values &lt;- summary(sim_pcm_fit,  
                                pars = c(&quot;beta&quot;, &quot;lambda&quot;, &quot;sigma&quot;),
                                probs = c(.025, .975))
pcm_estimated_values &lt;- pcm_estimated_values[[&quot;summary&quot;]]

# Make a data frame of the discrepancies
pcm_discrep &lt;- data.frame(par = rownames(pcm_estimated_values),
                            mean = pcm_estimated_values[, &quot;mean&quot;],
                            p025 = pcm_estimated_values[, &quot;2.5%&quot;],
                            p975 = pcm_estimated_values[, &quot;97.5%&quot;],
                            gen = pcm_generating_values)
pcm_discrep$par &lt;- with(pcm_discrep, factor(par, rev(par)))
pcm_discrep$lower &lt;- with(pcm_discrep, p025 - gen)
pcm_discrep$middle &lt;- with(pcm_discrep, mean - gen)
pcm_discrep$upper &lt;- with(pcm_discrep, p975 - gen)

# Plot the discrepancies
ggplot(pcm_discrep) +
  aes(x = par, y = middle, ymin = lower, ymax = upper) +
  scale_x_discrete() +
  labs(y = &quot;Discrepancy&quot;, x = NULL) +
  geom_abline(intercept = 0, slope = 0, color = &quot;white&quot;) +
  geom_linerange() +
  geom_point(size = 2) +
  theme(panel.grid = element_blank()) +
  coord_flip()</code></pre>
<div class="figure">
<img src="pcm_and_gpcm_files/figure-html/sim_pcm_dif-1.png" alt="Discrepancies between estimated and generating parameters. Points indicate the difference between the posterior means and generating values for a parameter, and horizontal lines indicate 95% posterior intervals for the difference. Most of the discrepancies are about zero, indicating that **Stan** successfully recovers the true parameters." width="672" />
<p class="caption">
Discrepancies between estimated and generating parameters. Points indicate the difference between the posterior means and generating values for a parameter, and horizontal lines indicate 95% posterior intervals for the difference. Most of the discrepancies are about zero, indicating that <strong>Stan</strong> successfully recovers the true parameters.
</p>
</div>
</div>
</div>
<div id="generalized-partial-credit-model-with-latent-regression" class="section level1">
<h1><span class="header-section-number">2</span> Generalized partial credit model with latent regression</h1>
<div id="overview-of-the-model-1" class="section level2">
<h2><span class="header-section-number">2.1</span> Overview of the model</h2>
<p>The GPCM <span class="citation">(Muraki 1992)</span> extends the PCM by including a discrimination term. For dichotomous items (items with exactly two response categories), the generalized partial credit model is equivalent to the two-parameter logistic model. The version presented includes a latent regression. However, the latent regression may be restricted to a model intercept, resulting in the standard generalized partial credit model.</p>
<p><span class="math display">\[
\Pr(Y_{ij} = y,~y &gt; 0 | \theta_j, \alpha_i, \beta_i) =
\frac{\exp \sum_{s=1}^y (\alpha_i \theta_j - \beta_{is})}
     {1 + \sum_{k=1}^{m_i} \exp \sum_{s=1}^k 
       (\alpha_i \theta_j - \beta_{is})}
\]</span> <span class="math display">\[
\Pr(Y_{ij} = y,~y = 0 | \theta_j, \alpha_i, \beta_i) =
\frac{1}
     {1 + \sum_{k=1}^{m_i} \exp \sum_{s=1}^k 
       (\alpha_i \theta_j - \beta_{is})}
\]</span> <span class="math display">\[
\theta_j \sim \mathrm{N}(w_{j}&#39; \lambda, 1)
\]</span></p>
<p>Many aspects of the GPCM are similar to the PCM described earlier. Parameters <span class="math inline">\(\beta_i\)</span>, <span class="math inline">\(\theta_j\)</span>, and <span class="math inline">\(\lambda\)</span> have the same interpretation, but the GPCM adds a discrimination parameter <span class="math inline">\(\alpha_i\)</span> and constrains the variance of <span class="math inline">\(\theta_j\)</span> to one. The prior <span class="math inline">\(\alpha_i \sim \mathrm{log~N}(1, 1)\)</span> is added, which is weakly informative but assumes positive discriminations. The same priors are placed on <span class="math inline">\(\beta_i\)</span> and <span class="math inline">\(\lambda\)</span>, and the same constraint is placed on <span class="math inline">\(\beta_I\)</span>.</p>
</div>
<div id="stan-code-for-the-generalized-partial-credit-model-with-latent-regression" class="section level2">
<h2><span class="header-section-number">2.2</span> <strong>Stan</strong> code for the generalized partial credit model with latent regression</h2>
<p>The <strong>Stan</strong> code for the GPCM is similar to that for the PCM except for the addition of the discrimination parameters.</p>
<pre class="r"><code># Print the latent regression gpcm model from the edstan package
gpcm_latreg_file &lt;- system.file(&quot;extdata/gpcm_latent_reg.stan&quot;, 
                                package = &quot;edstan&quot;)
cat(readLines(gpcm_latreg_file), sep = &quot;\n&quot;)</code></pre>
<pre><code>functions {
  real pcm(int y, real theta, vector beta) {
    vector[rows(beta) + 1] unsummed;
    vector[rows(beta) + 1] probs;
    unsummed = append_row(rep_vector(0.0, 1), theta - beta);
    probs = softmax(cumulative_sum(unsummed));
    return categorical_lpmf(y + 1 | probs);
  }
  matrix obtain_adjustments(matrix W) {
    real min_w;
    real max_w;
    int minmax_count;
    matrix[2, cols(W)] adj;
    adj[1, 1] = 0;
    adj[2, 1] = 1;
    if(cols(W) &gt; 1) {
      for(k in 2:cols(W)) {                       // remaining columns
        min_w = min(W[1:rows(W), k]);
        max_w = max(W[1:rows(W), k]);
        minmax_count = 0;
        for(j in 1:rows(W))
          minmax_count = minmax_count + W[j,k] == min_w || W[j,k] == max_w;
        if(minmax_count == rows(W)) {       // if column takes only 2 values
          adj[1, k] = mean(W[1:rows(W), k]);
          adj[2, k] = (max_w - min_w);
        } else {                            // if column takes &gt; 2 values
          adj[1, k] = mean(W[1:rows(W), k]);
          adj[2, k] = sd(W[1:rows(W), k]) * 2;
        }
      }
    }
    return adj;
  }
}
data {
  int&lt;lower=1&gt; I;                // # items
  int&lt;lower=1&gt; J;                // # persons
  int&lt;lower=1&gt; N;                // # responses
  int&lt;lower=1,upper=I&gt; ii[N];    // i for n
  int&lt;lower=1,upper=J&gt; jj[N];    // j for n
  int&lt;lower=0&gt; y[N];             // response for n; y = 0, 1 ... m_i
  int&lt;lower=1&gt; K;                // # person covariates
  matrix[J,K] W;                 // person covariate matrix
}
transformed data {
  int m[I];                      // # parameters per item
  int pos[I];                    // first position in beta vector for item
  matrix[2,K] adj;               // values for centering and scaling covariates
  matrix[J,K] W_adj;             // centered and scaled covariates
  m = rep_array(0, I);
  for(n in 1:N)
    if(y[n] &gt; m[ii[n]]) m[ii[n]] = y[n];
  pos[1] = 1;
  for(i in 2:(I))
    pos[i] = m[i-1] + pos[i-1];
  adj = obtain_adjustments(W);
  for(k in 1:K) for(j in 1:J)
      W_adj[j,k] = (W[j,k] - adj[1,k]) / adj[2,k];
}
parameters {
  vector&lt;lower=0&gt;[I] alpha;
  vector[sum(m)-1] beta_free;
  vector[J] theta;
  vector[K] lambda_adj;
}
transformed parameters {
  vector[sum(m)] beta;
  beta[1:(sum(m)-1)] = beta_free;
  beta[sum(m)] = -1*sum(beta_free);
}
model {
  alpha ~ lognormal(1, 1);
  target += normal_lpdf(beta | 0, 3);
  theta ~ normal(W_adj*lambda_adj, 1);
  lambda_adj ~ student_t(3, 0, 1);
  for (n in 1:N)
    target += pcm(y[n], theta[jj[n]].*alpha[ii[n]],
                  segment(beta, pos[ii[n]], m[ii[n]]));
}
generated quantities {
  vector[K] lambda;
  lambda[2:K] = lambda_adj[2:K] ./ to_vector(adj[2,2:K]);
  lambda[1] = W_adj[1, 1:K]*lambda_adj[1:K] - W[1, 2:K]*lambda[2:K];
}</code></pre>
</div>
<div id="simulation-for-parameter-recovery-1" class="section level2">
<h2><span class="header-section-number">2.3</span> Simulation for parameter recovery</h2>
<p>The <strong>Stan</strong> model is fit to a simulated dataset to evaluate it’s ability to recover the generating parameter values. The <strong>R</strong> code that follows simulates a dataset conforming to the model. The step difficulties and some other elements are borrowed from the PCM simulation.</p>
<pre class="r"><code># Set alpha, and otherwise use parameters from the previous simulation
alpha &lt;- rep(c(.8, 1.2),  length.out = I)

# Calculate or sample remaining variables and parameters where needed
gpcm_theta &lt;-  W %*% matrix(lambda) + rnorm(J, 0, 1)
gpcm_y &lt;- numeric(N)
for(n in 1:N) {
  gpcm_y[n] &lt;- simulate_response(alpha[ii[n]]*gpcm_theta[jj[n]], 
                                 Beta_centered[ii[n], ])
}

# Assemble the data list using an edstan function
sim_gpcm_list &lt;- irt_data(y = gpcm_y, ii = ii, jj = jj, 
                          covariates = as.data.frame(W), 
                          formula = NULL)</code></pre>
<p>The simulated dataset is next fit with <strong>Stan</strong> using <code>irt_stan()</code> from the <strong>edstan</strong> package.</p>
<pre class="r"><code># Fit model to simulated data using an edstan function
sim_gpcm_fit &lt;- irt_stan(sim_gpcm_list, model = &quot;gpcm_latent_reg.stan&quot;,
                         chains = 4, iter = 1000)</code></pre>
<p>The highest value for <span class="math inline">\(\hat R\)</span> was 1.005 for all parameters and the log posterior. The <strong>Stan</strong> model is evaluated in terms of its ability to recover the generating values of the parameters. The R code below prepares a plot in which the points indicate the difference between the posterior means and generating values for the parameters of main interest. This difference is referred to as discrepancy. The lines indicate the 95% poster intervals for the difference, defined as the 2.5th and 97.5th percentiles of the posterior draws. Ideally, (nearly) all the 95% intervals would include zero.</p>
<pre class="r"><code># Get estimated and generating values for wanted parameters
gpcm_generating_values &lt;- c(alpha, beta, lambda)
gpcm_estimated_values &lt;- summary(sim_gpcm_fit,  
                                 pars = c(&quot;alpha&quot;, &quot;beta&quot;, &quot;lambda&quot;),
                                 probs = c(.025, .975))
gpcm_estimated_values &lt;- gpcm_estimated_values[[&quot;summary&quot;]]

# Make a data frame of the discrepancies
gpcm_discrep &lt;- data.frame(par = rownames(gpcm_estimated_values),
                            mean = gpcm_estimated_values[, &quot;mean&quot;],
                            p025 = gpcm_estimated_values[, &quot;2.5%&quot;],
                            p975 = gpcm_estimated_values[, &quot;97.5%&quot;],
                            gen = gpcm_generating_values)
gpcm_discrep$par &lt;- with(gpcm_discrep, factor(par, rev(par)))
gpcm_discrep$lower &lt;- with(gpcm_discrep, p025 - gen)
gpcm_discrep$middle &lt;- with(gpcm_discrep, mean - gen)
gpcm_discrep$upper &lt;- with(gpcm_discrep, p975 - gen)

# Plot the discrepancies
ggplot(gpcm_discrep) +
  aes(x = par, y = middle, ymin = lower, ymax = upper) +
  scale_x_discrete() +
  labs(y = &quot;Discrepancy&quot;, x = NULL) +
  geom_abline(intercept = 0, slope = 0, color = &quot;white&quot;) +
  geom_linerange() +
  geom_point(size = 2) +
  theme(panel.grid = element_blank()) +
  coord_flip()</code></pre>
<div class="figure">
<img src="pcm_and_gpcm_files/figure-html/sim_gpcm_dif-1.png" alt="Discrepancies between estimated and generating parameters. Points indicate the difference between the posterior means and generating values for a parameter, and horizontal lines indicate 95% posterior intervals for the difference. Most of the discrepancies are about zero, indicating that **Stan** successfully recovers the true parameters." width="672" />
<p class="caption">
Discrepancies between estimated and generating parameters. Points indicate the difference between the posterior means and generating values for a parameter, and horizontal lines indicate 95% posterior intervals for the difference. Most of the discrepancies are about zero, indicating that <strong>Stan</strong> successfully recovers the true parameters.
</p>
</div>
</div>
</div>
<div id="example-application" class="section level1">
<h1><span class="header-section-number">3</span> Example application</h1>
<div id="data" class="section level2">
<h2><span class="header-section-number">3.1</span> Data</h2>
<p>The example data are from the TIMSS 2011 mathematics assessment <span class="citation">(Mullis et al. 2012)</span> of Australian and Taiwanese students. For convenience, a subset of 500 students is used. The subsetted data is then divided into a person covariate matrix and an item response matrix.</p>
<pre class="r"><code># Attach the example dataset. The TAM package is required.
data(data.timssAusTwn.scored, package = &quot;TAM&quot;)

# Subset the full data
select &lt;- floor(seq(from = 1, to = nrow(data.timssAusTwn.scored), 
                    length.out = 500))
subsetted_df &lt;- data.timssAusTwn.scored[select, ]
str(subsetted_df)</code></pre>
<pre><code>## &#39;data.frame&#39;:    500 obs. of  14 variables:
##  $ M032166 : int  1 1 1 0 1 1 1 0 0 1 ...
##  $ M032721 : int  0 1 1 0 0 1 0 0 1 1 ...
##  $ M032757 : int  2 2 2 0 2 2 2 0 2 2 ...
##  $ M032760A: int  0 2 2 0 2 0 2 0 1 2 ...
##  $ M032760B: int  1 1 1 0 1 0 0 0 1 1 ...
##  $ M032760C: int  0 0 1 0 0 0 0 0 0 1 ...
##  $ M032761 : int  2 2 2 0 1 0 0 0 0 1 ...
##  $ M032692 : int  2 2 2 0 0 0 0 0 0 2 ...
##  $ M032626 : int  0 1 1 0 1 1 1 0 1 1 ...
##  $ M032595 : int  1 1 1 1 1 1 1 0 1 1 ...
##  $ M032673 : int  1 1 1 0 1 1 1 0 0 1 ...
##  $ IDCNTRY : int  36 36 36 36 36 36 36 36 36 36 ...
##  $ ITSEX   : int  1 1 2 2 2 2 1 1 2 1 ...
##  $ IDBOOK  : int  1 1 1 1 1 1 1 1 1 1 ...</code></pre>
<p>The dataset is next divided into an item response matrix and a matrix of student covariates.</p>
<pre class="r"><code># Make a matrix of person predictors
w_mat &lt;- cbind(intercept = rep(1, times = nrow(subsetted_df)),
               taiwan = as.numeric(subsetted_df$IDCNTRY == 158),
               female = as.numeric(subsetted_df$ITSEX == 2),
               book14 = as.numeric(subsetted_df$IDBOOK == 14))
head(w_mat)</code></pre>
<pre><code>##      intercept taiwan female book14
## [1,]         1      0      0      0
## [2,]         1      0      0      0
## [3,]         1      0      1      0
## [4,]         1      0      1      0
## [5,]         1      0      1      0
## [6,]         1      0      1      0</code></pre>
<pre class="r"><code># Make a matrix of item responses
y_mat &lt;- as.matrix(subsetted_df[, grep(&quot;^M&quot;, names(subsetted_df))])
head(y_mat)</code></pre>
<pre><code>##    M032166 M032721 M032757 M032760A M032760B M032760C M032761 M032692
## 1        1       0       2        0        1        0       2       2
## 4        1       1       2        2        1        0       2       2
## 8        1       1       2        2        1        1       2       2
## 11       0       0       0        0        0        0       0       0
## 15       1       0       2        2        1        0       1       0
## 18       1       1       2        0        0        0       0       0
##    M032626 M032595 M032673
## 1        0       1       1
## 4        1       1       1
## 8        1       1       1
## 11       0       1       0
## 15       1       1       1
## 18       1       1       1</code></pre>
<p>The person covariate matrix <code>w_mat</code> has columns representing an intercept and three indicator variables for being in Taiwan (versus Australia), being female (versus male), and being assigned test booklet 14 (instead of booklet 1). The item response matrix <code>y_mat</code> contains 11 items. Neither the response matrix or person covariates contain missing data.</p>
<p>The following <strong>R</strong> code checks the maximum score per item.</p>
<pre class="r"><code># Maximum score for each item
apply(y_mat, 2, max)</code></pre>
<pre><code>##  M032166  M032721  M032757 M032760A M032760B M032760C  M032761  M032692 
##        1        1        2        2        1        1        2        2 
##  M032626  M032595  M032673 
##        1        1        1</code></pre>
<p>The above results show that the data are a mixture of dichotomous item and polytomous items with three responses categories. The first and second items are dichotomous, while the third and fourth are polytomous, for example. Consequently, the first and second items will have one step parameter each, while the third and fourth will have two each.</p>
<p>The data are now formatted into a data list.</p>
<pre class="r"><code># Assemble data list for Stan
ex_list &lt;- irt_data(response_matrix = y_mat, covariates = as.data.frame(w_mat),
                    formula = ~ taiwan*female + book14)</code></pre>
</div>
<div id="partial-credit-model-results" class="section level2">
<h2><span class="header-section-number">3.2</span> Partial credit model results</h2>
<pre class="r"><code># Run Stan model
pcm_fit &lt;- irt_stan(ex_list, model = &quot;pcm_latent_reg.stan&quot;,
                    chains = 4, iter = 300)</code></pre>
<p>As discussed above, convergence of the chains is assessed for every parameter, and also the log posterior density, using <span class="math inline">\(\hat{R}\)</span>.</p>
<pre class="r"><code># Plot of convergence statistics
stan_columns_plot(pcm_fit)</code></pre>
<div class="figure">
<img src="pcm_and_gpcm_files/figure-html/ex_pcm_converge-1.png" alt="Convergence statistics ($\hat{R}$) by parameter for the example. All values should be less than 1.1 to infer convergence." width="672" />
<p class="caption">
Convergence statistics (<span class="math inline">\(\hat{R}\)</span>) by parameter for the example. All values should be less than 1.1 to infer convergence.
</p>
</div>
<p>Next we view a summary of the parameter posteriors.</p>
<pre class="r"><code># View table of parameter posteriors
print_irt_stan(pcm_fit, ex_list)</code></pre>
<pre><code>## Inference for Stan model: pcm_latent_reg.
## 4 chains, each with iter=300; warmup=150; thin=1; 
## post-warmup draws per chain=150, total post-warmup draws=600.
##   
##              mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat
## Item 1: M032166
##   beta[1]   -1.02    0.00 0.11 -1.24 -1.10 -1.02 -0.95 -0.81   600 1.00
## Item 2: M032721
##   beta[2]    0.11    0.00 0.10 -0.10  0.03  0.11  0.18  0.31   600 0.99
## Item 3: M032757
##   beta[3]    0.68    0.01 0.23  0.25  0.52  0.68  0.83  1.19   369 1.01
##   beta[4]   -2.77    0.01 0.22 -3.20 -2.92 -2.76 -2.62 -2.34   396 1.01
## Item 4: M032760A
##   beta[5]    1.89    0.01 0.25  1.41  1.72  1.87  2.04  2.41   600 1.00
##   beta[6]   -1.83    0.01 0.25 -2.39 -2.00 -1.80 -1.66 -1.36   600 1.00
## Item 5: M032760B
##   beta[7]    0.86    0.00 0.11  0.64  0.78  0.86  0.94  1.09   600 1.00
## Item 6: M032760C
##   beta[8]    1.37    0.00 0.11  1.14  1.29  1.37  1.45  1.58   600 1.01
## Item 7: M032761
##   beta[9]    1.13    0.01 0.14  0.87  1.03  1.14  1.23  1.41   600 1.00
##   beta[10]   0.72    0.01 0.15  0.42  0.61  0.72  0.83  1.00   600 1.00
## Item 8: M032692
##   beta[11]   2.95    0.01 0.30  2.40  2.75  2.93  3.15  3.53   600 1.00
##   beta[12]  -1.50    0.01 0.30 -2.12 -1.70 -1.49 -1.31 -0.90   600 1.00
## Item 9: M032626
##   beta[13]  -0.64    0.00 0.11 -0.87 -0.72 -0.64 -0.57 -0.43   600 1.00
## Item 10: M032595
##   beta[14]  -1.06    0.00 0.11 -1.28 -1.12 -1.06 -0.98 -0.86   600 1.00
## Item 11: M032673
##   beta[15]  -0.88    0.00 0.12 -1.11 -0.96 -0.88 -0.81 -0.64   600 1.01
## Ability distribution
##   lambda[1] -0.51    0.01 0.14 -0.79 -0.61 -0.51 -0.42 -0.25   600 1.00
##   lambda[2]  2.00    0.01 0.20  1.63  1.87  2.01  2.14  2.41   365 1.00
##   lambda[3]  0.07    0.01 0.19 -0.31 -0.05  0.08  0.20  0.42   600 1.00
##   lambda[4] -0.28    0.01 0.13 -0.54 -0.37 -0.28 -0.19 -0.02   600 1.00
##   lambda[5]  0.02    0.01 0.27 -0.53 -0.15  0.03  0.21  0.53   340 1.01
##   sigma      1.40    0.01 0.07  1.27  1.35  1.39  1.44  1.54   179 1.01
##   
## Samples were drawn using NUTS(diag_e) at Wed Jun 28 14:24:06 2017.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
<p>If person covariates are unavailable, or their inclusion unwanted, the model may be fit restricting the matrix of person covariates to an intercept only. In this case, the vector <code>lambda</code> contains only one element, which will represent the mean of the ability distribution. The code below is an example of how to create the data list for this purpose.</p>
<pre class="r"><code># Fit the example data without latent regression
noreg_list &lt;- irt_data(response_matrix = y_mat)
noreg_fit &lt;- irt_stan(noreg_list, model = &quot;pcm_latent_reg.stan&quot;,
                      chains = 4, iter = 300)</code></pre>
</div>
<div id="generalized-partial-credit-model-results" class="section level2">
<h2><span class="header-section-number">3.3</span> Generalized partial credit model results</h2>
<pre class="r"><code># Run Stan model
gpcm_fit &lt;- irt_stan(ex_list, model = &quot;gpcm_latent_reg.stan&quot;,
                     chains = 4, iter = 300)</code></pre>
<p>As discussed above, convergence of the chains is assessed for every parameter, and also the log posterior density, using <span class="math inline">\(\hat{R}\)</span>.</p>
<pre class="r"><code># Plot of convergence statistics
stan_columns_plot(gpcm_fit)</code></pre>
<div class="figure">
<img src="pcm_and_gpcm_files/figure-html/ex_gpcm_converge-1.png" alt="Convergence statistics ($\hat{R}$) by parameter for the example. All values should be less than 1.1 to infer convergence." width="672" />
<p class="caption">
Convergence statistics (<span class="math inline">\(\hat{R}\)</span>) by parameter for the example. All values should be less than 1.1 to infer convergence.
</p>
</div>
<p>Next we view a summary of the parameter posteriors.</p>
<pre class="r"><code># View table of parameter posteriors
print_irt_stan(gpcm_fit, ex_list)</code></pre>
<pre><code>## Inference for Stan model: gpcm_latent_reg.
## 4 chains, each with iter=300; warmup=150; thin=1; 
## post-warmup draws per chain=150, total post-warmup draws=600.
##   
##              mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat
## Item 1: M032166
##   alpha[1]   0.80    0.00 0.11  0.59  0.72  0.80  0.88  1.04   600 1.01
##   beta[1]   -0.94    0.00 0.11 -1.15 -1.01 -0.94 -0.87 -0.72   600 1.00
## Item 2: M032721
##   alpha[2]   0.50    0.00 0.09  0.33  0.43  0.49  0.55  0.67   600 1.00
##   beta[2]   -0.04    0.00 0.10 -0.21 -0.10 -0.03  0.03  0.15   600 0.99
## Item 3: M032757
##   alpha[3]   1.10    0.01 0.14  0.87  1.01  1.09  1.19  1.43   303 1.01
##   beta[3]    0.85    0.01 0.30  0.29  0.67  0.85  1.03  1.52   449 1.00
##   beta[4]   -2.86    0.01 0.23 -3.34 -3.01 -2.87 -2.70 -2.42   600 1.00
## Item 4: M032760A
##   alpha[4]   1.98    0.02 0.29  1.51  1.78  1.96  2.15  2.64   271 1.00
##   beta[5]    1.42    0.01 0.27  0.87  1.23  1.42  1.59  1.96   415 1.00
##   beta[6]   -1.76    0.01 0.26 -2.30 -1.91 -1.76 -1.59 -1.26   415 1.01
## Item 5: M032760B
##   alpha[5]   2.04    0.01 0.24  1.61  1.88  2.05  2.19  2.52   264 1.01
##   beta[7]    0.86    0.01 0.14  0.59  0.76  0.85  0.95  1.14   557 1.00
## Item 6: M032760C
##   alpha[6]   3.17    0.03 0.42  2.40  2.90  3.15  3.42  4.07   218 1.00
##   beta[8]    2.16    0.02 0.27  1.66  1.97  2.16  2.33  2.75   247 1.00
## Item 7: M032761
##   alpha[7]   2.67    0.04 0.35  2.03  2.41  2.65  2.88  3.38   102 1.02
##   beta[9]    0.85    0.01 0.17  0.52  0.74  0.85  0.97  1.19   600 1.00
##   beta[10]   1.73    0.02 0.32  1.19  1.48  1.72  1.95  2.39   176 1.01
## Item 8: M032692
##   alpha[8]   1.28    0.01 0.14  1.02  1.18  1.27  1.36  1.57   381 1.00
##   beta[11]   2.86    0.02 0.29  2.33  2.65  2.86  3.04  3.40   371 1.00
##   beta[12]  -1.82    0.02 0.31 -2.45 -2.02 -1.80 -1.60 -1.26   334 1.00
## Item 9: M032626
##   alpha[9]   1.69    0.01 0.18  1.37  1.58  1.68  1.80  2.06   410 1.00
##   beta[13]  -0.92    0.01 0.13 -1.18 -1.00 -0.91 -0.83 -0.66   600 1.00
## Item 10: M032595
##   alpha[10]  1.65    0.01 0.19  1.32  1.53  1.65  1.76  2.04   385 1.02
##   beta[14]  -1.36    0.01 0.14 -1.66 -1.46 -1.36 -1.26 -1.09   600 1.00
## Item 11: M032673
##   alpha[11]  1.37    0.01 0.16  1.09  1.26  1.36  1.48  1.72   600 1.00
##   beta[15]  -1.04    0.01 0.13 -1.31 -1.13 -1.04 -0.95 -0.81   600 1.00
## Ability distribution
##   lambda[1] -0.50    0.01 0.10 -0.70 -0.57 -0.49 -0.43 -0.31   400 1.00
##   lambda[2]  1.44    0.01 0.15  1.13  1.34  1.44  1.53  1.74   247 1.00
##   lambda[3]  0.06    0.01 0.12 -0.19 -0.02  0.06  0.14  0.31   512 0.99
##   lambda[4] -0.22    0.00 0.10 -0.41 -0.28 -0.21 -0.15 -0.04   600 1.00
##   lambda[5]  0.00    0.01 0.19 -0.36 -0.12  0.00  0.12  0.39   335 1.00
##   
## Samples were drawn using NUTS(diag_e) at Wed Jun 28 14:27:14 2017.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
</div>
</div>
<div id="references" class="section level1">
<h1><span class="header-section-number">4</span> References</h1>
<!-- This comment causes section to be numbered -->
<div id="refs" class="references">
<div id="ref-gelman2008weakly">
<p>Gelman, Andrew, Aleks Jakulin, Maria Grazia Pittau, and Yu-Sung Su. 2008. “A Weakly Informative Default Prior Distribution for Logistic and Other Regression Models.” <em>The Annals of Applied Statistics</em>. JSTOR, 1360–83.</p>
</div>
<div id="ref-masters1982rasch">
<p>Masters, Geoff N. 1982. “A Rasch Model for Partial Credit Scoring.” <em>Psychometrika</em> 47 (2). Springer: 149–74.</p>
</div>
<div id="ref-mullis2012timss">
<p>Mullis, Ina V S, Michael O Martin, Pierre Foy, and Alka Arora. 2012. <em>TIMSS 2011 International Results in Mathematics.</em> ERIC.</p>
</div>
<div id="ref-muraki1992generalized">
<p>Muraki, Eiji. 1992. “A Generalized Partial Credit Model: Application of an EM Algorithm.” <em>ETS Research Report Series</em> 1992 (1). Wiley Online Library: i–30.</p>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
