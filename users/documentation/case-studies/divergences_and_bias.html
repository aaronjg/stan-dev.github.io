<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Michael Betancourt" />


<title>Diagnosing Biased Inference with Divergences</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="site_libs/highlight/default.css"
      type="text/css" />
<script src="site_libs/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Diagnosing Biased Inference with Divergences</h1>
<h4 class="author"><em>Michael Betancourt</em></h4>
<h4 class="date"><em>January 2017</em></h4>

</div>


<p>Markov chain Monte Carlo (MCMC) approximates expectations with respect to a given target distribution, <span class="math display">\[
\mathbb{E}_{\pi} [ f ] = \int \mathrm{d}q \, \pi (q) \, f(q),
\]</span> using the states of a Markov chain, <span class="math inline">\(\{q_{0}, \ldots, q_{N} \}\)</span>, <span class="math display">\[
\mathbb{E}_{\pi} [ f ] \approx
\hat{f}_{N} = \frac{1}{N + 1} \sum_{n = 0}^{N} f(q_{n}).
\]</span> These estimators, however, are guaranteed to be accurate only <em>asymptotically</em> as the chain grows to be infinitely long, <span class="math display">\[
\lim_{N \rightarrow \infty} \hat{f}_{N} = \mathbb{E}_{\pi} [ f ].
\]</span></p>
<p>To be useful in applied analyses, we need MCMC estimators to converge to the true expectation values sufficiently quickly that they are reasonably accurate before we exhaust our finite computational resources. This fast convergence requires strong ergodicity conditions to hold, in particular <em>geometric ergodicity</em> between a Markov transition and a target distribution. Geometric ergodicity is usually the necessary condition for MCMC estimators to follow a central limit theorem, which ensures not only that they are unbiased even after only a finite number of iterations but also that we can empirically quantify their precision using the MCMC standard error.</p>
<p>Unfortunately, proving geometric ergodicity theoretically is infeasible for any nontrivial problem. Instead we must rely on empirical diagnostics that identify obstructions to geometric ergodicity, and hence well-behaved MCMC estimators. For a general Markov transition and target distribution, the best known diagnostic is the split <span class="math inline">\(\hat{R}\)</span> statistic over an ensemble of Markov chains initialized from diffuse points in parameter space; to do any better we need to exploit the particular structure of a given transition or target distribution.</p>
<p>Hamiltonian Monte Carlo, for example, is especially powerful in this regard as its failures to be geometrically ergodic with respect to any target distribution manifest in distinct behaviors that have been developed into sensitive diagnostics. One of these behaviors is the appearance of <em>divergences</em> that indicate the Hamiltonian Markov chain has encountered regions of high curvature in the target distribution which it cannot adequately explore.</p>
<p>In this case study I will show how divergences signal bias in the fitting of hierarchical models and how they can be used to study the underlying pathologies. I will also show how those pathologies can be mitigated by utilizing an alternative implementation of the same model.</p>
<div id="the-eight-schools-model" class="section level1">
<h1>The Eight Schools Model</h1>
<p>Let’s consider a hierarchical model of the the Eight Schools dataset <span class="citation">(Rubin 1981)</span>,</p>
<p><span class="math display">\[\mu \sim \mathcal{N}(0, 5)\]</span></p>
<p><span class="math display">\[\tau \sim \text{Half-Cauchy}(0, 5)\]</span></p>
<p><span class="math display">\[\theta_{n} \sim \mathcal{N}(\mu, \tau)\]</span></p>
<p><span class="math display">\[y_{n} \sim \mathcal{N}(\theta_{n}, \sigma_{n}),\]</span></p>
<p>where <span class="math inline">\(n \in \left\{1, \ldots, 8 \right\}\)</span> and the <span class="math inline">\(\left\{ y_{n}, \sigma_{n} \right\}\)</span> are given as data.</p>
<p>Inferring the hierarchical hyperparameters, <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>, together with the group-level parameters, <span class="math inline">\(\theta_{1}, \ldots, \theta_{8}\)</span>, allows the model to pool data across the groups and reduce their posterior variance. Unfortunately this pooling also squeezes the posterior distribution into a particularly challenging geometry that obstructs geometric ergodicity and hence biases MCMC estimation.</p>
<p>In this case study we’ll first examine the direct <em>centered</em> parameterization of the Eight Schools model and see how divergences identify this bias before there are any other indications of problems. We’ll then use these divergences to study the source of the bias and motivate the necessary fix, a reimplementation of the model with a <em>non-centered</em> parameterization. For a more thorough discussion of the geometry of centered and non-centered parameterizations of hierarchical models see <span class="citation">Betancourt and Girolami (2015)</span>.</p>
</div>
<div id="a-centered-eight-schools-implementation" class="section level1">
<h1>A Centered Eight Schools Implementation</h1>
<p>A centered parameterization of the Eight Schools model is straightforward to directly implement as a Stan program,</p>
<pre class="r"><code>writeLines(readLines(&quot;eight_schools_cp.stan&quot;))</code></pre>
<pre><code>data {
  int&lt;lower=0&gt; J;
  real y[J];
  real&lt;lower=0&gt; sigma[J];
}

parameters {
  real mu;
  real&lt;lower=0&gt; tau;
  real theta[J];
}

model {
  mu ~ normal(0, 5);
  tau ~ cauchy(0, 5);
  theta ~ normal(mu, tau);
  y ~ normal(theta, sigma);
}</code></pre>
<p>Unfortunately, this direct implementation of the model exhibits a pathological geometry that frustrates geometric ergodicity. Even more worrisome, the resulting bias is subtle and may not be obvious upon inspection of the Markov chain alone. To understand this bias, let’s consider first a short Markov chain, commonly used when computational expediency is a motivating factor, and only afterwards a longer Markov chain.</p>
<div id="a-dangerously-short-markov-chain" class="section level2">
<h2>A Dangerously-Short Markov Chain</h2>
<p>We begin by setting up our R environment,</p>
<pre class="r"><code>library(rstan)
rstan_options(auto_write = TRUE)</code></pre>
<p>and then some graphic customizations that we’ll use later,</p>
<pre class="r"><code>c_light &lt;- c(&quot;#DCBCBC&quot;)
c_light_highlight &lt;- c(&quot;#C79999&quot;)
c_mid &lt;- c(&quot;#B97C7C&quot;)
c_mid_highlight &lt;- c(&quot;#A25050&quot;)
c_dark &lt;- c(&quot;#8F2727&quot;)
c_dark_highlight &lt;- c(&quot;#7C0000&quot;)</code></pre>
<p>Against the best practices preached by the Stan development team, let’s fit the model in RStan using just a single short chain,</p>
<pre class="r"><code>input_data &lt;- read_rdump(&quot;eight_schools.data.R&quot;)

fit_cp &lt;- stan(file=&#39;eight_schools_cp.stan&#39;, data=input_data,
            iter=1200, warmup=500, chains=1, seed=483892929, refresh=1200)</code></pre>
<pre><code>
SAMPLING FOR MODEL &#39;eight_schools_cp&#39; NOW (CHAIN 1).

Chain 1, Iteration:    1 / 1200 [  0%]  (Warmup)
Chain 1, Iteration:  501 / 1200 [ 41%]  (Sampling)
Chain 1, Iteration: 1200 / 1200 [100%]  (Sampling)
 Elapsed Time: 0.054132 seconds (Warm-up)
               0.048684 seconds (Sampling)
               0.102816 seconds (Total)</code></pre>
<p>Although this scenario may appear superficial, it is not uncommon for users to use short chains when prototyping their analysis, or even for their final analysis if they are limited by time or computational resources.</p>
<p>For this lone chain split <span class="math inline">\(\hat{R}\)</span> doesn’t indicate any problems and the effective sample size per iteration is reasonable,</p>
<pre class="r"><code>print(fit_cp)</code></pre>
<pre><code>Inference for Stan model: eight_schools_cp.
1 chains, each with iter=1200; warmup=500; thin=1; 
post-warmup draws per chain=700, total post-warmup draws=700.

           mean se_mean   sd   2.5%    25%    50%    75% 97.5% n_eff Rhat
mu         4.29    0.26 3.29  -2.99   2.06   4.53   6.33 10.86   165 1.00
tau        4.50    0.37 3.09   0.89   2.27   3.54   5.94 13.05    70 1.01
theta[1]   6.60    0.34 5.73  -3.59   2.95   6.21   9.52 19.25   281 1.00
theta[2]   5.28    0.31 4.90  -4.72   2.50   5.44   7.83 15.27   249 1.01
theta[3]   3.83    0.33 5.77 -10.62   0.91   4.29   7.16 14.36   311 1.00
theta[4]   4.99    0.30 5.25  -5.53   1.98   4.96   8.28 15.13   302 1.00
theta[5]   3.37    0.37 5.13  -7.39   0.22   4.00   6.36 13.44   192 1.00
theta[6]   3.77    0.36 5.02  -7.33   0.99   4.08   6.90 12.81   200 1.00
theta[7]   6.87    0.38 5.51  -3.17   3.28   6.09   9.60 20.43   206 1.01
theta[8]   4.68    0.28 5.25  -6.01   1.80   4.81   7.36 15.53   341 1.00
lp__     -16.77    0.83 5.42 -26.88 -20.65 -16.91 -13.14 -5.12    43 1.02

Samples were drawn using NUTS(diag_e) at Thu Mar  2 15:24:17 2017.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>Moreover, the trace plots all look fine. Let’s consider, for example, the hierarchical standard deviation, <span class="math inline">\(\tau\)</span> or, more specifically, its logarithm, <span class="math inline">\(\log \tau\)</span>. Because <span class="math inline">\(\tau\)</span> is constrained to be positive, its logarithm will allow us to better resolve behavior for small values. Indeed the chains seems to be exploring both small and large values reasonably well,</p>
<pre class="r"><code>params_cp &lt;- as.data.frame(extract(fit_cp, permuted=FALSE))
names(params_cp) &lt;- gsub(&quot;chain:1.&quot;, &quot;&quot;, names(params_cp), fixed = TRUE)
names(params_cp) &lt;- gsub(&quot;[&quot;, &quot;.&quot;, names(params_cp), fixed = TRUE)
names(params_cp) &lt;- gsub(&quot;]&quot;, &quot;&quot;, names(params_cp), fixed = TRUE)
params_cp$iter &lt;- 1:700

par(mar = c(4, 4, 0.5, 0.5))
plot(params_cp$iter, log(params_cp$tau), col=c_dark, pch=16, cex=0.8,
     xlab=&quot;Iteration&quot;, ylab=&quot;log(tau)&quot;, ylim=c(-6, 4))</code></pre>
<p><img src="divergences_and_bias_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Unfortunately, the resulting estimate for the mean of <span class="math inline">\(log(\tau)\)</span> is strongly biased away from the true value, here shown in grey,</p>
<pre class="r"><code>running_means &lt;- sapply(params_cp$iter, function(n) mean(log(params_cp$tau)[1:n]))

par(mar = c(4, 4, 0.5, 0.5))
plot(params_cp$iter, running_means, col=c_dark, pch=16, cex=0.8, ylim=c(0, 2),
    xlab=&quot;Iteration&quot;, ylab=&quot;MCMC mean of log(tau)&quot;)
abline(h=0.7657852, col=&quot;grey&quot;, lty=&quot;dashed&quot;, lwd=3)</code></pre>
<p><img src="divergences_and_bias_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Hamiltonian Monte Carlo, however, is not so oblivious to these issues as almost 2% of the iterations in our lone Markov chain ended with a divergence,</p>
<pre class="r"><code>divergent &lt;- get_sampler_params(fit_cp, inc_warmup=FALSE)[[1]][,&#39;divergent__&#39;]
sum(divergent)</code></pre>
<pre><code>[1] 12</code></pre>
<pre class="r"><code>sum(divergent) / 700</code></pre>
<pre><code>[1] 0.01714286</code></pre>
<p>Even with a single short chain these divergences are able to identity the bias and advise skepticism of any resulting MCMC estimators.</p>
<p>Additionally, because the divergent transitions, here shown here in green, tend to be located near the pathologies we can use them to identify the location of the problematic neighborhoods in parameter space,</p>
<pre class="r"><code>params_cp$divergent &lt;- divergent

div_params_cp &lt;- params_cp[params_cp$divergent == 1,]
nondiv_params_cp &lt;- params_cp[params_cp$divergent == 0,]

par(mar = c(4, 4, 0.5, 0.5))
plot(nondiv_params_cp$theta.1, log(nondiv_params_cp$tau),
     col=c_dark, pch=16, cex=0.8, xlab=&quot;theta.1&quot;, ylab=&quot;log(tau)&quot;,
     xlim=c(-20, 50), ylim=c(-6,4))
points(div_params_cp$theta.1, log(div_params_cp$tau),
       col=&quot;green&quot;, pch=16, cex=0.8)</code></pre>
<p><img src="divergences_and_bias_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>In this case the divergences are clustering at small values of <span class="math inline">\(\tau\)</span> where the hierarchical distribution, and hence all of the group-level <span class="math inline">\(\theta_{n}\)</span>, are squeezed together. Eventually this squeezing would yield the funnel geometry infamous to hierarchical models, but here it appears that the Hamiltonian Markov chain is diverging before it can fully explore the neck of the funnel.</p>
</div>
<div id="a-safer-longer-markov-chain" class="section level2">
<h2>A Safer, Longer Markov Chain</h2>
<p>Aware of the potential insensitivity of split <span class="math inline">\(\hat{R}\)</span> on single short chains, we recommend always running multiple chains as long as possible to have the best chance to observe any obstructions to geometric ergodicity. Because it is not always possible to run long chains for complex models, however, divergences are an incredibly powerful diagnostic for biased MCMC estimation.</p>
<p>With divergences already indicating a problem for our centered implementation of the Eight Schools model, let’s run a much longer chain to see how the problems more obviously manifest,</p>
<pre class="r"><code>fit_cp80 &lt;- stan(file=&#39;eight_schools_cp.stan&#39;, data=input_data,
                 iter=11000, warmup=1000, chains=1, seed=483892929,
                refresh=11000)</code></pre>
<pre><code>
SAMPLING FOR MODEL &#39;eight_schools_cp&#39; NOW (CHAIN 1).

Chain 1, Iteration:     1 / 11000 [  0%]  (Warmup)
Chain 1, Iteration:  1001 / 11000 [  9%]  (Sampling)
Chain 1, Iteration: 11000 / 11000 [100%]  (Sampling)
 Elapsed Time: 0.09754 seconds (Warm-up)
               0.534603 seconds (Sampling)
               0.632143 seconds (Total)</code></pre>
<p>Even with so many more iterations, split <span class="math inline">\(\hat{R}\)</span> does not indicate any serious issues,</p>
<pre class="r"><code>print(fit_cp80)</code></pre>
<pre><code>Inference for Stan model: eight_schools_cp.
1 chains, each with iter=11000; warmup=1000; thin=1; 
post-warmup draws per chain=10000, total post-warmup draws=10000.

           mean se_mean   sd   2.5%    25%    50%    75% 97.5% n_eff Rhat
mu         4.36    0.12 3.40  -2.31   2.19   4.39   6.76 10.98   799    1
tau        3.82    0.17 3.26   0.67   1.51   2.91   5.06 12.28   376    1
theta[1]   6.23    0.17 5.62  -3.15   2.54   5.64   8.99 19.59  1136    1
theta[2]   4.93    0.13 4.77  -4.52   2.12   4.78   7.87 14.68  1315    1
theta[3]   3.86    0.13 5.37  -8.12   0.86   4.00   7.16 13.68  1644    1
theta[4]   4.72    0.14 4.94  -5.18   1.73   4.62   7.71 14.43  1327    1
theta[5]   3.55    0.13 4.81  -6.97   0.80   3.65   6.76 12.18  1302    1
theta[6]   3.96    0.13 4.88  -6.40   1.16   3.94   6.97 13.22  1524    1
theta[7]   6.37    0.17 5.28  -2.58   2.81   5.92   9.14 18.79  1007    1
theta[8]   4.82    0.12 5.33  -5.63   1.79   4.72   7.80 15.77  1961    1
lp__     -14.94    0.47 6.14 -26.56 -19.39 -15.07 -10.27 -3.98   170    1

Samples were drawn using NUTS(diag_e) at Thu Mar  2 15:24:18 2017.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>We really need to be incorporating multiple chains for split <span class="math inline">\(\hat{R}\)</span> to be effective. Still, note that the effective sample size per iteration has drastically fallen, indicating that we are exploring less efficiently the longer we run. This odd behavior is a clear sign that something problematic is afoot.</p>
<p>The trace plots are more indicative of the underlying pathologies, showing the chain occasionally “sticking” as it approaches small values of <span class="math inline">\(\tau\)</span>, exactly where we saw the divergences concentrating,</p>
<pre class="r"><code>params_cp80 &lt;- as.data.frame(extract(fit_cp80, permuted=FALSE))
names(params_cp80) &lt;- gsub(&quot;chain:1.&quot;, &quot;&quot;, names(params_cp80), fixed = TRUE)
names(params_cp80) &lt;- gsub(&quot;[&quot;, &quot;.&quot;, names(params_cp80), fixed = TRUE)
names(params_cp80) &lt;- gsub(&quot;]&quot;, &quot;&quot;, names(params_cp80), fixed = TRUE)
params_cp80$iter &lt;- 1:10000

par(mar = c(4, 4, 0.5, 0.5))
plot(params_cp80$iter, log(params_cp80$tau), col=c_dark, pch=16, cex=0.8,
     xlab=&quot;Iteration&quot;, ylab=&quot;log(tau)&quot;, ylim=c(-6, 4))</code></pre>
<p><img src="divergences_and_bias_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>These sticky intervals induce severe oscillations in the MCMC estimators early on, until they seem to finally settle into biased values,</p>
<pre class="r"><code>running_means_cp80 &lt;- sapply(1:1000, function(n) mean(log(params_cp80$tau)[1:(10*n)]))

par(mar = c(4, 4, 0.5, 0.5))
plot(10*(1:1000), running_means_cp80, col=c_dark, pch=16, cex=0.8, ylim=c(0, 2),
    xlab=&quot;Iteration&quot;, ylab=&quot;MCMC mean of log(tau)&quot;)
abline(h=0.7657852, col=&quot;grey&quot;, lty=&quot;dashed&quot;, lwd=3)</code></pre>
<p><img src="divergences_and_bias_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>In fact the sticky intervals are the Markov chain trying to correct the biased exploration. If we ran the chain even longer then it would eventually get stuck again and drag the MCMC estimator down towards the true value. Given an infinite number of iterations this delicate balance asymptotes to the true expectation as we’d expect given the consistency guarantee of MCMC. Stopping the after any finite number of iterations, however, destroys this balance and leaves us with a significant bias.</p>
<p>The rate of divergences remains near 2% of all iterations,</p>
<pre class="r"><code>divergent &lt;- get_sampler_params(fit_cp80, inc_warmup=FALSE)[[1]][,&#39;divergent__&#39;]
sum(divergent)</code></pre>
<pre><code>[1] 299</code></pre>
<pre class="r"><code>sum(divergent) / 10000</code></pre>
<pre><code>[1] 0.0299</code></pre>
<p>and the increased sampling really allows us to see the truncated funnel geometry of the Markov chain,</p>
<pre class="r"><code>params_cp80$divergent &lt;- divergent

div_params_cp &lt;- params_cp80[params_cp80$divergent == 1,]
nondiv_params_cp &lt;- params_cp80[params_cp80$divergent == 0,]

par(mar = c(4, 4, 0.5, 0.5))
plot(nondiv_params_cp$theta.1, log(nondiv_params_cp$tau),
     col=c_dark, pch=16, cex=0.8, xlab=&quot;theta.1&quot;, ylab=&quot;log(tau)&quot;,
     xlim=c(-20, 50), ylim=c(-6,4))
points(div_params_cp$theta.1, log(div_params_cp$tau),
       col=&quot;green&quot;, pch=16, cex=0.8)</code></pre>
<p><img src="divergences_and_bias_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
<div id="mitigating-divergences-by-adjusting-stans-adaptation-routine" class="section level2">
<h2>Mitigating Divergences by Adjusting Stan’s Adaptation Routine</h2>
<p>Divergences in Hamiltonian Monte Carlo arise when the Hamiltonian transition encounters regions of extremely large curvature, such as the opening of the hierarchical funnel. Unable to accurate resolve these regions, the transition malfunctions and flies off towards infinity. With the transitions unable to completely explore these regions of extreme curvature, we lose geometric ergodicity and our MCMC estimators become biased.</p>
<p>Stan uses a heuristic to quickly identify these misbehaving trajectories, and hence label divergences, without having to wait for them to run all the way to infinity. This heuristic can be a bit aggressive, however, and sometimes label transitions as divergent even when we have not lost geometric ergodicity.</p>
<p>To resolve this potential ambiguity we can adjust the step size, <span class="math inline">\(\epsilon\)</span>, of the Hamiltonian transition. The smaller the step size the more accurate the trajectory and the less likely it will be mislabeled as a divergence. In other words, if we have geometric ergodicity between the Hamiltonian transition and the target distribution then decreasing the step size will reduce and then ultimately remove the divergences entirely. If we do not have geometric ergodicity, however, then decreasing the step size will not completely remove the divergences.</p>
<p>Within Stan the step size is tuned automatically during warm up, but we can coerce smaller step sizes by tweaking the configuration of Stan’s adaptation routine. In particular, we can increase the <code>adapt_delta</code> parameter from its default value of 0.8 closer to its maximum value of 1.</p>
<pre class="r"><code>fit_cp85 &lt;- stan(file=&#39;eight_schools_cp.stan&#39;, data=input_data,
                 iter=11000, warmup=1000, chains=1, seed=483892929,
                 refresh=11000, control=list(adapt_delta=0.85))</code></pre>
<pre><code>
SAMPLING FOR MODEL &#39;eight_schools_cp&#39; NOW (CHAIN 1).

Chain 1, Iteration:     1 / 11000 [  0%]  (Warmup)
Chain 1, Iteration:  1001 / 11000 [  9%]  (Sampling)
Chain 1, Iteration: 11000 / 11000 [100%]  (Sampling)
 Elapsed Time: 0.08179 seconds (Warm-up)
               0.618241 seconds (Sampling)
               0.700031 seconds (Total)</code></pre>
<pre class="r"><code>fit_cp90 &lt;- stan(file=&#39;eight_schools_cp.stan&#39;, data=input_data,
                 iter=11000, warmup=1000, chains=1, seed=483892929,
                 refresh=11000, control=list(adapt_delta=0.90))</code></pre>
<pre><code>
SAMPLING FOR MODEL &#39;eight_schools_cp&#39; NOW (CHAIN 1).

Chain 1, Iteration:     1 / 11000 [  0%]  (Warmup)
Chain 1, Iteration:  1001 / 11000 [  9%]  (Sampling)
Chain 1, Iteration: 11000 / 11000 [100%]  (Sampling)
 Elapsed Time: 0.105562 seconds (Warm-up)
               0.835096 seconds (Sampling)
               0.940658 seconds (Total)</code></pre>
<pre class="r"><code>fit_cp95 &lt;- stan(file=&#39;eight_schools_cp.stan&#39;, data=input_data,
                 iter=11000, warmup=1000, chains=1, seed=483892929,
                 refresh=11000, control=list(adapt_delta=0.95))</code></pre>
<pre><code>
SAMPLING FOR MODEL &#39;eight_schools_cp&#39; NOW (CHAIN 1).

Chain 1, Iteration:     1 / 11000 [  0%]  (Warmup)
Chain 1, Iteration:  1001 / 11000 [  9%]  (Sampling)
Chain 1, Iteration: 11000 / 11000 [100%]  (Sampling)
 Elapsed Time: 0.122472 seconds (Warm-up)
               0.992679 seconds (Sampling)
               1.11515 seconds (Total)</code></pre>
<pre class="r"><code>fit_cp99 &lt;- stan(file=&#39;eight_schools_cp.stan&#39;, data=input_data,
                 iter=11000, warmup=1000, chains=1, seed=483892929,
                 refresh=11000, control=list(adapt_delta=0.99))</code></pre>
<pre><code>
SAMPLING FOR MODEL &#39;eight_schools_cp&#39; NOW (CHAIN 1).

Chain 1, Iteration:     1 / 11000 [  0%]  (Warmup)
Chain 1, Iteration:  1001 / 11000 [  9%]  (Sampling)
Chain 1, Iteration: 11000 / 11000 [100%]  (Sampling)
 Elapsed Time: 0.300047 seconds (Warm-up)
               1.20478 seconds (Sampling)
               1.50482 seconds (Total)</code></pre>
<p>Despite increasing <code>adapt_delta</code> and decreasing step size,</p>
<pre class="r"><code>adapt_delta=c(0.80, 0.85, 0.90, 0.95, 0.99)
step_scan=c(get_sampler_params(fit_cp80, inc_warmup=FALSE)[[1]][,&#39;stepsize__&#39;][1],
            get_sampler_params(fit_cp85, inc_warmup=FALSE)[[1]][,&#39;stepsize__&#39;][1],
            get_sampler_params(fit_cp90, inc_warmup=FALSE)[[1]][,&#39;stepsize__&#39;][1],
            get_sampler_params(fit_cp95, inc_warmup=FALSE)[[1]][,&#39;stepsize__&#39;][1],
            get_sampler_params(fit_cp99, inc_warmup=FALSE)[[1]][,&#39;stepsize__&#39;][1])

par(mar = c(4, 4, 0.5, 0.5))
plot(adapt_delta, step_scan, xlab=&quot;Adapt Delta&quot;, ylab=&quot;Adapted Step Size&quot;,
     xlim=c(0.79, 1.0), ylim=c(0, 0.2), col=c_dark, type=&quot;l&quot;, lwd=3)
points(adapt_delta, step_scan, col=c_dark, pch=16, cex=0.8)</code></pre>
<p><img src="divergences_and_bias_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>the number of divergent transitions remains nearly constant,</p>
<pre class="r"><code>div_scan=c(sum(params_cp80$divergent),
           sum(get_sampler_params(fit_cp85, inc_warmup=FALSE)[[1]][,&#39;divergent__&#39;]),
           sum(get_sampler_params(fit_cp90, inc_warmup=FALSE)[[1]][,&#39;divergent__&#39;]),
           sum(get_sampler_params(fit_cp95, inc_warmup=FALSE)[[1]][,&#39;divergent__&#39;]),
           sum(get_sampler_params(fit_cp99, inc_warmup=FALSE)[[1]][,&#39;divergent__&#39;]))

par(mar = c(4, 4, 0.5, 0.5))
plot(adapt_delta, div_scan, xlab=&quot;Adapt Delta&quot;, ylab=&quot;Number of Divergences&quot;,
     xlim=c(0.79, 1.0), ylim=c(0, 400), col=c_dark, type=&quot;l&quot;, lwd=3)
points(adapt_delta, div_scan, col=c_dark, pch=16, cex=0.8)</code></pre>
<p><img src="divergences_and_bias_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>This indicates that the Hamiltonian transition is not geometrically ergodic with respect to the centered implementation of the Eight Schools model. Indeed, this is expected given the observed bias.</p>
<p>This behavior also has a nice geometric intuition. The more we decrease the step size the more the Hamiltonian Markov chain can explore the neck of the funnel. Consequently, the marginal posterior distribution for <span class="math inline">\(\log \tau\)</span> stretches further and further towards negative values with the decreasing step size,</p>
<pre class="r"><code>common_breaks=14 * (0:60) / 60 - 9.5

p_cp80 &lt;- hist(log(extract(fit_cp80)$tau), breaks=common_breaks, plot=FALSE)
p_cp90 &lt;- hist(log(extract(fit_cp90)$tau), breaks=common_breaks, plot=FALSE)
p_cp99 &lt;- hist(log(extract(fit_cp99)$tau), breaks=common_breaks, plot=FALSE)

par(mar = c(4, 4, 0.5, 0.5))
plot(p_cp99, col=c_dark, main=&quot;&quot;, xlab=&quot;log(tau)&quot;, yaxt=&#39;n&#39;, ann=FALSE)
plot(p_cp90, col=c_mid, add=T)
plot(p_cp80, col=c_light, add=T)
legend(&quot;topleft&quot;,
       c(&quot;Centered, delta=0.80&quot;, &quot;Centered, delta=0.90&quot;, &quot;Centered, delta=0.99&quot;),
       fill=c(c_light, c_mid, c_dark), bty=&quot;n&quot;)</code></pre>
<p><img src="divergences_and_bias_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>The deeper into the funnel we explore, however, the more highly-curved and pathological it becomes. The chain with the largest <code>adapt_delta</code> pushes deeper into the neck of the funnel but still ends up diverging once it probes too far,</p>
<pre class="r"><code>params_cp99 &lt;- as.data.frame(extract(fit_cp99, permuted=FALSE))
names(params_cp99) &lt;- gsub(&quot;chain:1.&quot;, &quot;&quot;, names(params_cp99), fixed = TRUE)
names(params_cp99) &lt;- gsub(&quot;[&quot;, &quot;.&quot;, names(params_cp99), fixed = TRUE)
names(params_cp99) &lt;- gsub(&quot;]&quot;, &quot;&quot;, names(params_cp99), fixed = TRUE)

divergent &lt;- get_sampler_params(fit_cp99, inc_warmup=FALSE)[[1]][,&#39;divergent__&#39;]
params_cp99$divergent &lt;- divergent

div_params_cp99 &lt;- params_cp99[params_cp99$divergent == 1,]
nondiv_params_cp99 &lt;- params_cp99[params_cp99$divergent == 0,]

par(mar = c(4, 4, 0.5, 0.5))
plot(nondiv_params_cp99$theta.1, log(nondiv_params_cp99$tau),
     xlab=&quot;theta.1&quot;, ylab=&quot;log(tau)&quot;, xlim=c(-20, 50), ylim=c(-6,4),
     col=c_dark, pch=16, cex=0.8)
points(div_params_cp99$theta.1, log(div_params_cp99$tau),
       col=&quot;green&quot;, pch=16, cex=0.8)</code></pre>
<p><img src="divergences_and_bias_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>The improved exploration is evident when comparing the samples, here without labeling the divergences, from the chain with the default settings and the the <code>adapt_delta=0.99</code> chain,</p>
<pre class="r"><code>par(mar = c(4, 4, 0.5, 0.5))
plot(params_cp99$theta.1, log(params_cp99$tau),
     xlab=&quot;theta.1&quot;, ylab=&quot;log(tau)&quot;, xlim=c(-20, 50), ylim=c(-6,4),
     col=c_dark, pch=16, cex=0.8)
points(params_cp80$theta.1, log(params_cp80$tau), col=c_light, pch=16, cex=0.8)
legend(&quot;bottomright&quot;, c(&quot;Centered, delta=0.80&quot;, &quot;Centered, delta=0.99&quot;),
       fill=c(c_light, c_dark), border=&quot;white&quot;, bty=&quot;n&quot;)</code></pre>
<p><img src="divergences_and_bias_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>That said, the improved exploration given by decreasing the step size does not realize geometric ergodicity. While the bias does decrease with decreasing step size,</p>
<pre class="r"><code>params_cp90 &lt;- as.data.frame(extract(fit_cp90, permuted=FALSE))
names(params_cp90) &lt;- gsub(&quot;chain:1.&quot;, &quot;&quot;, names(params_cp90), fixed = TRUE)
names(params_cp90) &lt;- gsub(&quot;[&quot;, &quot;.&quot;, names(params_cp90), fixed = TRUE)
names(params_cp90) &lt;- gsub(&quot;]&quot;, &quot;&quot;, names(params_cp90), fixed = TRUE)

running_means_cp90 &lt;- sapply(1:1000, function(n) mean(log(params_cp90$tau)[1:(10*n)]))
running_means_cp99 &lt;- sapply(1:1000, function(n) mean(log(params_cp99$tau)[1:(10*n)]))

plot(10*(1:1000), running_means_cp80, col=c_light, pch=16, cex=0.8, ylim=c(0, 2),
    xlab=&quot;Iteration&quot;, ylab=&quot;MCMC mean of log(tau)&quot;)
points(10*(1:1000), running_means_cp90, col=c_mid, pch=16, cex=0.8)
points(10*(1:1000), running_means_cp99, col=c_dark, pch=16, cex=0.8)
abline(h=0.7657852, col=&quot;grey&quot;, lty=&quot;dashed&quot;, lwd=3)
legend(&quot;bottomright&quot;,
       c(&quot;Centered, delta=0.80&quot;, &quot;Centered, delta=0.90&quot;, &quot;Centered, delta=0.99&quot;),
       fill=c(c_light, c_mid, c_dark), border=&quot;white&quot;, bty=&quot;n&quot;)</code></pre>
<p><img src="divergences_and_bias_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>it never completely vanishes, even for the extreme setting of <code>adapt_delta=0.99</code>.</p>
</div>
</div>
<div id="a-non-centered-eight-schools-implementation" class="section level1">
<h1>A Non-Centered Eight Schools Implementation</h1>
<p>Although reducing the step size improves exploration, ultimately it only reveals the true extent the pathology in the centered implementation. Fortunately, there is another way to implement hierarchical models that does not suffer from the same pathologies.</p>
<p>In a non-centered parameterization we do not try to fit the group-level parameters directly, rather we fit a latent Gaussian variable from which we can recover the group-level parameters with a scaling and a translation,</p>
<p><span class="math display">\[\mu \sim \mathcal{N}(0, 5)\]</span></p>
<p><span class="math display">\[\tau \sim \text{Half-Cauchy}(0, 5)\]</span></p>
<p><span class="math display">\[\tilde{\theta}_{n} \sim \mathcal{N}(0, 1)\]</span></p>
<p><span class="math display">\[\theta_{n} = \mu + \tau \cdot \tilde{\theta}_{n}.\]</span></p>
<p>Because we are actively sampling from different parameters, we should expect, and indeed observe, a very different posterior distribution.</p>
<p>The Stan program for the non-centered parameterization is almost as simple as that for the centered parameterization,</p>
<pre class="r"><code>writeLines(readLines(&quot;eight_schools_ncp.stan&quot;))</code></pre>
<pre><code>data {
  int&lt;lower=0&gt; J;
  real y[J];
  real&lt;lower=0&gt; sigma[J];
}

parameters {
  real mu;
  real&lt;lower=0&gt; tau;
  real theta_tilde[J];
}

transformed parameters {
  real theta[J];
  for (j in 1:J)
    theta[j] = mu + tau * theta_tilde[j];
}

model {
  mu ~ normal(0, 5);
  tau ~ cauchy(0, 5);
  theta_tilde ~ normal(0, 1);
  y ~ normal(theta, sigma);
}</code></pre>
<p>Running the new model in Stan,</p>
<pre class="r"><code>fit_ncp80 &lt;- stan(file=&#39;eight_schools_ncp.stan&#39;, data=input_data,
                  iter=11000, warmup=1000, chains=1, seed=483892929,
                  refresh=11000)</code></pre>
<pre><code>
SAMPLING FOR MODEL &#39;eight_schools_ncp&#39; NOW (CHAIN 1).

Gradient evaluation took 6e-06 seconds
1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.
Adjust your expectations accordingly!


Iteration:     1 / 11000 [  0%]  (Warmup)
Iteration:  1001 / 11000 [  9%]  (Sampling)
Iteration: 11000 / 11000 [100%]  (Sampling)

 Elapsed Time: 0.030725 seconds (Warm-up)
               0.330795 seconds (Sampling)
               0.36152 seconds (Total)</code></pre>
<p>we see that the effective sample size per iteration has drastically improved,</p>
<pre class="r"><code>print(fit_ncp80)</code></pre>
<pre><code>Inference for Stan model: eight_schools_ncp.
1 chains, each with iter=11000; warmup=1000; thin=1; 
post-warmup draws per chain=10000, total post-warmup draws=10000.

                mean se_mean   sd   2.5%   25%   50%   75% 97.5% n_eff
mu              4.44    0.04 3.35  -2.21  2.17  4.47  6.71 10.88  7618
tau             3.58    0.04 3.18   0.12  1.23  2.76  5.00 11.80  7007
theta_tilde[1]  0.31    0.01 0.97  -1.61 -0.34  0.31  0.95  2.21 10000
theta_tilde[2]  0.11    0.01 0.95  -1.81 -0.52  0.12  0.74  1.98 10000
theta_tilde[3] -0.08    0.01 0.97  -1.99 -0.73 -0.09  0.59  1.81 10000
theta_tilde[4]  0.06    0.01 0.94  -1.77 -0.57  0.06  0.69  1.88 10000
theta_tilde[5] -0.16    0.01 0.94  -2.01 -0.79 -0.16  0.47  1.67  9512
theta_tilde[6] -0.09    0.01 0.91  -1.86 -0.69 -0.10  0.53  1.69 10000
theta_tilde[7]  0.35    0.01 0.96  -1.61 -0.29  0.38  1.00  2.17  8492
theta_tilde[8]  0.06    0.01 1.00  -1.90 -0.61  0.06  0.74  2.01  9300
theta[1]        6.20    0.06 5.56  -3.28  2.77  5.67  8.94 19.18  9280
theta[2]        5.03    0.05 4.73  -4.31  2.13  5.00  7.81 14.69 10000
theta[3]        3.97    0.06 5.27  -7.67  1.10  4.27  7.15 13.63  9052
theta[4]        4.77    0.05 4.78  -4.78  1.85  4.79  7.63 14.54 10000
theta[5]        3.63    0.05 4.72  -6.78  0.89  3.87  6.78 12.10 10000
theta[6]        4.02    0.05 4.85  -6.77  1.26  4.29  7.06 12.91 10000
theta[7]        6.25    0.05 5.04  -2.49  2.98  5.85  8.97 18.15 10000
theta[8]        4.84    0.06 5.54  -6.41  1.72  4.81  7.78 16.15  8291
lp__           -6.97    0.04 2.38 -12.53 -8.31 -6.63 -5.24 -3.35  4246
               Rhat
mu                1
tau               1
theta_tilde[1]    1
theta_tilde[2]    1
theta_tilde[3]    1
theta_tilde[4]    1
theta_tilde[5]    1
theta_tilde[6]    1
theta_tilde[7]    1
theta_tilde[8]    1
theta[1]          1
theta[2]          1
theta[3]          1
theta[4]          1
theta[5]          1
theta[6]          1
theta[7]          1
theta[8]          1
lp__              1

Samples were drawn using NUTS(diag_e) at Tue Jun 27 21:53:06 2017.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>and the trace plots no longer show any “stickyness”,</p>
<pre class="r"><code>params_ncp80 &lt;- as.data.frame(extract(fit_ncp80, permuted=FALSE))
names(params_ncp80) &lt;- gsub(&quot;chain:1.&quot;, &quot;&quot;, names(params_ncp80), fixed = TRUE)
names(params_ncp80) &lt;- gsub(&quot;[&quot;, &quot;.&quot;, names(params_ncp80), fixed = TRUE)
names(params_ncp80) &lt;- gsub(&quot;]&quot;, &quot;&quot;, names(params_ncp80), fixed = TRUE)
params_ncp80$iter &lt;- 1:10000

par(mar = c(4, 4, 0.5, 0.5))
plot(params_ncp80$iter, log(params_ncp80$tau), col=c_dark, pch=16, cex=0.8,
     xlab=&quot;Iteration&quot;, ylab=&quot;log(tau)&quot;, ylim=c(-6, 4))</code></pre>
<p><img src="divergences_and_bias_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p>We do, however, we do still see the rare divergence,</p>
<pre class="r"><code>divergent &lt;- get_sampler_params(fit_ncp80, inc_warmup=FALSE)[[1]][,&#39;divergent__&#39;]
sum(divergent)</code></pre>
<pre><code>[1] 0</code></pre>
<pre class="r"><code>sum(divergent) / 10000</code></pre>
<pre><code>[1] 0</code></pre>
<p>These infrequent divergences do not seem concentrate anywhere in parameter space,</p>
<pre class="r"><code>divergent &lt;- get_sampler_params(fit_ncp80, inc_warmup=FALSE)[[1]][,&#39;divergent__&#39;]
params_ncp80$divergent &lt;- divergent

div_params_ncp &lt;- params_ncp80[params_ncp80$divergent == 1,]
nondiv_params_ncp &lt;- params_ncp80[params_ncp80$divergent == 0,]

par(mar = c(4, 4, 0.5, 0.5))
plot(nondiv_params_ncp$theta.1, log(nondiv_params_ncp$tau),
     xlab=&quot;theta.1&quot;, ylab=&quot;log(tau)&quot;, xlim=c(-20, 50), ylim=c(-6,4),
     col=c_dark, pch=16, cex=0.8)
points(div_params_ncp$theta.1, log(div_params_ncp$tau),
       col=&quot;green&quot;, pch=16, cex=0.8)</code></pre>
<p><img src="divergences_and_bias_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<p>which is indicative of the divergences being false positives. As expected of false positives, we can remove the divergences entirely by decreasing the step size,</p>
<pre class="r"><code>fit_ncp90 &lt;- stan(file=&#39;eight_schools_ncp.stan&#39;, data=input_data,
                  iter=11000, warmup=1000, chains=1, seed=483892929,
                  refresh=11000, control=list(adapt_delta=0.90))</code></pre>
<pre><code>
SAMPLING FOR MODEL &#39;eight_schools_ncp&#39; NOW (CHAIN 1).

Gradient evaluation took 5e-06 seconds
1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
Adjust your expectations accordingly!


Iteration:     1 / 11000 [  0%]  (Warmup)
Iteration:  1001 / 11000 [  9%]  (Sampling)
Iteration: 11000 / 11000 [100%]  (Sampling)

 Elapsed Time: 0.03765 seconds (Warm-up)
               0.408745 seconds (Sampling)
               0.446395 seconds (Total)</code></pre>
<pre class="r"><code>divergent &lt;- get_sampler_params(fit_ncp90, inc_warmup=FALSE)[[1]][,&#39;divergent__&#39;]
sum(divergent)</code></pre>
<pre><code>[1] 0</code></pre>
<p>The more agreeable geometry of the non-centered implementation allows the Markov chain to explore deep into the neck of the funnel, capturing even the smallest values of <span class="math inline">\(\tau\)</span> that are consistent with the measurements,</p>
<pre class="r"><code>params_ncp90 &lt;- as.data.frame(extract(fit_ncp90, permuted=FALSE))
names(params_ncp90) &lt;- gsub(&quot;chain:1.&quot;, &quot;&quot;, names(params_ncp90), fixed = TRUE)
names(params_ncp90) &lt;- gsub(&quot;[&quot;, &quot;.&quot;, names(params_ncp90), fixed = TRUE)
names(params_ncp90) &lt;- gsub(&quot;]&quot;, &quot;&quot;, names(params_ncp90), fixed = TRUE)

par(mar = c(4, 4, 0.5, 0.5))
plot(params_ncp90$theta.1, log(params_ncp90$tau),
     xlab=&quot;theta.1&quot;, ylab=&quot;log(tau)&quot;, xlim=c(-20, 50), ylim=c(-6,4),
     col=c_dark_highlight, pch=16, cex=0.8)
points(params_cp99$theta.1, log(params_cp99$tau), col=c_dark, pch=16, cex=0.8)
points(params_cp90$theta.1, log(params_cp90$tau), col=c_mid, pch=16, cex=0.8)
legend(&quot;bottomright&quot;, c(&quot;Centered, delta=0.90&quot;, &quot;Centered, delta=0.99&quot;,
                        &quot;Non-Centered, delta=0.90&quot;),
       fill=c(c_mid, c_dark, c_dark_highlight), border=&quot;white&quot;, bty=&quot;n&quot;)</code></pre>
<p><img src="divergences_and_bias_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<pre class="r"><code>p_ncp90 &lt;- hist(log(params_ncp90$tau), breaks=common_breaks, plot=FALSE)

par(mar = c(4, 4, 0.5, 0.5))
plot(p_ncp90, col=c_dark_highlight, main=&quot;&quot;, xlab=&quot;log(tau)&quot;, yaxt=&#39;n&#39;, ann=FALSE)
plot(p_cp99, col=c_dark, add=T)
plot(p_cp90, col=c_mid, add=T)

legend(&quot;topleft&quot;, c(&quot;Centered, delta=0.90&quot;, &quot;Centered, delta=0.99&quot;,
                    &quot;Non-Centered, delta=0.90&quot;),
       fill=c(c_mid, c_dark, c_dark_highlight), bty=&quot;n&quot;)</code></pre>
<p><img src="divergences_and_bias_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<p>Consequently, MCMC estimators from the non-centered chain rapidly converge towards their true expectation values,</p>
<pre class="r"><code>running_means_ncp &lt;- sapply(1:1000, function(n) mean(log(params_ncp90$tau)[1:(10*n)]))

par(mar = c(4, 4, 0.5, 0.5))
plot(10*(1:1000), running_means_cp90, col=c_mid, pch=16, cex=0.8, ylim=c(0, 2),
    xlab=&quot;Iteration&quot;, ylab=&quot;MCMC mean of log(tau)&quot;)
points(10*(1:1000), running_means_cp99, col=c_dark, pch=16, cex=0.8)
points(10*(1:1000), running_means_ncp, col=c_dark_highlight, pch=16, cex=0.8)
abline(h=0.7657852, col=&quot;grey&quot;, lty=&quot;dashed&quot;, lwd=3)
legend(&quot;bottomright&quot;, c(&quot;Centered, delta=0.90&quot;, &quot;Centered, delta=0.99&quot;,
                        &quot;Non-Centered, delta=0.90&quot;),
       fill=c(c_mid, c_dark, c_dark_highlight), border=&quot;white&quot;, bty=&quot;n&quot;)</code></pre>
<p><img src="divergences_and_bias_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
</div>
<div id="why-wasnt-biased-estimation-previously-identified" class="section level1">
<h1>Why wasn’t Biased Estimation Previously Identified?</h1>
<p>We have seen in these examples that hierarchical models implemented with a centered parameterization impede the geometric ergodicity of Hamiltonian Monte Carlo, at least when the data are not extremely informative. The ultimate importance of the resulting bias, however, depends on the specific application.</p>
<p>If we are interested in the behavior of <span class="math inline">\(\log \tau\)</span> then this bias will be serious and affect the quality of any inferences drawn from the fit. Often, however, one is interested not in the behavior of <span class="math inline">\(\log \tau\)</span> but rather the behavior of <span class="math inline">\(\tau\)</span>. When we transform from the logarithmic scale to the linear scale the pathological region of the posterior is compressed into a small volume and the biases are somewhat mediated,</p>
<pre class="r"><code>breaks=20 * (0:50) / 50

p_cp &lt;- hist(params_cp90$tau[params_cp90$tau &lt; 20], breaks=breaks, plot=FALSE)
p_ncp &lt;- hist(params_ncp90$tau[params_ncp90$tau &lt; 20], breaks=breaks, plot=FALSE)

par(mar = c(4, 4, 0.5, 0.5))
plot(p_ncp, col=c_dark_highlight, main=&quot;&quot;, xlab=&quot;tau&quot;,
     yaxt=&#39;n&#39;, ann=FALSE, ylim=c(0, 1000))
plot(p_cp, col=c_mid, add=T)
legend(&quot;topright&quot;, c(&quot;Centered, delta=0.90&quot;, &quot;Non-Centered, delta=0.90&quot;),
       fill=c(c_mid, c_dark_highlight), bty=&quot;n&quot;)</code></pre>
<p><img src="divergences_and_bias_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<pre class="r"><code>running_means_cp &lt;- sapply(1:1000, function(n) mean(params_cp90$tau[1:(10*n)]))
running_means_ncp &lt;- sapply(1:1000, function(n) mean(params_ncp90$tau[1:(10*n)]))

par(mar = c(4, 4, 0.5, 0.5))
plot(10*(1:1000), running_means_cp, col=c_mid, pch=16, cex=0.8, ylim=c(2, 4.25),
    xlab=&quot;Iteration&quot;, ylab=&quot;MCMC mean of tau&quot;)
points(10*(1:1000), running_means_ncp, col=c_dark_highlight, pch=16, cex=0.8)
abline(h=3.575019, col=&quot;grey&quot;, lty=&quot;dashed&quot;, lwd=3)
legend(&quot;bottomright&quot;, c(&quot;Centered, delta=0.90&quot;, &quot;Non-Centered, delta=0.90&quot;),
       fill=c(c_mid, c_dark_highlight), border=&quot;white&quot;, bty=&quot;n&quot;)</code></pre>
<p><img src="divergences_and_bias_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
<p>Certainly the bias is not as obvious as it was on the logarithmic scale.</p>
<p>This mediation also affects how the bias propagates to other parameters in the model. When we average over both <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span>, the small bias of <span class="math inline">\(\tau\)</span> actually compels the marginal posterior for the group-level parameters to be <em>narrower</em> than they should be,</p>
<pre class="r"><code>breaks=80 * (0:50) / 50 - 25

p_cp &lt;- hist(params_cp90$theta.1, breaks=breaks, plot=FALSE)
p_ncp &lt;- hist(params_ncp90$theta.1, breaks=breaks, plot=FALSE)

par(mar = c(4, 4, 0.5, 0.5))
plot(p_ncp, col=c_dark_highlight, main=&quot;&quot;, xlab=&quot;theta.1&quot;, yaxt=&#39;n&#39;, ann=FALSE)
plot(p_cp, col=c_mid, add=T)

legend(&quot;topright&quot;, c(&quot;Centered, delta=0.90&quot;, &quot;Non-Centered, delta=0.90&quot;),
       fill=c(c_mid, c_dark_highlight), bty=&quot;n&quot;)</code></pre>
<p><img src="divergences_and_bias_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
<p>This narrowness does not strongly affect the accuracy of the mean of the group-level parameters,</p>
<pre class="r"><code>running_means_cp &lt;- sapply(1:1000, function(n) mean(params_cp90$theta.1[1:(10*n)]))
running_means_ncp &lt;- sapply(1:1000, function(n) mean(params_ncp90$theta.1[1:(10*n)]))

par(mar = c(4, 4, 0.5, 0.5))
plot(10*(1:1000), running_means_cp, col=c_mid, pch=16, cex=0.8, ylim=c(4, 7),
    xlab=&quot;Iteration&quot;, ylab=&quot;MCMC mean of theta.1&quot;)
points(10*(1:1000), running_means_ncp, col=c_dark_highlight, pch=16, cex=0.8)
abline(h=6.250004, col=&quot;grey&quot;, lty=&quot;dashed&quot;, lwd=3)
legend(&quot;bottomright&quot;, c(&quot;Centered, delta=0.90&quot;, &quot;Non-Centered, delta=0.90&quot;),
       fill=c(c_mid, c_dark_highlight), border=&quot;white&quot;, bty=&quot;n&quot;)</code></pre>
<p><img src="divergences_and_bias_files/figure-html/unnamed-chunk-37-1.png" width="672" /></p>
<p>but it does systematically underestimate the variance for many iterations,</p>
<pre class="r"><code>running_means_cp &lt;- sapply(1:1000, function(n) var(params_cp90$theta.1[1:(10*n)]))
running_means_ncp &lt;- sapply(1:1000, function(n) var(params_ncp90$theta.1[1:(10*n)]))

par(mar = c(4, 4, 0.5, 0.5))
plot(10*(1:1000), running_means_cp, col=c_mid, pch=16, cex=0.8, ylim=c(10, 40),
    xlab=&quot;Iteration&quot;, ylab=&quot;MCMC variance of theta.1&quot;)
points(10*(1:1000), running_means_ncp, col=c_dark_highlight, pch=16, cex=0.8)
abline(h=29.78573, col=&quot;grey&quot;, lty=&quot;dashed&quot;, lwd=3)
legend(&quot;bottomright&quot;, c(&quot;Centered, delta=0.90&quot;, &quot;Non-Centered, delta=0.90&quot;),
       fill=c(c_mid, c_dark_highlight), border=&quot;white&quot;, bty=&quot;n&quot;)</code></pre>
<p><img src="divergences_and_bias_files/figure-html/unnamed-chunk-38-1.png" width="672" /></p>
<p>In practice this bias can be hard to observe if the Markov chain is slow and the MCMC estimators are noisy, as is common when using older MCMC algorithms like Random Walk Metropolis and Gibbs samplers. This may help explain why the lack of geometric ergodicity in centered implementations of hierarchical models is so often overlooked in practice.</p>
<p>Ultimately, identifying the breakdown of geometric ergodicity for a given Markov transition and target distribution indicates only that there is a bias, not how significant that bias will be. The precise significance of this bias depends not only on the structure of the model but also on the details of how inferences from that model will be applied. Sometimes an analysis taking these factors into account can quantify the significance of the bias and potentially deem it acceptable. These analyses, however, are extremely subtle, challenging, and time-consuming. It is almost always easier to modify the model to restore geometric ergodicity and unbiased MCMC estimation.</p>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>Divergences are an incredibly sensitive diagnostic for the breakdown of geometric ergodicity and hence critical guidance for best implementing our models in practice. If the divergences can be removed by increasing <code>adapt_delta</code> then Hamiltonian Monte Carlo applied to the given implementation will yield accurate inferences. If the divergences persist, however, then the model will need to be reimplemented to avoid biases.</p>
</div>
<div id="acknowledgements" class="section level1">
<h1>Acknowledgements</h1>
<p>I thank Bob Carpenter for many helpful comments.</p>
</div>
<div id="original-computing-environment" class="section level1">
<h1>Original Computing Environment</h1>
<pre class="r"><code>writeLines(readLines(file.path(Sys.getenv(&quot;HOME&quot;), &quot;.R/Makevars&quot;)))</code></pre>
<pre><code>#CFLAGS =              -O3 -Wall -pipe -pedantic -std=gnu99 -march=native 
#CXXFLAGS =            -O3 -Wall -pipe -Wno-unused -pedantic -march=native </code></pre>
<pre class="r"><code>devtools::session_info(&quot;rstan&quot;)</code></pre>
<pre><code>Session info --------------------------------------------------------------</code></pre>
<pre><code> setting  value                       
 version  R version 3.3.3 (2017-03-06)
 system   x86_64, linux-gnu           
 ui       X11                         
 language (EN)                        
 collate  en_US.UTF-8                 
 tz       America/Los_Angeles         
 date     2017-06-27                  </code></pre>
<pre><code>Packages ------------------------------------------------------------------</code></pre>
<pre><code> package      * version   date       source        
 assertthat     0.1       2013-12-06 CRAN (R 3.3.0)
 BH             1.62.0-1  2016-11-19 CRAN (R 3.3.3)
 colorspace     1.2-6     2015-03-11 CRAN (R 3.3.0)
 dichromat      2.0-0     2013-01-24 CRAN (R 3.3.0)
 digest         0.6.11    2017-01-03 CRAN (R 3.3.2)
 ggplot2      * 2.2.1     2016-12-30 CRAN (R 3.3.2)
 gridExtra      2.2.1     2016-02-29 CRAN (R 3.3.0)
 gtable         0.2.0     2016-02-26 CRAN (R 3.3.0)
 inline         0.3.14    2015-04-13 CRAN (R 3.3.3)
 labeling       0.3       2014-08-23 CRAN (R 3.3.0)
 lattice        0.20-34   2016-09-06 CRAN (R 3.3.1)
 lazyeval       0.2.0     2016-06-12 CRAN (R 3.3.0)
 magrittr       1.5       2014-11-22 CRAN (R 3.3.0)
 MASS           7.3-45    2015-11-10 CRAN (R 3.2.5)
 Matrix         1.2-8     2017-01-20 CRAN (R 3.3.2)
 munsell        0.4.3     2016-02-13 CRAN (R 3.3.0)
 plyr           1.8.4     2016-06-08 CRAN (R 3.3.0)
 RColorBrewer   1.1-2     2014-12-07 CRAN (R 3.3.0)
 Rcpp           0.12.11   2017-05-22 CRAN (R 3.3.3)
 RcppEigen      0.3.3.3.0 2017-05-01 CRAN (R 3.3.3)
 reshape2       1.4.1     2014-12-06 CRAN (R 3.3.0)
 rlang          0.1.1     2017-05-18 CRAN (R 3.3.3)
 rstan        * 2.15.1    2017-04-19 CRAN (R 3.3.3)
 scales         0.4.1     2016-11-09 CRAN (R 3.3.2)
 StanHeaders  * 2.15.0-1  2017-04-19 CRAN (R 3.3.3)
 stringi        1.1.1     2016-05-27 CRAN (R 3.3.0)
 stringr        1.1.0     2016-08-19 CRAN (R 3.3.0)
 tibble         1.3.1     2017-05-17 CRAN (R 3.3.3)</code></pre>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-BetancourtEtAl:2015">
<p>Betancourt, Michael, and Mark Girolami. 2015. “Hamiltonian Monte Carlo for Hierarchical Models.” In <em>Current Trends in Bayesian Methodology with Applications</em>, edited by Umesh Singh Dipak K. Dey and A. Loganathan. Chapman &amp; Hall/CRC Press.</p>
</div>
<div id="ref-Rubin:1981">
<p>Rubin, Donald B. 1981. “Estimation in Parallel Randomized Experiments.” <em>Journal of Educational and Behavioral Statistics</em> 6 (4): 377–401.</p>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
