<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Max Joseph" />

<meta name="date" content="2016-08-20" />

<title>Exact sparse CAR models in Stan</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="site_libs/highlight/default.css"
      type="text/css" />
<script src="site_libs/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Exact sparse CAR models in Stan</h1>
<h4 class="author"><em>Max Joseph</em></h4>
<h4 class="date"><em>August 20, 2016</em></h4>

</div>


<p>This document details sparse exact conditional autoregressive (CAR) models in Stan as an extension of previous work on approximate sparse CAR models in Stan. Sparse representations seem to give order of magnitude efficiency gains, scaling better for large spatial data sets.</p>
<div id="car-priors-for-spatial-random-effects" class="section level2">
<h2>CAR priors for spatial random effects</h2>
<p>Conditional autoregressive (CAR) models are popular as prior distributions for spatial random effects with areal spatial data. If we have a random quantity <span class="math inline">\(\phi = (\phi_1, \phi_2, ..., \phi_n)&#39;\)</span> at <span class="math inline">\(n\)</span> areal locations, the CAR model is often expressed via full conditional distributions:</p>
<p><span class="math display">\[\phi_i \mid \phi_j, j \neq i \sim N(\alpha \sum_{j = 1}^n b_{ij} \phi_j, \tau_i^{-1})\]</span></p>
<p>where <span class="math inline">\(\tau_i\)</span> is a spatially varying precision parameter, and <span class="math inline">\(b_{ii} = 0\)</span>.</p>
<p>By Brook’s Lemma, the joint distribution of <span class="math inline">\(\phi\)</span> is then:</p>
<p><span class="math display">\[\phi \sim N(0, [D_\tau (I - \alpha B)]^{-1}).\]</span></p>
<p>If we assume the following:</p>
<ul>
<li><span class="math inline">\(D_\tau = \tau D\)</span></li>
<li><span class="math inline">\(D = diag(m_i)\)</span>: an <span class="math inline">\(n \times n\)</span> diagonal matrix with <span class="math inline">\(m_i\)</span> = the number of neighbors for location <span class="math inline">\(i\)</span></li>
<li><span class="math inline">\(I\)</span>: an <span class="math inline">\(n \times n\)</span> identity matrix</li>
<li><span class="math inline">\(\alpha\)</span>: a parameter that controls spatial dependence (<span class="math inline">\(\alpha = 0\)</span> implies spatial independence, and <span class="math inline">\(\alpha = 1\)</span> collapses to an <em>intrisnic conditional autoregressive</em> (IAR) specification)</li>
<li><span class="math inline">\(B = D^{-1} W\)</span>: the scaled adjacency matrix</li>
<li><span class="math inline">\(W\)</span>: the adjacency matrix (<span class="math inline">\(w_{ii} = 0, w_{ij} = 1\)</span> if <span class="math inline">\(i\)</span> is a neighbor of <span class="math inline">\(j\)</span>, and <span class="math inline">\(w_{ij}=0\)</span> otherwise)</li>
</ul>
<p>then the CAR prior specification simplifies to:</p>
<p><span class="math display">\[\phi \sim N(0, [\tau (D - \alpha W)]^{-1}).\]</span></p>
<p>The <span class="math inline">\(\alpha\)</span> parameter ensures propriety of the joint distrbution of <span class="math inline">\(\phi\)</span> as long as <span class="math inline">\(| \alpha | &lt; 1\)</span> (Gelfand &amp; Vounatsou 2003). However, <span class="math inline">\(\alpha\)</span> is often taken as 1, leading to the IAR specification which creates a singular precision matrix and an improper prior distribution.</p>
</div>
<div id="a-poisson-specification" class="section level2">
<h2>A Poisson specification</h2>
<p>Suppose we have aggregated count data <span class="math inline">\(y_1, y_2, ..., y_n\)</span> at <span class="math inline">\(n\)</span> locations, and we expect that neighboring locations will have similar counts. With a Poisson likelihood:</p>
<p><span class="math display">\[y_i \sim \text{Poisson}(\text{exp}(X_{i} \beta + \phi_i + \log(\text{offset}_i)))\]</span></p>
<p>where <span class="math inline">\(X_i\)</span> is a design vector (the <span class="math inline">\(i^{th}\)</span> row from a design matrix), <span class="math inline">\(\beta\)</span> is a vector of coefficients, <span class="math inline">\(\phi_i\)</span> is a spatial adjustment, and <span class="math inline">\(\log(\text{offset}_i)\)</span> accounts for differences in expected values or exposures at the spatial units (popular choices include area for physical processes, or population size for disease applications).</p>
<p>If we specify a proper CAR prior for <span class="math inline">\(\phi\)</span>, then we have that <span class="math inline">\(\phi \sim \text{N}(0, [\tau (D - \alpha W)]^{-1})\)</span> where <span class="math inline">\(\tau (D - \alpha W)\)</span> is the precision matrix <span class="math inline">\(\Sigma^{-1}\)</span>. A complete Bayesian specification would include priors for the remaining parameters <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\tau\)</span>, and <span class="math inline">\(\beta\)</span>, such that our posterior distribution is:</p>
<p><span class="math display">\[p(\phi, \beta, \alpha, \tau \mid y) \propto p(y \mid \beta, \phi) p(\phi \mid \alpha, \tau) p(\alpha) p(\tau) p(\beta)\]</span></p>
</div>
<div id="example-scottish-lip-cancer-data" class="section level2">
<h2>Example: Scottish lip cancer data</h2>
<p>To demonstrate this approach we’ll use the Scottish lip cancer data example (some documentation <a href="https://cran.r-project.org/web/packages/CARBayesdata/CARBayesdata.pdf">here</a>). This data set includes observed lip cancer case counts at 56 spatial units in Scotland, with an expected number of cases to be used as an offset, and an area-specific continuous covariate that represents the proportion of the population employed in agriculture, fishing, or forestry. The model structure is identical to the Poisson model outlined above.</p>
<pre><code>## Warning in gpclibPermit(): support for gpclib will be withdrawn from
## maptools at the next major release</code></pre>
<pre><code>## [1] TRUE</code></pre>
<pre><code>## Warning: use rgdal::readOGR or sf::st_read</code></pre>
<p><img src="mbjoseph-CARStan_files/figure-html/make-scotland-map-1.png" width="672" /></p>
<p>Let’s start by loading packages and data, specifying the number of MCMC iterations and chains.</p>
<pre class="r"><code>library(ggmcmc)
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
source(&#39;data/scotland_lip_cancer.RData&#39;)

# Define MCMC parameters 
niter &lt;- 1E4   # definitely overkill, but good for comparison
nchains &lt;- 4</code></pre>
<p>To fit the full model, we’ll pull objects loaded with our Scotland lip cancer data. I’ll use <code>model.matrix</code> to generate a design matrix, centering and scaling the continuous covariate <code>x</code> to reduce correlation between the intercept and slope estimates.</p>
<pre class="r"><code>W &lt;- A # adjacency matrix
scaled_x &lt;- c(scale(x))
X &lt;- model.matrix(~scaled_x)
  
full_d &lt;- list(n = nrow(X),         # number of observations
               p = ncol(X),         # number of coefficients
               X = X,               # design matrix
               y = O,               # observed number of cases
               log_offset = log(E), # log(expected) num. cases
               W = W)               # adjacency matrix</code></pre>
<div id="stan-implementation-car-with-multi_normal_prec" class="section level4">
<h4>Stan implementation: CAR with <code>multi_normal_prec</code></h4>
<p>Our model statement mirrors the structure outlined above, with explicit normal and gamma priors on <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\tau\)</span> respectively, and a <span class="math inline">\(\text{Uniform}(0, 1)\)</span> prior for <span class="math inline">\(\alpha\)</span>. The prior on <span class="math inline">\(\phi\)</span> is specified via the <code>multi_normal_prec</code> function, passing in <span class="math inline">\(\tau (D - \alpha W)\)</span> as the precision matrix.</p>
<pre><code>data {
  int&lt;lower = 1&gt; n;
  int&lt;lower = 1&gt; p;
  matrix[n, p] X;
  int&lt;lower = 0&gt; y[n];
  vector[n] log_offset;
  matrix&lt;lower = 0, upper = 1&gt;[n, n] W;
}
transformed data{
  vector[n] zeros;
  matrix&lt;lower = 0&gt;[n, n] D;
  {
    vector[n] W_rowsums;
    for (i in 1:n) {
      W_rowsums[i] = sum(W[i, ]);
    }
    D = diag_matrix(W_rowsums);
  }
  zeros = rep_vector(0, n);
}
parameters {
  vector[p] beta;
  vector[n] phi;
  real&lt;lower = 0&gt; tau;
  real&lt;lower = 0, upper = 1&gt; alpha;
}
model {
  phi ~ multi_normal_prec(zeros, tau * (D - alpha * W));
  beta ~ normal(0, 1);
  tau ~ gamma(2, 2);
  y ~ poisson_log(X * beta + phi + log_offset);
}</code></pre>
<p>Fitting the model with <code>rstan</code>:</p>
<pre class="r"><code>full_fit &lt;- stan(&#39;stan/car_prec.stan&#39;, data = full_d, 
                 iter = niter, chains = nchains, verbose = FALSE)
print(full_fit, pars = c(&#39;beta&#39;, &#39;tau&#39;, &#39;alpha&#39;, &#39;lp__&#39;))</code></pre>
<pre><code>## Inference for Stan model: car_prec.
## 4 chains, each with iter=10000; warmup=5000; thin=1; 
## post-warmup draws per chain=5000, total post-warmup draws=20000.
## 
##           mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat
## beta[1]  -0.01    0.01 0.28  -0.56  -0.17  -0.01   0.14   0.57   525    1
## beta[2]   0.27    0.00 0.10   0.08   0.21   0.27   0.33   0.45  4891    1
## tau       1.64    0.01 0.50   0.86   1.29   1.58   1.93   2.81  5511    1
## alpha     0.93    0.00 0.06   0.77   0.91   0.95   0.98   1.00  4083    1
## lp__    820.73    0.09 6.69 806.64 816.49 821.10 825.39 832.80  4971    1
## 
## Samples were drawn using NUTS(diag_e) at Tue Jun 27 22:17:39 2017.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
<pre class="r"><code># visualize results 
to_plot &lt;- c(&#39;beta&#39;, &#39;tau&#39;, &#39;alpha&#39;, &#39;phi[1]&#39;, &#39;phi[2]&#39;, &#39;phi[3]&#39;, &#39;lp__&#39;)
traceplot(full_fit, pars = to_plot)</code></pre>
<p><img src="mbjoseph-CARStan_files/figure-html/fit-prec-model-1.png" width="672" /></p>
</div>
<div id="a-more-efficient-sparse-representation" class="section level3">
<h3>A more efficient sparse representation</h3>
<p>Although we could specify our multivariate normal prior for <span class="math inline">\(\phi\)</span> directly in Stan via <code>multi_normal_prec</code>, as we did above, in this case we will accrue computational efficiency gains by manually specifying <span class="math inline">\(p(\phi \mid \tau, \alpha)\)</span> directly via the log probability accumulator. The log probability of <span class="math inline">\(\phi\)</span> is:</p>
<p><span class="math display">\[\log(p(\phi \mid \tau, \alpha)) = - \frac{n}{2} \log(2 \pi) + \frac{1}{2} \log(\text{det}( \Sigma^{-1})) - \frac{1}{2} \phi^T \Sigma^{-1} \phi\]</span></p>
<p>In Stan, we only need the log posterior up to an additive constant so we can drop the first term. Then, substituting <span class="math inline">\(\tau (D - \alpha W)\)</span> for <span class="math inline">\(\Sigma^{-1}\)</span>:</p>
<p><span class="math display">\[\frac{1}{2} \log(\text{det}(\tau (D - \alpha W))) - \frac{1}{2} \phi^T \Sigma^{-1} \phi\]</span></p>
<p><span class="math display">\[ = \frac{1}{2} \log(\tau ^ n \text{det}(D - \alpha W)) - \frac{1}{2} \phi^T \Sigma^{-1} \phi\]</span></p>
<p><span class="math display">\[ = \frac{n}{2} \log(\tau) + \frac{1}{2} \log(\text{det}(D - \alpha W)) - \frac{1}{2} \phi^T \Sigma^{-1} \phi\]</span></p>
<p>There are two ways that we can accrue computational efficiency gains:</p>
<ol style="list-style-type: decimal">
<li><p>Sparse representations of <span class="math inline">\(\Sigma^{-1}\)</span> to expedite computation of <span class="math inline">\(\phi^T \Sigma^{-1} \phi\)</span> (this work was done by Kyle foreman previously, e.g., <a href="https://groups.google.com/d/topic/stan-users/M7T7EIlyhoo/discussion" class="uri">https://groups.google.com/d/topic/stan-users/M7T7EIlyhoo/discussion</a>).</p></li>
<li><p>Efficient computation of the determinant. Jin, Carlin, and Banerjee (2005) show that:</p></li>
</ol>
<p><span class="math display">\[\text{det}(D - \alpha W) \propto \prod_{i = 1}^n (1 - \alpha \lambda_i)\]</span></p>
<p>where <span class="math inline">\(\lambda_1, ..., \lambda_n\)</span> are the eigenvalues of <span class="math inline">\(D^{-\frac{1}{2}} W D^{-\frac{1}{2}}\)</span>, which can be computed ahead of time and passed in as data. Because we only need the log posterior up to an additive constant, we can use this result which is proportional up to some multiplicative constant <span class="math inline">\(c\)</span>:</p>
<p><span class="math display">\[\frac{n}{2} \log(\tau) + \frac{1}{2} \log(c \prod_{i = 1}^n (1 - \alpha \lambda_i)) - \frac{1}{2} \phi^T \Sigma^{-1} \phi\]</span></p>
<p><span class="math display">\[= \frac{n}{2} \log(\tau) + \frac{1}{2} \log(c) +  \frac{1}{2} \log(\prod_{i = 1}^n (1 - \alpha \lambda_i)) - \frac{1}{2} \phi^T \Sigma^{-1} \phi\]</span></p>
<p>Again dropping additive constants:</p>
<p><span class="math display">\[\frac{n}{2} \log(\tau) + \frac{1}{2} \log(\prod_{i = 1}^n (1 - \alpha \lambda_i)) - \frac{1}{2} \phi^T \Sigma^{-1} \phi\]</span></p>
<p><span class="math display">\[= \frac{n}{2} \log(\tau) + \frac{1}{2} \sum_{i = 1}^n \log(1 - \alpha \lambda_i) - \frac{1}{2} \phi^T \Sigma^{-1} \phi\]</span></p>
</div>
<div id="stan-implementation-sparse-car" class="section level3">
<h3>Stan implementation: sparse CAR</h3>
<p>In the Stan model statement’s <code>transformed data</code> block, we compute <span class="math inline">\(\lambda_1, ..., \lambda_n\)</span> (the eigenvalues of <span class="math inline">\(D^{-\frac{1}{2}} W D^{-\frac{1}{2}}\)</span>), and generate a sparse representation for W (<code>Wsparse</code>), which is assumed to be symmetric, such that the adjacency relationships can be represented in a two column matrix where each row is an adjacency relationship between two sites.</p>
<p>The Stan model statement for the sparse implementation never constructs the precision matrix, and does not call any of the <code>multi_normal*</code> functions. Instead, we use define a <code>sparse_car_lpdf()</code> function and use it in the model block.</p>
<pre><code>functions {
  /**
  * Return the log probability of a proper conditional autoregressive (CAR) prior 
  * with a sparse representation for the adjacency matrix
  *
  * @param phi Vector containing the parameters with a CAR prior
  * @param tau Precision parameter for the CAR prior (real)
  * @param alpha Dependence (usually spatial) parameter for the CAR prior (real)
  * @param W_sparse Sparse representation of adjacency matrix (int array)
  * @param n Length of phi (int)
  * @param W_n Number of adjacent pairs (int)
  * @param D_sparse Number of neighbors for each location (vector)
  * @param lambda Eigenvalues of D^{-1/2}*W*D^{-1/2} (vector)
  *
  * @return Log probability density of CAR prior up to additive constant
  */
  real sparse_car_lpdf(vector phi, real tau, real alpha, 
    int[,] W_sparse, vector D_sparse, vector lambda, int n, int W_n) {
      row_vector[n] phit_D; // phi&#39; * D
      row_vector[n] phit_W; // phi&#39; * W
      vector[n] ldet_terms;
    
      phit_D = (phi .* D_sparse)&#39;;
      phit_W = rep_row_vector(0, n);
      for (i in 1:W_n) {
        phit_W[W_sparse[i, 1]] = phit_W[W_sparse[i, 1]] + phi[W_sparse[i, 2]];
        phit_W[W_sparse[i, 2]] = phit_W[W_sparse[i, 2]] + phi[W_sparse[i, 1]];
      }
    
      for (i in 1:n) ldet_terms[i] = log1m(alpha * lambda[i]);
      return 0.5 * (n * log(tau)
                    + sum(ldet_terms)
                    - tau * (phit_D * phi - alpha * (phit_W * phi)));
  }
}
data {
  int&lt;lower = 1&gt; n;
  int&lt;lower = 1&gt; p;
  matrix[n, p] X;
  int&lt;lower = 0&gt; y[n];
  vector[n] log_offset;
  matrix&lt;lower = 0, upper = 1&gt;[n, n] W; // adjacency matrix
  int W_n;                // number of adjacent region pairs
}
transformed data {
  int W_sparse[W_n, 2];   // adjacency pairs
  vector[n] D_sparse;     // diagonal of D (number of neigbors for each site)
  vector[n] lambda;       // eigenvalues of invsqrtD * W * invsqrtD
  
  { // generate sparse representation for W
  int counter;
  counter = 1;
  // loop over upper triangular part of W to identify neighbor pairs
    for (i in 1:(n - 1)) {
      for (j in (i + 1):n) {
        if (W[i, j] == 1) {
          W_sparse[counter, 1] = i;
          W_sparse[counter, 2] = j;
          counter = counter + 1;
        }
      }
    }
  }
  for (i in 1:n) D_sparse[i] = sum(W[i]);
  {
    vector[n] invsqrtD;  
    for (i in 1:n) {
      invsqrtD[i] = 1 / sqrt(D_sparse[i]);
    }
    lambda = eigenvalues_sym(quad_form(W, diag_matrix(invsqrtD)));
  }
}
parameters {
  vector[p] beta;
  vector[n] phi;
  real&lt;lower = 0&gt; tau;
  real&lt;lower = 0, upper = 1&gt; alpha;
}
model {
  phi ~ sparse_car(tau, alpha, W_sparse, D_sparse, lambda, n, W_n);
  beta ~ normal(0, 1);
  tau ~ gamma(2, 2);
  y ~ poisson_log(X * beta + phi + log_offset);
}</code></pre>
<p>Fitting the model:</p>
<pre class="r"><code>sp_d &lt;- list(n = nrow(X),         # number of observations
             p = ncol(X),         # number of coefficients
             X = X,               # design matrix
             y = O,               # observed number of cases
             log_offset = log(E), # log(expected) num. cases
             W_n = sum(W) / 2,    # number of neighbor pairs
             W = W)               # adjacency matrix

sp_fit &lt;- stan(&#39;stan/car_sparse.stan&#39;, data = sp_d, 
               iter = niter, chains = nchains, verbose = FALSE)

print(sp_fit, pars = c(&#39;beta&#39;, &#39;tau&#39;, &#39;alpha&#39;, &#39;lp__&#39;))</code></pre>
<pre><code>## Inference for Stan model: car_sparse.
## 4 chains, each with iter=10000; warmup=5000; thin=1; 
## post-warmup draws per chain=5000, total post-warmup draws=20000.
## 
##           mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat
## beta[1]  -0.01    0.01 0.28  -0.60  -0.16  -0.01   0.15   0.58   714    1
## beta[2]   0.27    0.00 0.10   0.08   0.21   0.27   0.33   0.46  4424    1
## tau       1.64    0.01 0.50   0.86   1.29   1.58   1.93   2.79  5937    1
## alpha     0.93    0.00 0.06   0.77   0.91   0.95   0.98   0.99  4438    1
## lp__    782.79    0.10 6.79 768.51 778.42 783.18 787.58 795.01  4679    1
## 
## Samples were drawn using NUTS(diag_e) at Tue Jun 27 22:18:30 2017.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
<pre class="r"><code>traceplot(sp_fit, pars = to_plot)</code></pre>
<p><img src="mbjoseph-CARStan_files/figure-html/fit-sparse-model-1.png" width="672" /></p>
</div>
<div id="mcmc-efficiency-comparison" class="section level3">
<h3>MCMC Efficiency comparison</h3>
<p>The main quantity of interest is the effective number of samples per unit time. Sparsity gives us an order of magnitude or so gains, mostly via reductions in run time.</p>
<table>
<thead>
<tr class="header">
<th align="left">Model</th>
<th align="right">Number of effective samples</th>
<th align="right">Elapsed time (sec)</th>
<th align="right">Effective samples / sec)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">full</td>
<td align="right">4971.357</td>
<td align="right">547.48570</td>
<td align="right">9.080341</td>
</tr>
<tr class="even">
<td align="left">sparse</td>
<td align="right">4679.154</td>
<td align="right">55.02817</td>
<td align="right">85.031976</td>
</tr>
</tbody>
</table>
</div>
<div id="posterior-distribution-comparison" class="section level3">
<h3>Posterior distribution comparison</h3>
<p>Let’s compare the estimates to make sure that we get the same answer with both approaches. In this case, I’ve used more MCMC iterations than we would typically need in to get a better estimate of the tails of each marginal posterior distribution so that we can compare the 95% credible intervals among the two approaches.</p>
<p><img src="mbjoseph-CARStan_files/figure-html/compare-parameter-estimates-1.png" width="672" /></p>
<p><img src="mbjoseph-CARStan_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>The two approaches give the same answers (more or less, with small differences arising due to MCMC sampling error).</p>
</div>
</div>
<div id="postscript-sparse-iar-specification" class="section level2">
<h2>Postscript: sparse IAR specification</h2>
<p>Although the IAR prior for <span class="math inline">\(\phi\)</span> that results from <span class="math inline">\(\alpha = 1\)</span> is improper, it remains popular (Besag, York, and Mollie, 1991). In practice, these models are typically fit with a sum to zero constraints: <span class="math inline">\(\sum_{i\text{ in connected coponent}} \phi_i = 0\)</span> for each connected component of the graph. This allows us to interpret both the overall mean and the component-wise means.</p>
<p>With <span class="math inline">\(\alpha\)</span> fixed to one, we have:</p>
<p><span class="math display">\[\log(p(\phi \mid \tau)) = - \frac{n}{2} \log(2 \pi) + \frac{1}{2} \log(\text{det}^*(\tau (D - W))) - \frac{1}{2} \phi^T \tau (D - W) \phi\]</span></p>
<p><span class="math display">\[ = - \frac{n}{2} \log(2 \pi) + \frac{1}{2} \log(\tau^{n-k} \text{det}^*(D - W)) - \frac{1}{2} \phi^T \tau (D - W) \phi\]</span></p>
<p><span class="math display">\[ = - \frac{n}{2} \log(2 \pi) + \frac{1}{2} \log(\tau^{n-k}) + \frac{1}{2} \log(\text{det}^*(D - W)) - \frac{1}{2} \phi^T \tau (D - W) \phi\]</span></p>
<p>Here <span class="math inline">\(\text{det}^*(A)\)</span> is the generalized determinant of the square matrix <span class="math inline">\(A\)</span> defined as the product of its non-zero eigenvalues, and <span class="math inline">\(k\)</span> is the number of connected components in the graph. For the Scottish Lip Cancer data, there is only one connected component and <span class="math inline">\(k=1\)</span>. The reason that we need to use the generalized determinant is that the precision matrix is, by definition, singular in intrinsic models as the support of the Gaussian distribution is on a subspace with fewer than <span class="math inline">\(n\)</span> dimensions. For the classical ICAR(1) model, we know that the directions correpsonding to the zero eigenvalues are exactly the vectors that are constant on each connected component of the graph and hence <span class="math inline">\(k\)</span> is the number of connected components.</p>
<p>Dropping additive constants, the quantity to increment becomes:</p>
<p><span class="math display">\[ \frac{1}{2} \log(\tau^{n-k}) - \frac{1}{2} \phi^T \tau (D - W) \phi\]</span></p>
<p>And the corresponding Stan syntax would be:</p>
<pre><code>functions {
  /**
  * Return the log probability of a proper intrinsic autoregressive (IAR) prior 
  * with a sparse representation for the adjacency matrix
  *
  * @param phi Vector containing the parameters with a IAR prior
  * @param tau Precision parameter for the IAR prior (real)
  * @param W_sparse Sparse representation of adjacency matrix (int array)
  * @param n Length of phi (int)
  * @param W_n Number of adjacent pairs (int)
  * @param D_sparse Number of neighbors for each location (vector)
  * @param lambda Eigenvalues of D^{-1/2}*W*D^{-1/2} (vector)
  *
  * @return Log probability density of IAR prior up to additive constant
  */
  real sparse_iar_lpdf(vector phi, real tau,
    int[,] W_sparse, vector D_sparse, vector lambda, int n, int W_n) {
      row_vector[n] phit_D; // phi&#39; * D
      row_vector[n] phit_W; // phi&#39; * W
      vector[n] ldet_terms;
    
      phit_D = (phi .* D_sparse)&#39;;
      phit_W = rep_row_vector(0, n);
      for (i in 1:W_n) {
        phit_W[W_sparse[i, 1]] = phit_W[W_sparse[i, 1]] + phi[W_sparse[i, 2]];
        phit_W[W_sparse[i, 2]] = phit_W[W_sparse[i, 2]] + phi[W_sparse[i, 1]];
      }
    
      return 0.5 * ((n-1) * log(tau)
                    - tau * (phit_D * phi - (phit_W * phi)));
  }
}
data {
  int&lt;lower = 1&gt; n;
  int&lt;lower = 1&gt; p;
  matrix[n, p] X;
  int&lt;lower = 0&gt; y[n];
  vector[n] log_offset;
  matrix&lt;lower = 0, upper = 1&gt;[n, n] W; // adjacency matrix
  int W_n;                // number of adjacent region pairs
}
transformed data {
  int W_sparse[W_n, 2];   // adjacency pairs
  vector[n] D_sparse;     // diagonal of D (number of neigbors for each site)
  vector[n] lambda;       // eigenvalues of invsqrtD * W * invsqrtD
  
  { // generate sparse representation for W
  int counter;
  counter = 1;
  // loop over upper triangular part of W to identify neighbor pairs
    for (i in 1:(n - 1)) {
      for (j in (i + 1):n) {
        if (W[i, j] == 1) {
          W_sparse[counter, 1] = i;
          W_sparse[counter, 2] = j;
          counter = counter + 1;
        }
      }
    }
  }
  for (i in 1:n) D_sparse[i] = sum(W[i]);
  {
    vector[n] invsqrtD;  
    for (i in 1:n) {
      invsqrtD[i] = 1 / sqrt(D_sparse[i]);
    }
    lambda = eigenvalues_sym(quad_form(W, diag_matrix(invsqrtD)));
  }
}
parameters {
  vector[p] beta;
  vector[n] phi_unscaled;
  real&lt;lower = 0&gt; tau;
}
transformed parameters {
  vector[n] phi; // brute force centering
  phi = phi_unscaled - mean(phi_unscaled);
}
model {
  phi_unscaled ~ sparse_iar(tau, W_sparse, D_sparse, lambda, n, W_n);
  beta ~ normal(0, 1);
  tau ~ gamma(2, 2);
  y ~ poisson_log(X * beta + phi + log_offset);
}</code></pre>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<p>Besag, Julian, Jeremy York, and Annie Mollié. “Bayesian image restoration, with two applications in spatial statistics.” Annals of the institute of statistical mathematics 43.1 (1991): 1-20.</p>
<p>Gelfand, Alan E., and Penelope Vounatsou. “Proper multivariate conditional autoregressive models for spatial data analysis.” Biostatistics 4.1 (2003): 11-15.</p>
<p>Jin, Xiaoping, Bradley P. Carlin, and Sudipto Banerjee. “Generalized hierarchical multivariate CAR models for areal data.” Biometrics 61.4 (2005): 950-961.</p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
