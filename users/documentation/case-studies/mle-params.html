<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Bob Carpenter" />

<meta name="date" content="2016-04-24" />

<title>The Impact of Reparameterization on Point Estimates</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="site_libs/highlight/default.css"
      type="text/css" />
<script src="site_libs/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="fluid-row" id="header">



<h1 class="title toc-ignore">The Impact of Reparameterization on Point Estimates</h1>
<h4 class="author"><em>Bob Carpenter</em></h4>
<h4 class="date"><em>24 April 2016</em></h4>

</div>


<div id="abstract" class="section level3">
<h3>Abstract</h3>
<p>When changing variables, a Jacobian adjustment needs to be provided to account for the rate of change of the transform. Applying the adjustment ensures that inferences that are based on expectations over the posterior are invariant under reparameterizations. In contrast, the posterior mode changes as a result of the reparameterization. In this note, we use Stan to code a repeated binary trial model parameterized by chance of success, along with its reparameterization in terms of log odds in order to demonstrate the effect of the Jacobian adjustment on the Bayesian posterior and the posterior mode. We contrast the posterior mode to the maximum likelihood estimate, which, like the Bayesian estimates, is invariant under reparameterization. Along the way, we derive the logistic distribution by transforming a uniformly distributed variable.</p>
</div>
<div id="visualizing-a-change-of-variables" class="section level2">
<h2>Visualizing a Change of Variables</h2>
<p>Suppose <span class="math inline">\(\theta \in (0,1)\)</span> represents a probability of success. Then the odds ratio is <span class="math inline">\(\frac{\theta}{1 - \theta} \in (0, \infty)\)</span>. For example, if <span class="math inline">\(\theta = 0.75\)</span>, the odds ratio is <span class="math inline">\(\frac{0.75}{1 - 0.75} = 3\)</span>, whereas if <span class="math inline">\(\theta = 0.25\)</span>, the odds ratio is <span class="math inline">\(\frac{0.25}{1 -  0.25} = \frac{1}{3}\)</span>. An odds ratio of <span class="math inline">\(3\)</span> is conventionally written as <span class="math inline">\(3{:}1\)</span> and pronounced as “three to one odds” (in favor of success), wheras an odds ratio of <span class="math inline">\(\frac{1}{3}\)</span> is written as <span class="math inline">\(1{:}3\)</span> and prononced “one to three odds” (in favor, or “three to one odds against”). If <span class="math inline">\(\theta = 0.5\)</span>, then the odds are <span class="math inline">\(1{:}1\)</span>, or “even”.</p>
<p>The log odds is just the logarithm of the odds, and the function <span class="math inline">\(\mathrm{logit}:(0,1) \rightarrow \mathbb{R}\)</span> converts chance of success to log odds by <span class="math display">\[
 \mathrm{logit}(\theta)
\ = \
\log \frac{\theta}{1 - \theta}.
\]</span> The nice thing about log odds is that they are symmetric around zero. If the chance of success is <span class="math inline">\(0.5\)</span>, the odds ratio is <span class="math inline">\(1\)</span>, and the log odds are <span class="math inline">\(0\)</span>. If the chance of success is <span class="math inline">\(0.75\)</span> then the log odds are just <span class="math inline">\(\log 3\)</span>, or about <span class="math inline">\(1.10\)</span>, whereas if the chance of success is <span class="math inline">\(0.25\)</span>, the log odds are just <span class="math inline">\(\log \frac{1}{3} = -\log 3\)</span>, or about <span class="math inline">\(-1.10\)</span>. The log-odds transform has the symmetry <span class="math display">\[
\mathrm{logit}(\theta)
\ = \
-\mathrm{logit}(1 - \theta),
\]</span> which is useful for defining symmetric priors on log odds centered around zero.</p>
<p>Now suppose we have a uniformly-distributed random variable <span class="math inline">\(\theta \sim \mathsf{Uniform}(0,1)\)</span> representing a chance of success. We can draw a large sample and plot a histogram.</p>
<pre class="r"><code>theta &lt;- runif(100000, 0, 1);
hist(theta, breaks=20);</code></pre>
<p><img src="mle-params_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>It’s clear this is a nice uniform distribution. Now what happens if we log-odds transform the draws?</p>
<pre class="r"><code>logit &lt;- function(u) log(u / (1 - u));
logit_theta &lt;- logit(theta);
hist(logit_theta, breaks=20);</code></pre>
<p><img src="mle-params_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Although the distribution of <span class="math inline">\(\theta\)</span> is uniform, the distribution of <span class="math inline">\(\mathrm{logit}(\theta)\)</span> is most certainly not. The non-linear change of variables stretches the distribution in some places more than other. As usual, this rate of change is quantified by the derivative, as we will see below in the definition of the Jacobian. In this case, we can define the density of <span class="math inline">\(\mathrm{logit}(z)\)</span> analytically in terms of the density of <span class="math inline">\(\theta\)</span> and its derivatives; the result is the logistic distribution, as shown below.</p>
</div>
<div id="binary-trial-data" class="section level2">
<h2>Binary Trial Data</h2>
<p>We will assume a very simple data set <span class="math inline">\(y_1,\ldots,y_N\)</span>, consisting of <span class="math inline">\(N\)</span> repeated binary trials with a chance <span class="math inline">\(\theta \in [0, 1]\)</span> of success. Let’s start by creating a data set of <span class="math inline">\(N = 10\)</span> observations and parameter <span class="math inline">\(\theta = 0.3\)</span>.</p>
<pre class="r"><code>set.seed(123);
theta &lt;- 0.3;
N &lt;- 10;
y &lt;- rbinom(N, 1, theta);
y;</code></pre>
<pre><code>##  [1] 0 1 0 1 1 0 0 1 0 0</code></pre>
<p>Now the maximum likelihood estimate <span class="math inline">\(\theta^*\)</span> for <span class="math inline">\(\theta\)</span> in this case is easy to compute, being just the proportion of successes.</p>
<pre class="r"><code>theta_star &lt;- sum(y) / N;
theta_star;</code></pre>
<pre><code>## [1] 0.4</code></pre>
<p>Don’t worry that <span class="math inline">\(\theta^*\)</span> isn’t the same as <span class="math inline">\(\theta\)</span>; discrepancies arise due to sampling variation. Try several runs of the binomial generation and look at <code>sum(y)</code> to get a feeling for the variation due to sampling. If you want to know how to derive the maximum likelihood estimate here, differentiate the log density <span class="math inline">\(\log p(y \, | \, N, \theta)\)</span> with respect to <span class="math inline">\(\theta\)</span> and set it equal to zero and solve for <span class="math inline">\(\theta\)</span>; the solution is <span class="math inline">\(\theta^*\)</span> as defined above (hint: convert to binomial form using the sum of <span class="math inline">\(y\)</span> as a sufficient statistic and note that the binomial coefficients are constant).</p>
</div>
<div id="model-1-chance-of-success-parameterization" class="section level2">
<h2>Model 1: Chance-of-Success Parameterization</h2>
<p>The first model involves a direct parameterization of the joint density of the chance-of-success <span class="math inline">\(\theta\)</span> and observed data <span class="math inline">\(y\)</span>. <span class="math display">\[
p(\theta, y \, | \, N)
\ \propto \
p(y \, | \, \theta, N) \, p(\theta)
\]</span> The likelihood is defined by independent Bernoulli draws, <span class="math display">\[
p(y \, | \, \theta, N)
\ = \
\prod_{n=1}^N
\mathsf{Bernoulli}(y_n \, | \, \theta).
\]</span> We can assume a uniform prior for the sake of simplicity, but it is not a critical detail. <span class="math display">\[
p(\theta)
\ = \
\mathsf{Uniform}(\theta \, | \, 0, 1)
\ = \
1.
\]</span> The number of observations, <span class="math inline">\(N\)</span>, is not modeled, and so remains on the right-side of the conditioning bar in the equations.</p>
<p>The model is straightforward to program in Stan.</p>
<div id="stan-model-1-prob.stan" class="section level4">
<h4>Stan Model 1: <tt>prob.stan</tt></h4>
<pre><code>data { 
  int&lt;lower=0&gt; N; 
  int&lt;lower=0, upper=1&gt; y[N]; 
} 
parameters { 
  real&lt;lower=0, upper=1&gt; theta; 
} 
model { 
  for (n in 1:N) 
    y[n] ~ bernoulli(theta); 
  theta ~ uniform(0, 1); 
} </code></pre>
<p>It would be more efficient to vectorize the Bernoulli and drop the constant uniform, leaving just</p>
<pre><code>model {
  y ~ bernoulli(theta);
}</code></pre>
<p>The long form is used to illustrate the direct encoding of the model’s likelihood and prior.</p>
</div>
<div id="fitting-the-model" class="section level4">
<h4>Fitting the Model</h4>
<p>First, we have to load the <code>rstan</code> package,</p>
<pre class="r"><code>library(rstan);</code></pre>
<p>Then we can compile the model as follows.</p>
<pre class="r"><code>model_prob &lt;- stan_model(&quot;prob.stan&quot;);</code></pre>
<pre><code>In file included from /home/aaronjg/R/x86_64-pc-linux-gnu-library/3.3/BH/include/boost/config.hpp:39:0,
                 from /home/aaronjg/R/x86_64-pc-linux-gnu-library/3.3/BH/include/boost/math/tools/config.hpp:13,
                 from /home/aaronjg/R/x86_64-pc-linux-gnu-library/3.3/StanHeaders/include/stan/math/rev/core/var.hpp:7,
                 from /home/aaronjg/R/x86_64-pc-linux-gnu-library/3.3/StanHeaders/include/stan/math/rev/core/gevv_vvv_vari.hpp:5,
                 from /home/aaronjg/R/x86_64-pc-linux-gnu-library/3.3/StanHeaders/include/stan/math/rev/core.hpp:12,
                 from /home/aaronjg/R/x86_64-pc-linux-gnu-library/3.3/StanHeaders/include/stan/math/rev/mat.hpp:4,
                 from /home/aaronjg/R/x86_64-pc-linux-gnu-library/3.3/StanHeaders/include/stan/math.hpp:4,
                 from /home/aaronjg/R/x86_64-pc-linux-gnu-library/3.3/StanHeaders/include/src/stan/model/model_header.hpp:4,
                 from file16175cac6580.cpp:8:
/home/aaronjg/R/x86_64-pc-linux-gnu-library/3.3/BH/include/boost/config/compiler/gcc.hpp:186:0: warning: &quot;BOOST_NO_CXX11_RVALUE_REFERENCES&quot; redefined
 #  define BOOST_NO_CXX11_RVALUE_REFERENCES
 ^
&lt;command-line&gt;:0:0: note: this is the location of the previous definition</code></pre>
<p>With a compiled model, we can calculate the posterior mode given the names of the data variables.</p>
<pre class="r"><code>fit_pmode_prob &lt;- optimizing(model_prob, data=c(&quot;N&quot;, &quot;y&quot;));</code></pre>
<pre><code>Initial log joint probability = -7.06237
Optimization terminated normally: 
  Convergence detected: relative gradient magnitude is below tolerance</code></pre>
<pre class="r"><code>fit_pmode_prob;</code></pre>
<pre><code>$par
    theta 
0.4000028 

$value
[1] -6.730117

$return_code
[1] 0</code></pre>
<p>As the prior is uniform in <span class="math inline">\(\theta\)</span>, the posterior mode for <span class="math inline">\(\theta\)</span> is equal to maximum likelihood estimate <span class="math inline">\(\theta^*\)</span> calculated analytically above.</p>
<p>Next, we can fit the same model using Stan’s default MCMC algorithm for Bayesian posteriors.</p>
<pre class="r"><code>fit_bayes_prob &lt;- sampling(model_prob, data=c(&quot;N&quot;, &quot;y&quot;),
                           iter=10000, refresh=5000);
print(fit_bayes_prob, probs=c(0.1, 0.5, 0.9));</code></pre>
<pre><code>Inference for Stan model: prob.
4 chains, each with iter=10000; warmup=5000; thin=1; 
post-warmup draws per chain=5000, total post-warmup draws=20000.

       mean se_mean   sd   10%   50%   90% n_eff Rhat
theta  0.41    0.00 0.14  0.24  0.41  0.59  6744    1
lp__  -8.67    0.01 0.74 -9.56 -8.38 -8.16  6722    1

Samples were drawn using NUTS(diag_e) at Tue Jun 27 21:15:11 2017.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>The posterior mean is 0.42, not 0.4. The reason for this is that the posterior is not symmetric around its mode, but rather skewed to the right. In the case of this simple Bernoulli example with uniform prior, the posterior is available in closed form (because it is conjugate), with form <span class="math display">\[
p(\theta \, | \, y) = \mathsf{Beta}(\theta \, | \, 1 + \mathrm{sum}(y), 1 + N -
\mathrm{sum}(y))
\]</span> where <span class="math inline">\(\mathrm{sum}(y) = \sum_{n=1}^N y_n\)</span>. In our case, with <span class="math inline">\(N = 10\)</span> and <span class="math inline">\(\mathrm{sum}(y) = 4\)</span>, we get a posterior distribution <span class="math inline">\(p(\theta \, | \, y) = \mathsf{Beta}(\theta \, | \, 5, 7)\)</span>, and we know that if <span class="math inline">\(\theta \sim \mathsf{Beta}(5, 7)\)</span>, then <span class="math inline">\(\theta\)</span> has a mean of<span class="math display">\[
\bar{\theta} = \frac{5}{5 + 7} = 0.416666\cdots \approx 0.42,
\]</span> and a mode of <span class="math display">\[
\theta^{M} = \frac{5 - 1}{5 + 7 - 2} = 0.40.
\]</span> In other words, Stan is producing the correct results here.</p>
</div>
</div>
<div id="model-2-log-odds-parameterization-without-jacobian" class="section level2">
<h2>Model 2: Log Odds Parameterization without Jacobian</h2>
<p>Model 1 used the chance of success <span class="math inline">\(\theta \in [0, 1]\)</span> as a parameter. A popular alternative parameterization is the log odds, because it leads directly to the use of predictors in logistic regression. Model 2 uses a log-odds transformed parameter, but fails to include the Jacobian adjustment for the nonlinear change of variables.</p>
<div id="stan-model-2-logodds.stan" class="section level4">
<h4>Stan Model 2: <tt>logodds.stan</tt></h4>
<pre><code>data { 
  int&lt;lower=0&gt; N; 
  int&lt;lower=0, upper=1&gt; y[N]; 
} 
parameters { 
  real alpha; 
} 
transformed parameters { 
  real&lt;lower=0, upper=1&gt; theta; 
  theta &lt;- inv_logit(alpha); 
} 
model { 
  for (n in 1:N) 
    y[n] ~ bernoulli(theta); 
  theta ~ uniform(0, 1); 
} </code></pre>
</div>
<div id="fitting-model-2" class="section level4">
<h4>Fitting Model 2</h4>
<p>The model is compiled as before.</p>
<pre class="r"><code>model_logodds &lt;- stan_model(&quot;logodds.stan&quot;);</code></pre>
<pre><code>In file included from /home/aaronjg/R/x86_64-pc-linux-gnu-library/3.3/BH/include/boost/config.hpp:39:0,
                 from /home/aaronjg/R/x86_64-pc-linux-gnu-library/3.3/BH/include/boost/math/tools/config.hpp:13,
                 from /home/aaronjg/R/x86_64-pc-linux-gnu-library/3.3/StanHeaders/include/stan/math/rev/core/var.hpp:7,
                 from /home/aaronjg/R/x86_64-pc-linux-gnu-library/3.3/StanHeaders/include/stan/math/rev/core/gevv_vvv_vari.hpp:5,
                 from /home/aaronjg/R/x86_64-pc-linux-gnu-library/3.3/StanHeaders/include/stan/math/rev/core.hpp:12,
                 from /home/aaronjg/R/x86_64-pc-linux-gnu-library/3.3/StanHeaders/include/stan/math/rev/mat.hpp:4,
                 from /home/aaronjg/R/x86_64-pc-linux-gnu-library/3.3/StanHeaders/include/stan/math.hpp:4,
                 from /home/aaronjg/R/x86_64-pc-linux-gnu-library/3.3/StanHeaders/include/src/stan/model/model_header.hpp:4,
                 from file161734843ae2.cpp:8:
/home/aaronjg/R/x86_64-pc-linux-gnu-library/3.3/BH/include/boost/config/compiler/gcc.hpp:186:0: warning: &quot;BOOST_NO_CXX11_RVALUE_REFERENCES&quot; redefined
 #  define BOOST_NO_CXX11_RVALUE_REFERENCES
 ^
&lt;command-line&gt;:0:0: note: this is the location of the previous definition</code></pre>
<p>But now the compiler produces a warning to alert the user that they may need to apply a Jacobian adjustment because of the change of variables (we ignore the warning for now, but heed it in the next section).</p>
<pre><code>DIAGNOSTIC(S) FROM PARSER:Warning (non-fatal):
Left-hand side of sampling statement (~) may contain a non-linear transform of a parameter or local variable.
If so, you need to call increment_log_prob() with the log absolute determinant of the Jacobian of the transform.
Left-hand-side of sampling statement:
    theta ~ uniform(...)</code></pre>
<p>The fit proceeds as usual.</p>
<pre class="r"><code>fit_pmode_logodds &lt;- optimizing(model_logodds, data=c(&quot;N&quot;, &quot;y&quot;));</code></pre>
<pre><code>Initial log joint probability = -7.57025
Optimization terminated normally: 
  Convergence detected: relative gradient magnitude is below tolerance</code></pre>
<pre class="r"><code>fit_pmode_logodds;</code></pre>
<pre><code>$par
     alpha      theta 
-0.4054646  0.4000001 

$value
[1] -6.730117

$return_code
[1] 0</code></pre>
<pre class="r"><code>inv_logit &lt;- function(alpha) 1 / (1 + exp(-alpha));
inv_logit(fit_pmode_logodds$par[[&quot;alpha&quot;]])</code></pre>
<pre><code>[1] 0.4000001</code></pre>
<p>Note that when we transform this incorrectly computed ‘posterior mode’ for log odds back to a chance-of-success, it is the same as before. But now consider what happens with the posterior.</p>
<pre class="r"><code>print(fit_bayes_logodds, probs=c(0.1, 0.5, 0.9));</code></pre>
<pre><code>Inference for Stan model: logodds.
4 chains, each with iter=10000; warmup=5000; thin=1; 
post-warmup draws per chain=5000, total post-warmup draws=20000.

       mean se_mean   sd   10%   50%   90% n_eff Rhat
alpha -0.45    0.01 0.68 -1.31 -0.43  0.41  7079    1
theta  0.40    0.00 0.15  0.21  0.39  0.60  7127    1
lp__  -7.25    0.01 0.74 -8.12 -6.96 -6.74  8974    1

Samples were drawn using NUTS(diag_e) at Tue Jun 27 21:16:02 2017.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>Model 2 uses the log odds as a parameterization, but because it does not apply an appropriate Jacobian adjustment, it produces the wrong Bayesian posterior. That means that optimization does not produce the correct posterior mode (aka, maximum a posteriori or MAP estimate).</p>
</div>
</div>
<div id="jacobian-adjustments" class="section level2">
<h2>Jacobian Adjustments</h2>
<p>The Jacobian for a univariate transform is the absolute derivative of the inverse of the transform; it’s called “Jacobian” because in the general multivariate case, it’s the absolute determinant of the Jacobian matrix of the transform. More concretely, suppose we have a variable <span class="math inline">\(\theta\)</span> distributed as <span class="math inline">\(p(\theta)\)</span>, and I want to consider the distribution of a variable <span class="math inline">\(\alpha = f(\theta)\)</span>. To preserve probability mass (intervals in the univariate setting) under change of variables, the resulting density is <span class="math display">\[
p(\alpha)
\ = \
p(f^{-1}(\alpha))
\,
\left|
\frac{d}{d\alpha} f^{-1}(\alpha)
\right|,
\]</span> where the absolute value term is the Jacobian adjustment.</p>
<p>The inverse of the logit function, <span class="math inline">\(\mathrm{logit}^{-1} : \mathbb{R} \rightarrow (0,1)\)</span>, is known as the logistic sigmoid function, and defined as follows. <span class="math display">\[
\mathrm{logit}^{-1}(\alpha)
\ = \
\frac{1}{1 + \exp(-\alpha)}.
\]</span></p>
<p>In our particular case, where <span class="math inline">\(f = \mathrm{logit}\)</span>, the Jacobian adjustment is <span class="math display">\[
\left|
\frac{d}{d\alpha} \mathrm{logit}^{-1}(\alpha)
\right|
\ = \
\mathrm{logit}^{-1}(\alpha) \times (1 - \mathrm{logit}^{-1}(\alpha)).
\]</span> Writing it out in full, <span class="math display">\[
p(\alpha)
\ = \
p(\mathrm{logit}^{-1}(\alpha))
\times \mathrm{logit}^{-1}(\alpha)
\times (1 - \mathrm{logit}^{-1}(\alpha)).
\]</span></p>
<p>The logit function and its inverse are built into Stan as <code>logit</code> and <code>inv_logit</code>; Stan also provides many compound functions involving the log odds transform.</p>
</div>
<div id="model-3-log-odds-with-jacobian-adjustment" class="section level2">
<h2>Model 3: Log Odds with Jacobian Adjustment</h2>
<p>In this section, we apply the Jacobian adjustment we derived in the previous section, and see what it does to the posterior mode, posterior mean, and posterior quantiles (median or intervals).</p>
<div id="stan-model-3-logodds-jac.stan" class="section level4">
<h4>Stan Model 3: <tt>logodds-jac.stan</tt></h4>
<p>The last model we consider is the version of Model 2 with the appropriate Jacobian correction so that <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\mathrm{logit}^{-1}(\alpha)\)</span> have the same distribution. The Jacobian was derived in the previous section, and is coded up directly in Stan.</p>
<pre><code>data { 
  int&lt;lower=0&gt; N; 
  int&lt;lower=0, upper=1&gt; y[N]; 
} 
parameters { 
  real alpha; 
} 
transformed parameters { 
  real&lt;lower=0, upper=1&gt; theta; 
  theta &lt;- inv_logit(alpha); 
} 
model { 
  for (n in 1:N) 
    y[n] ~ bernoulli(theta); 
  theta ~ uniform(0, 1); 
  increment_log_prob(log(theta) + log(1 - theta)); 
} </code></pre>
<p>This is not the most efficient Stan model; the likelihood may be implemented with <code>bernoulli_logit</code> rather than an explicit application of <code>inv_logit</code>, the likelihood may be vectorized, the constant uniform may be dropped, and the efficient and stable <code>log_inv_logit</code> and <code>log1m_inv_logit</code> may be used for the Jacobian calculation. The more arithmetically stable form (and the more efficient form if <code>theta</code> is not available) is as follows.</p>
<pre><code>model {
  y ~ bernoulli_logit(alpha);
  increment_log_prob(log_inv_logit(alpha));
  increment_log_prob(log1m_inv_logit(alpha));
}</code></pre>
</div>
<div id="fitting-model-3" class="section level4">
<h4>Fitting Model 3</h4>
<p>The steps are exactly the same as before.</p>
<pre class="r"><code>model_logodds_jac &lt;- stan_model(&quot;logodds-jac.stan&quot;);</code></pre>
<pre><code>In file included from /home/aaronjg/R/x86_64-pc-linux-gnu-library/3.3/BH/include/boost/config.hpp:39:0,
                 from /home/aaronjg/R/x86_64-pc-linux-gnu-library/3.3/BH/include/boost/math/tools/config.hpp:13,
                 from /home/aaronjg/R/x86_64-pc-linux-gnu-library/3.3/StanHeaders/include/stan/math/rev/core/var.hpp:7,
                 from /home/aaronjg/R/x86_64-pc-linux-gnu-library/3.3/StanHeaders/include/stan/math/rev/core/gevv_vvv_vari.hpp:5,
                 from /home/aaronjg/R/x86_64-pc-linux-gnu-library/3.3/StanHeaders/include/stan/math/rev/core.hpp:12,
                 from /home/aaronjg/R/x86_64-pc-linux-gnu-library/3.3/StanHeaders/include/stan/math/rev/mat.hpp:4,
                 from /home/aaronjg/R/x86_64-pc-linux-gnu-library/3.3/StanHeaders/include/stan/math.hpp:4,
                 from /home/aaronjg/R/x86_64-pc-linux-gnu-library/3.3/StanHeaders/include/src/stan/model/model_header.hpp:4,
                 from file161769692b97.cpp:8:
/home/aaronjg/R/x86_64-pc-linux-gnu-library/3.3/BH/include/boost/config/compiler/gcc.hpp:186:0: warning: &quot;BOOST_NO_CXX11_RVALUE_REFERENCES&quot; redefined
 #  define BOOST_NO_CXX11_RVALUE_REFERENCES
 ^
&lt;command-line&gt;:0:0: note: this is the location of the previous definition</code></pre>
<pre class="r"><code>fit_pmode_logodds_jac &lt;- optimizing(model_logodds_jac, data=c(&quot;N&quot;, &quot;y&quot;));</code></pre>
<pre><code>Initial log joint probability = -9.01541
Optimization terminated normally: 
  Convergence detected: relative gradient magnitude is below tolerance</code></pre>
<pre class="r"><code>fit_pmode_logodds_jac;</code></pre>
<pre><code>$par
     alpha      theta 
-0.3364722  0.4166667 

$value
[1] -8.150319

$return_code
[1] 0</code></pre>
<pre class="r"><code>inv_logit(fit_pmode_logodds_jac$par[[&quot;alpha&quot;]])</code></pre>
<pre><code>[1] 0.4166667</code></pre>
<p>Note that this answer is different from the result obtained from model 1, demonstrating that the posterior mode depends on parameterization. But now consider what happens in the Bayesian setting.</p>
<pre class="r"><code>print(fit_bayes_logodds_jac, probs=c(0.1, 0.5, 0.9));</code></pre>
<pre><code>Inference for Stan model: logodds-jac.
4 chains, each with iter=10000; warmup=5000; thin=1; 
post-warmup draws per chain=5000, total post-warmup draws=20000.

       mean se_mean   sd   10%   50%   90% n_eff Rhat
alpha -0.36    0.01 0.60 -1.14 -0.36  0.40  7799    1
theta  0.42    0.00 0.14  0.24  0.41  0.60  7869    1
lp__  -8.66    0.01 0.71 -9.53 -8.38 -8.16  7653    1

Samples were drawn using NUTS(diag_e) at Tue Jun 27 21:16:53 2017.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>All is now well again after the Jacobian adjustment, with the posterior distribution of <span class="math inline">\(\theta\)</span> matching that of the original model, thus preserving Bayesian inferences, such as posterior predictions for new data points. Similarly, the posterior mean (estimate minimizing expected square error) and median (estimate minimizing expected absolute error) are preserved.</p>
<p>Unlike the posterior mean, the posterior mode changed when we reparameterized the model. This highlights that posterior modes (i.e., MAP estimates) are not true Bayesian quantities, but mere side effects of parameterizations. In contrast, the Maximum Likehood Estimate, like the Bayesian quantities, is indenpendent of the parameterization of theta.</p>
</div>
</div>
<div id="the-logistic-distribution" class="section level2">
<h2>The Logistic Distribution</h2>
<div id="probability-density-function" class="section level4">
<h4>Probability density function</h4>
<p>Suppose <span class="math inline">\(\theta \sim \mathsf{Uniform}(0,1)\)</span>. Then <span class="math inline">\(\mathrm{logit}(\theta)\)</span> has a unit logistic distribution. We have already derived the probability density function, which is simply the uniform probability distribution times the Jacobian, or in symbols, <span class="math display">\[
\mathsf{Logistic}(y)
\ = \
\mathrm{logit}^{-1}(y) \times (1 - \mathrm{logit}^{-1}(y))
\ = \
\frac{\exp(-y)}{(1 + \exp(-y))^2}.
\]</span></p>
</div>
<div id="cumulative-distribution-function" class="section level4">
<h4>Cumulative distribution function</h4>
<p>It should come as no surprise at this point that the logistic sigmoid <span class="math inline">\(\mathrm{logit}^{-1}(\alpha)\)</span> is the cumulative distribution function for the logistic distribution, because differentiating it produces the logistic density function.</p>
</div>
<div id="scaling-and-centering" class="section level4">
<h4>Scaling and centering</h4>
<p>As with other distributions, the unit form of the distribution may be translated by a location parameter <span class="math inline">\(\mu\)</span> and scaled by a parameter <span class="math inline">\(\sigma\)</span>, to produce the general logistic distribution <span class="math display">\[
\mathsf{Logistic}(y \, | \, \mu, \sigma)
\ = \
\mathsf{Logistic}\left( \frac{y - \mu}{\sigma} \right).
\]</span> We’ll leave it as an exercise for the reader to work out the algebra and show this is the same as the <a href="https://en.wikipedia.org/wiki/Logistic_distribution">definition in the Wikipedia</a>.</p>
</div>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>To sum up, here’s a handy table that contrasts the results.</p>
<table>
<thead>
<tr class="header">
<th align="left">Parameterization</th>
<th align="left">Posterior Mode</th>
<th align="left">Posterior Mean</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">chance of success</td>
<td align="left">0.40</td>
<td align="left">0.42</td>
</tr>
<tr class="even">
<td align="left">log odds without Jacobian</td>
<td align="left">0.40</td>
<td align="left">0.40</td>
</tr>
<tr class="odd">
<td align="left">log odds with Jacobian</td>
<td align="left">0.42</td>
<td align="left">0.42</td>
</tr>
</tbody>
</table>
<p>The moral of the story is that full Bayesian inference is insensitive to parameterization as long as the approprieate Jacobian adjustment is applied. In contrast, posterior modes (i.e., maximum a posteriori estimates) do change under reparameterization, and thus are <a href="https://en.wikipedia.org/wiki/No_true_Scotsman">no true Bayesian quantity</a>.</p>
<p><br /> <br /></p>
<hr />
</div>
<div id="acknowledgements" class="section level2">
<h2>Acknowledgements</h2>
<p>Thanks to Stijn de Waele for corrections and clarifications on the first version of this document.</p>
<p><br /></p>
<div id="licensing" class="section level5">
<h5>Licensing</h5>
<p><small>The code in this document is copyrighted by Columbia University and licensed under the <a href="https://opensource.org/licenses/BSD-3-Clause">new BSD (3-clause)</a> license. The text is copyrighted by Bob Carpenter and licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY 4.0</a>.</small></p>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
