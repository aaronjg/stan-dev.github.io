<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Daniel C. Furr" />

<meta name="date" content="2017-06-28" />

<title>Hierarchical two-parameter logistic item response model</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="site_libs/highlight/default.css"
      type="text/css" />
<script src="site_libs/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="hierarchical_2pl_files/styles.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Hierarchical two-parameter logistic item response model</h1>
<h4 class="author"><em>Daniel C. Furr</em></h4>
<h4 class="date"><em>June 28, 2017</em></h4>

</div>

<div id="TOC">
<ul>
<li><a href="#model"><span class="toc-section-number">1</span> Model</a><ul>
<li><a href="#overview"><span class="toc-section-number">1.1</span> Overview</a></li>
<li><a href="#stan-program"><span class="toc-section-number">1.2</span> <strong>Stan</strong> program</a></li>
</ul></li>
<li><a href="#simulation"><span class="toc-section-number">2</span> Simulation</a></li>
<li><a href="#example-application"><span class="toc-section-number">3</span> Example application</a></li>
<li><a href="#references"><span class="toc-section-number">4</span> References</a></li>
</ul>
</div>

<div id="model" class="section level1">
<h1><span class="header-section-number">1</span> Model</h1>
<div id="overview" class="section level2">
<h2><span class="header-section-number">1.1</span> Overview</h2>
<p>The two-parameter logistic model (2PL) is an item response theory model that includes parameters for both the difficulty and discrimination of items. A hierarchical extension, presented here, models these item parameter pairs as correlated draws from a bivariate normal distribution. This model is similar to the hierarchical three-parameter logistic model proposed by <span class="citation">Glas and van der Linden (2003)</span>.</p>
<p><span class="math display">\[ 
\mathrm{logit} [ \Pr(y_{ij} = 1 | \theta_j, \alpha_i, \beta_i) ] = 
  \alpha_i (\theta_j - \beta_i) 
\]</span> <span class="math display">\[ 
\log \alpha_i, \beta_i \sim \mathrm{MVN}(\mu_1, \mu_2, \Sigma)
\]</span> <span class="math display">\[ 
\theta_p \sim \mathrm{N}(0, 1) 
\]</span></p>
<p>Variables:</p>
<ul>
<li><span class="math inline">\(i = 1 \ldots I\)</span> indexes items</li>
<li><span class="math inline">\(j = 1 \ldots J\)</span> indexes persons</li>
<li><span class="math inline">\(y_{ij} \in \{ 0,1 \}\)</span> is the response of person <span class="math inline">\(j\)</span> to item <span class="math inline">\(i\)</span></li>
</ul>
<p>Parameters:</p>
<ul>
<li><span class="math inline">\(\alpha_i\)</span> is the discrimination for item <span class="math inline">\(i\)</span></li>
<li><span class="math inline">\(\beta_i\)</span> is the difficulty for item <span class="math inline">\(i\)</span></li>
<li><span class="math inline">\(\theta_j\)</span> is the ability for person <span class="math inline">\(j\)</span></li>
<li><span class="math inline">\(\mu_1\)</span> is the mean for <span class="math inline">\(\log \alpha_i\)</span></li>
<li><span class="math inline">\(\mu_2\)</span> is the mean for <span class="math inline">\(\beta_i\)</span></li>
<li><span class="math inline">\(\Sigma\)</span> is the covariance matrix for <span class="math inline">\(\log \alpha_i\)</span> and <span class="math inline">\(\beta_i\)</span></li>
</ul>
<p>Priors:</p>
<ul>
<li><span class="math inline">\(\mu_1 \sim \mathrm{N}(0,1)\)</span> is a weakly informative prior for the mean of the log discrimination parameters.</li>
<li><span class="math inline">\(\mu_2 \sim \mathrm{N}(0,25)\)</span> is a weakly informative prior for the mean of the difficulty parameters.</li>
<li>Let <span class="math inline">\(\tau_1^2 = \Sigma_{1,1}\)</span> be the variance of the log discrimination parameters. Then <span class="math inline">\(\tau \sim \mathrm{Exp}(.1)\)</span> is a weakly informative prior for the standard deviation.</li>
<li>Let <span class="math inline">\(\tau_2^2 = \Sigma_{2,2}\)</span> be the variance of the difficulty parameters. Then <span class="math inline">\(\tau_2 \sim \mathrm{Exp}(.1)\)</span> is a weakly informative prior for the standard deviation.</li>
<li>A weakly informative prior is placed on the covariance, <span class="math inline">\(\Sigma_{1,2} = \Sigma_{2,1}\)</span>, shrinking the posterior towards zero. This is described in more detail in the next section.</li>
</ul>
</div>
<div id="stan-program" class="section level2">
<h2><span class="header-section-number">1.2</span> <strong>Stan</strong> program</h2>
<p>The <strong>Stan</strong> program for the model is provided below. It differs from the notation above in that it is written in terms of the Cholesky decomposition of the correlation matrix for better efficiency. This matrix is named <code>L_Omega</code>; <code>Omega</code> because it is the correlation rather than covariance matrix, and <code>L</code> because it is a Cholesky decomposition. The vector <code>tau</code> contains the standard deviations that would be the (square root of the) diagonal of the covariance matrix. (The standard deviations are invariant whether or not the the Cholesky decomposition is used.) In the <code>model</code> block, <code>L_Omega</code> is converted to its covariance equivalent <code>L_Sigma</code>, and item parameter pairs <code>xi[i]</code> are sampled from <code>L_Sigma</code>. The first element of vector <code>xi[i]</code> is the log discrimination for item <code>i</code>, and the second is the difficulty for item <code>i</code>.</p>
<p>Weakly informative normal priors are placed on <code>mu</code>, and weakly informative truncated normal priors are placed on <code>tau</code>. The prior placed on <code>L_Omega</code> using <code>lkj_corr_cholesky()</code> is weakly informative and slightly favors a correlation of zero. See the <strong>Stan</strong> manual for details.</p>
<p>While more efficient, parameters <code>L_Omega</code> and <code>xi</code> are difficult to interpret. To alleviate this inconvenience, item parameters <code>alpha</code> and <code>beta</code> are derived from <code>xi</code> in the <code>transformed parameters</code> block. (There is some redundancy, as <code>beta[i]</code> and <code>xi[i,2]</code> are equal.) Also, <code>L_Omega</code> is converted to a standard correlation matrix, <code>Omega</code>, in the <code>generated quantities</code> block.</p>
<pre><code>data {
  int&lt;lower=1&gt; I;               // # items
  int&lt;lower=1&gt; J;               // # persons
  int&lt;lower=1&gt; N;               // # observations
  int&lt;lower=1, upper=I&gt; ii[N];  // item for n
  int&lt;lower=1, upper=J&gt; jj[N];  // person for n
  int&lt;lower=0, upper=1&gt; y[N];   // correctness for n
}
parameters {
  vector[J] theta;              // abilities
  vector[2] xi[I];              // alpha/beta pair vectors
  vector[2] mu;                 // vector for alpha/beta means
  vector&lt;lower=0&gt;[2] tau;       // vector for alpha/beta residual sds
  cholesky_factor_corr[2] L_Omega;
}
transformed parameters {
  vector[I] alpha;
  vector[I] beta;
  for (i in 1:I) {
    alpha[i] &lt;- exp(xi[i,1]);
    beta[i] &lt;- xi[i,2];
  }
}
model {
  matrix[2,2] L_Sigma;
  L_Sigma &lt;- diag_pre_multiply(tau, L_Omega);
  for (i in 1:I)
    xi[i] ~ multi_normal_cholesky(mu, L_Sigma);
  theta ~ normal(0, 1);
  L_Omega ~ lkj_corr_cholesky(4);
  mu[1] ~ normal(0,1);
  tau[1] ~ exponential(.1);
  mu[2] ~ normal(0,5);
  tau[2] ~ exponential(.1);
  y ~ bernoulli_logit(alpha[ii] .* (theta[jj] - beta[ii]));
}
generated quantities {
  corr_matrix[2] Omega;
  Omega &lt;- multiply_lower_tri_self_transpose(L_Omega);
}</code></pre>
</div>
</div>
<div id="simulation" class="section level1">
<h1><span class="header-section-number">2</span> Simulation</h1>
<p>First, the necessary <strong>R</strong> packages are loaded.</p>
<pre class="r"><code># Load R packages
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
library(ggplot2)</code></pre>
<p>The <strong>R</strong> code that follows simulates a dataset conforming to the model. The <strong>Stan</strong> model will be evaluated in terms of its ability to recover the generating values of the parameters when fit to this dataset.</p>
<pre class="r"><code># Set paramters for the simulated data
I &lt;- 20
J &lt;- 1000
mu &lt;- c(0, 0)
tau &lt;- c(0.25, 1)
Omega &lt;- matrix(c(1, 0.3, 0.3, 1), ncol = 2)

# Calculate or sample remaining paramters
Sigma &lt;- tau %*% t(tau) * Omega
xi &lt;- MASS::mvrnorm(I, c(0, 0), Sigma)
alpha &lt;- exp(mu[1] + as.vector(xi[, 1]))
beta &lt;- as.vector(mu[2] + xi[, 2])
theta &lt;- rnorm(J, mean = 0, sd = 1)

# Assemble data and simulate response
data_list &lt;- list(I = I, J = J, N = I * J, ii = rep(1:I, times = J), jj = rep(1:J, 
    each = I))
eta &lt;- alpha[data_list$ii] * (theta[data_list$jj] - beta[data_list$ii])
data_list$y &lt;- as.numeric(boot::inv.logit(eta) &gt; runif(data_list$N))</code></pre>
<p>The simulated data consists of 20 items and 1000 persons. The log discriminations have mean 0 and standard deviation 0.25. The difficulties have mean 0 and standard deviation 1. The correlation between the log discrimination residuals and difficulty residuals is 0.3. The simulated dataset is fit with <strong>Stan</strong>.</p>
<pre class="r"><code># Fit model to simulated data
sim_fit &lt;- stan(file = &quot;hierarchical_2pl.stan&quot;, data = data_list, chains = 4, 
    iter = 500)</code></pre>
<pre><code>## Warning: There were 5 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See
## http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded</code></pre>
<pre><code>## Warning: Examine the pairs() plot to diagnose sampling problems</code></pre>
<p>Before interpreting the results, it is necessary to check that the chains have converged. <strong>Stan</strong> provides the <span class="math inline">\(\hat{R}\)</span> statistic for the model parameters and log posterior. These are provided in the following figure. All values for <span class="math inline">\(\hat{R}\)</span> should be less than 1.1.</p>
<pre class="r"><code>sim_summary &lt;- as.data.frame(summary(sim_fit)[[1]])
sim_summary$Parameter &lt;- as.factor(gsub(&quot;\\[.*]&quot;, &quot;&quot;, rownames(sim_summary)))
ggplot(sim_summary) + aes(x = Parameter, y = Rhat, color = Parameter) + geom_jitter(height = 0, 
    width = 0.5, show.legend = FALSE) + ylab(expression(hat(italic(R))))</code></pre>
<div class="figure">
<img src="hierarchical_2pl_files/figure-html/sim_converge-1.png" alt="Convergence statistics ($\hat{R}$) by parameter for the simulation. All values should be less than 1.1 to infer convergence." width="672" />
<p class="caption">
Convergence statistics (<span class="math inline">\(\hat{R}\)</span>) by parameter for the simulation. All values should be less than 1.1 to infer convergence.
</p>
</div>
<p>The <strong>Stan</strong> model is evaluated in terms of its ability to recover the generating values of the parameters. The R code below prepares a plot in which the points indicate the difference between the posterior means and generating values for the parameters of main interest. This difference is referred to as discrepancy. The lines indicate the 95% posterior intervals for the difference. Ideally, (nearly) all the 95% posterior intervals would include zero.</p>
<pre class="r"><code># Make vector of wanted parameter names
wanted_pars &lt;- c(paste0(&quot;alpha[&quot;, 1:I, &quot;]&quot;), paste0(&quot;beta[&quot;, 1:I, &quot;]&quot;), c(&quot;mu[1]&quot;, 
    &quot;mu[2]&quot;, &quot;tau[1]&quot;, &quot;tau[2]&quot;, &quot;Omega[1,2]&quot;))

# Get estimated and generating values for wanted parameters
generating_values = c(alpha, beta, mu, tau, Omega[1, 2])
estimated_values &lt;- sim_summary[wanted_pars, c(&quot;mean&quot;, &quot;2.5%&quot;, &quot;97.5%&quot;)]

# Assesmble a data frame to pass to ggplot()
sim_df &lt;- data.frame(parameter = factor(wanted_pars, rev(wanted_pars)), row.names = NULL)
sim_df$middle &lt;- estimated_values[, &quot;mean&quot;] - generating_values
sim_df$lower &lt;- estimated_values[, &quot;2.5%&quot;] - generating_values
sim_df$upper &lt;- estimated_values[, &quot;97.5%&quot;] - generating_values

# Plot the discrepancy
ggplot(sim_df) + aes(x = parameter, y = middle, ymin = lower, ymax = upper) + 
    scale_x_discrete() + geom_abline(intercept = 0, slope = 0, color = &quot;white&quot;) + 
    geom_linerange() + geom_point(size = 2) + labs(y = &quot;Discrepancy&quot;, x = NULL) + 
    theme(panel.grid = element_blank()) + coord_flip()</code></pre>
<div class="figure">
<img src="hierarchical_2pl_files/figure-html/sim_plot-1.png" alt="Discrepancies between estimated and generating parameters. Points indicate the difference between the posterior means and generating values for a parameter, and horizontal lines indicate 95% posterior intervals for the difference. Most of the discrepancies are about zero, indicating that **Stan** successfully recovers the true parameters." width="672" />
<p class="caption">
Discrepancies between estimated and generating parameters. Points indicate the difference between the posterior means and generating values for a parameter, and horizontal lines indicate 95% posterior intervals for the difference. Most of the discrepancies are about zero, indicating that <strong>Stan</strong> successfully recovers the true parameters.
</p>
</div>
</div>
<div id="example-application" class="section level1">
<h1><span class="header-section-number">3</span> Example application</h1>
<pre class="r"><code># Use data and scoring function from the mirt package
library(mirt)
sat &lt;- key2binary(SAT12, key = c(1, 4, 5, 2, 3, 1, 2, 1, 3, 1, 2, 4, 2, 1, 5, 
    3, 4, 4, 1, 4, 3, 3, 4, 1, 3, 5, 1, 3, 1, 5, 4, 5))</code></pre>
<p>The example data <span class="citation">(Wood et al. 2003)</span> are from a grade 12 science assessment. 600 students responded to 32 dichotomously scored items. Non-responses were scored as incorrect, so the data contain no missing values. The scored response matrix is converted to list form and fit with <strong>Stan</strong>.</p>
<pre class="r"><code># Assemble data list and fit model
sat_list &lt;- list(I = ncol(sat), J = nrow(sat), N = length(sat), ii = rep(1:ncol(sat), 
    each = nrow(sat)), jj = rep(1:nrow(sat), times = ncol(sat)), y = as.vector(sat))
sat_fit &lt;- stan(file = &quot;hierarchical_2pl.stan&quot;, data = sat_list, chains = 4, 
    iter = 500)</code></pre>
<p>As discussed above, convergence of the chains is assessed for every parameter, and also the log posterior, using <span class="math inline">\(\hat{R}\)</span>.</p>
<pre class="r"><code>ex_summary &lt;- as.data.frame(summary(sat_fit)[[1]])
ex_summary$Parameter &lt;- as.factor(gsub(&quot;\\[.*]&quot;, &quot;&quot;, rownames(ex_summary)))
ggplot(ex_summary) + aes(x = Parameter, y = Rhat, color = Parameter) + geom_jitter(height = 0, 
    width = 0.5, show.legend = FALSE) + ylab(expression(hat(italic(R))))</code></pre>
<div class="figure">
<img src="hierarchical_2pl_files/figure-html/example_converge-1.png" alt="Convergence statistics ($\hat{R}$) by parameter for the SAT data. All values should be less than 1.1 to infer convergence." width="672" />
<p class="caption">
Convergence statistics (<span class="math inline">\(\hat{R}\)</span>) by parameter for the SAT data. All values should be less than 1.1 to infer convergence.
</p>
</div>
<p>Next we view summaries of the parameter posteriors.</p>
<pre class="r"><code># View table of parameter posteriors
print(sat_fit, pars = c(&quot;alpha&quot;, &quot;beta&quot;, &quot;mu&quot;, &quot;tau&quot;, &quot;Omega[1,2]&quot;))</code></pre>
<pre><code>## Inference for Stan model: hierarchical_2pl.
## 4 chains, each with iter=500; warmup=250; thin=1; 
## post-warmup draws per chain=250, total post-warmup draws=1000.
## 
##             mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat
## alpha[1]    0.79    0.00 0.12  0.58  0.70  0.80  0.87  1.02  1000 1.00
## alpha[2]    1.45    0.01 0.17  1.12  1.33  1.45  1.55  1.78  1000 1.00
## alpha[3]    1.04    0.00 0.13  0.80  0.95  1.04  1.13  1.31  1000 1.00
## alpha[4]    0.59    0.00 0.10  0.39  0.53  0.59  0.66  0.79  1000 1.00
## alpha[5]    0.97    0.00 0.12  0.75  0.89  0.97  1.05  1.21  1000 1.00
## alpha[6]    1.11    0.01 0.16  0.83  0.99  1.10  1.21  1.43  1000 1.00
## alpha[7]    1.01    0.00 0.14  0.76  0.91  1.00  1.10  1.32  1000 1.00
## alpha[8]    0.69    0.00 0.11  0.49  0.61  0.69  0.77  0.92  1000 1.00
## alpha[9]    0.74    0.00 0.12  0.51  0.66  0.73  0.81  0.98  1000 1.00
## alpha[10]   0.99    0.00 0.13  0.73  0.90  0.99  1.08  1.25  1000 1.00
## alpha[11]   1.70    0.01 0.36  1.09  1.44  1.66  1.90  2.49  1000 1.00
## alpha[12]   0.29    0.00 0.07  0.16  0.23  0.29  0.33  0.42  1000 1.00
## alpha[13]   1.08    0.00 0.14  0.82  0.98  1.07  1.17  1.35  1000 1.00
## alpha[14]   1.03    0.00 0.14  0.77  0.93  1.02  1.12  1.33  1000 1.00
## alpha[15]   1.26    0.01 0.17  0.95  1.13  1.25  1.37  1.60  1000 1.00
## alpha[16]   0.72    0.00 0.11  0.51  0.64  0.72  0.79  0.94  1000 1.00
## alpha[17]   1.52    0.01 0.27  1.02  1.33  1.50  1.69  2.11  1000 1.00
## alpha[18]   1.64    0.01 0.18  1.32  1.52  1.63  1.75  2.01  1000 1.00
## alpha[19]   0.83    0.00 0.12  0.62  0.75  0.83  0.91  1.07  1000 1.00
## alpha[20]   1.47    0.01 0.22  1.09  1.32  1.46  1.62  1.92  1000 1.00
## alpha[21]   0.83    0.00 0.14  0.58  0.73  0.83  0.92  1.13  1000 1.00
## alpha[22]   1.48    0.01 0.24  1.05  1.31  1.46  1.64  1.96  1000 1.00
## alpha[23]   0.64    0.00 0.10  0.45  0.57  0.64  0.71  0.85  1000 1.00
## alpha[24]   1.17    0.00 0.15  0.89  1.06  1.16  1.27  1.48  1000 1.00
## alpha[25]   0.76    0.00 0.11  0.55  0.68  0.75  0.83  0.99  1000 1.00
## alpha[26]   1.47    0.01 0.16  1.18  1.36  1.47  1.57  1.80  1000 1.00
## alpha[27]   1.79    0.01 0.27  1.31  1.60  1.76  1.95  2.38  1000 1.00
## alpha[28]   1.04    0.00 0.13  0.81  0.96  1.04  1.13  1.30  1000 1.00
## alpha[29]   0.82    0.00 0.11  0.62  0.74  0.82  0.90  1.06  1000 1.00
## alpha[30]   0.43    0.00 0.09  0.27  0.37  0.43  0.49  0.61  1000 1.00
## alpha[31]   2.13    0.01 0.31  1.59  1.91  2.11  2.33  2.81  1000 1.00
## alpha[32]   0.38    0.00 0.08  0.24  0.32  0.37  0.43  0.54  1000 1.00
## beta[1]     1.34    0.01 0.22  0.98  1.18  1.31  1.46  1.81  1000 1.00
## beta[2]    -0.30    0.00 0.08 -0.45 -0.35 -0.30 -0.24 -0.16  1000 1.00
## beta[3]     1.09    0.00 0.15  0.83  0.99  1.08  1.18  1.41  1000 1.00
## beta[4]     0.93    0.01 0.23  0.56  0.78  0.90  1.05  1.45  1000 1.00
## beta[5]    -0.63    0.00 0.11 -0.84 -0.70 -0.62 -0.55 -0.42  1000 1.00
## beta[6]     1.85    0.01 0.24  1.46  1.68  1.82  1.99  2.40  1000 1.00
## beta[7]    -1.40    0.01 0.18 -1.79 -1.51 -1.38 -1.27 -1.09  1000 1.00
## beta[8]     2.22    0.01 0.36  1.64  1.96  2.17  2.43  3.01  1000 1.00
## beta[9]    -3.06    0.01 0.47 -4.11 -3.35 -3.01 -2.73 -2.32  1000 1.00
## beta[10]    0.37    0.00 0.11  0.15  0.29  0.36  0.43  0.59  1000 1.00
## beta[11]   -3.15    0.01 0.47 -4.21 -3.42 -3.10 -2.83 -2.39  1000 1.00
## beta[12]    1.31    0.01 0.45  0.61  1.00  1.26  1.57  2.31  1000 1.00
## beta[13]   -0.79    0.00 0.12 -1.06 -0.86 -0.78 -0.71 -0.56  1000 1.00
## beta[14]   -1.16    0.01 0.16 -1.49 -1.26 -1.15 -1.06 -0.88  1000 1.00
## beta[15]   -1.53    0.01 0.17 -1.91 -1.63 -1.52 -1.41 -1.24  1000 1.01
## beta[16]    0.55    0.00 0.16  0.28  0.44  0.53  0.64  0.86  1000 1.00
## beta[17]   -2.78    0.01 0.37 -3.66 -2.98 -2.74 -2.50 -2.22  1000 1.00
## beta[18]    0.51    0.00 0.08  0.34  0.45  0.51  0.56  0.67  1000 1.00
## beta[19]   -0.29    0.00 0.12 -0.54 -0.37 -0.29 -0.21 -0.05  1000 1.00
## beta[20]   -1.77    0.01 0.19 -2.19 -1.87 -1.76 -1.64 -1.44  1000 1.00
## beta[21]   -3.22    0.02 0.50 -4.40 -3.51 -3.16 -2.86 -2.40   767 1.01
## beta[22]   -2.36    0.01 0.27 -2.98 -2.53 -2.34 -2.17 -1.93  1000 1.00
## beta[23]    1.36    0.01 0.25  0.94  1.18  1.32  1.49  1.94  1000 1.00
## beta[24]   -1.09    0.00 0.13 -1.38 -1.17 -1.07 -0.99 -0.87  1000 1.00
## beta[25]    0.76    0.01 0.17  0.45  0.64  0.75  0.86  1.11  1000 1.00
## beta[26]    0.11    0.00 0.08 -0.03  0.06  0.11  0.16  0.27   763 1.00
## beta[27]   -1.52    0.00 0.15 -1.85 -1.62 -1.51 -1.41 -1.25  1000 1.01
## beta[28]   -0.17    0.00 0.09 -0.36 -0.23 -0.17 -0.11  0.02  1000 1.00
## beta[29]    0.91    0.01 0.16  0.64  0.80  0.90  1.01  1.25  1000 1.00
## beta[30]    0.60    0.01 0.24  0.19  0.44  0.60  0.75  1.13  1000 1.00
## beta[31]   -1.25    0.00 0.11 -1.49 -1.32 -1.24 -1.18 -1.06   684 1.00
## beta[32]    4.56    0.03 0.95  3.01  3.88  4.47  5.16  6.71  1000 1.00
## mu[1]      -0.05    0.00 0.10 -0.25 -0.12 -0.05  0.02  0.15  1000 1.01
## mu[2]      -0.25    0.01 0.34 -0.93 -0.46 -0.25 -0.04  0.47  1000 1.00
## tau[1]      0.50    0.00 0.07  0.37  0.45  0.49  0.54  0.65  1000 1.00
## tau[2]      1.81    0.01 0.28  1.32  1.60  1.78  1.97  2.41  1000 1.00
## Omega[1,2] -0.45    0.00 0.15 -0.71 -0.57 -0.46 -0.35 -0.13  1000 1.00
## 
## Samples were drawn using NUTS(diag_e) at Wed Jun 28 01:26:34 2017.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
<p>To visualize the correlation between item parameters, the values of <code>alpha[i]</code> and <code>beta[i]</code> may be plotted against one another. This is presented in the left side of the figure below. Because the correlation is actually between <code>xi[i,1]</code> (which is equal to <code>beta[i]</code>) and <code>xi[i,2]</code> (which is <code>alpha[i]</code> on the log scale), a scatter plot pf <code>xi[i,1]</code> versus <code>xi[i,2]</code> is given on the right side.</p>
<pre class="r"><code># Assesmble a data frame of item parameter estimates and pass to ggplot
ab_df &lt;- data.frame(Discrimination = ex_summary[paste0(&quot;alpha[&quot;, 1:sat_list$I, 
    &quot;]&quot;), &quot;mean&quot;], Difficulty = ex_summary[paste0(&quot;beta[&quot;, 1:sat_list$I, &quot;]&quot;), 
    &quot;mean&quot;], parameterization = &quot;alpha &amp; beta&quot;)
xi_df &lt;- data.frame(Discrimination = ex_summary[paste0(&quot;xi[&quot;, 1:sat_list$I, 
    &quot;,1]&quot;), &quot;mean&quot;], Difficulty = ex_summary[paste0(&quot;xi[&quot;, 1:sat_list$I, &quot;,2]&quot;), 
    &quot;mean&quot;], parameterization = &quot;xi&quot;)
full_df &lt;- rbind(ab_df, xi_df)
ggplot(full_df) + aes(x = Difficulty, y = Discrimination) + geom_point() + facet_wrap(~parameterization, 
    scales = &quot;free&quot;)</code></pre>
<div class="figure">
<img src="hierarchical_2pl_files/figure-html/example_plot-1.png" alt="Discrimination versus difficulty parameters for the SAT data. The lefthand plot shows the pairs in terms of alpha and beta, the usual formulation. The righthand plot shows the pairs in terms of xi, as used by the Stan model." width="672" />
<p class="caption">
Discrimination versus difficulty parameters for the SAT data. The lefthand plot shows the pairs in terms of alpha and beta, the usual formulation. The righthand plot shows the pairs in terms of xi, as used by the Stan model.
</p>
</div>
</div>
<div id="references" class="section level1">
<h1><span class="header-section-number">4</span> References</h1>
<!-- This comment causes section to be numbered -->
<div id="refs" class="references">
<div id="ref-glas2003computerized">
<p>Glas, Cees AW, and Wim J van der Linden. 2003. “Computerized Adaptive Testing with Item Cloning.” <em>Applied Psychological Measurement</em> 27 (4). Sage Publications: 247–61.</p>
</div>
<div id="ref-testfact4">
<p>Wood, R., Wilson D. T., Gibbons R. D., Schilling S. G., Muraki E., and R. D. Bock. 2003. <em>TESTFACT 4 for Windows: Test Scoring, Item Statistics, and Full-Information Item Factor Analysis</em>. Lincolnwood, IL: Scientific Software International.</p>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
