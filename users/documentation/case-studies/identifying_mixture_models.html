<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Michael Betancourt" />


<title>Identifying Bayesian Mixture Models</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="site_libs/highlight/default.css"
      type="text/css" />
<script src="site_libs/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Identifying Bayesian Mixture Models</h1>
<h4 class="author"><em>Michael Betancourt</em></h4>
<h4 class="date"><em>February 2017</em></h4>

</div>


<p>Mixture modeling is a powerful technique for integrating multiple data generating processes into a single model. Unfortunately when those data data generating processes are degenerate the resulting mixture model suffers from inherent combinatorial non-identifiabilities that frustrate accurate computation. Consequently, in order to utilize mixture models reliably in practice we need strong and principled prior information to ameliorate these frustrations.</p>
<p>In this case study I will first introduce how mixture models are implemented in Bayesian inference. I will then discuss the non-identifiability inherent to that construction as well as how the non-identifiability can be tempered with principled prior information. Lastly I will demonstrate how these issues manifest in a simple example, with a final tangent to consider an additional pathology that can arise in Bayesian mixture models.</p>
<div id="mixture-models" class="section level1">
<h1>Mixture Models</h1>
<p>In a mixture model we assume that a given measurement, <span class="math inline">\(y\)</span>, can be drawn from one of <span class="math inline">\(K\)</span> data generating processes, each with their own set of parameters, <span class="math inline">\(\pi_{k} ( y \mid \alpha_{k} )\)</span>. To implement such a model we need to construct the corresponding likelihood and then subsequent posterior distribution.</p>
<div id="the-mixture-likelihood" class="section level2">
<h2>The Mixture Likelihood</h2>
<p>Let <span class="math inline">\(z \in \{0 ,\ldots, K\}\)</span> be an <em>assignment</em> that indicates to which data generating process our measurement was generated. Conditioned on this assignment, the mixture likelihood is just <span class="math display">\[
\pi (y \mid \boldsymbol{\alpha}, z)
=
\pi_{z} ( y \mid \alpha_{z} ),
\]</span> where <span class="math inline">\(\boldsymbol{\alpha} = (\alpha_1, \ldots, \alpha_K)\)</span>.</p>
<p>By combining assignments with a set of data generating processes we admit an extremely expressive class of models that encompass many different inferential and decision problems. For example, if multiple measurements <span class="math inline">\(y_n\)</span> are given but the corresponding assignments <span class="math inline">\(z_n\)</span> are unknown then inference over the mixture model is equivalent to <em>clustering</em> the measurements across the component data generating processes. Similarly, if both the measurements and the assignments are given then inference over the mixture model admits <em>classification</em> of future measurements. Finally, <em>semi-supervised</em> learning corresponds to inference over a mixture model where only some of the assignments are known.</p>
<p>In practice discrete assignments are difficult to fit accurately and efficiently, but we can facilitate inference by <em>marginalizing</em> the assignments out of the model entirely. If each component in the mixture occurs with probability <span class="math inline">\(\theta_k\)</span>, <span class="math display">\[
\boldsymbol{\theta} = (\theta_1, \ldots, \theta_K), \,
0 \le \theta_{k} \le 1, \,
\sum_{k = 1}^{K} \theta_{k} = 1,
\]</span> then the assignments follow a multinomial distribution, <span class="math display">\[
\pi (z \mid \boldsymbol{\theta} )
=
\theta_{z},
\]</span> and the joint likelihood over the measurement and its assignment is given by <span class="math display">\[
\pi (y, z \mid \boldsymbol{\alpha}, \boldsymbol{\theta})
=
\pi (y \mid \boldsymbol{\alpha}, z) \,
\pi (z \mid \boldsymbol{\theta} )
=
\pi_{z} ( y \mid \alpha_{z} ) \, \theta_z.
\]</span> Marginalizing over all of the possible assignments then gives <span class="math display">\[
\begin{align*}
\pi (y \mid \boldsymbol{\alpha}, \boldsymbol{\theta})
&amp;=
\sum_{z} \pi (y, z \mid \boldsymbol{\alpha}, \boldsymbol{\theta})
\\
&amp;=
\sum_{z} \pi_{z} ( y \mid \alpha_{z} ) \, \theta_z
\\
&amp;=
\sum_{k = 1}^{K} \pi_{k} ( y \mid \alpha_{k} ) \, \theta_k
\\
&amp;=
\sum_{k = 1}^{K}  \theta_k \, \pi_{k} ( y \mid \alpha_{k} ).
\end{align*}
\]</span> In words, after marginalizing out the assignments the mixture likelihood reduces to a convex combination of the component data generating processes.</p>
<p>Marginalizing out the discrete assignments yields a likelihood that depends on only continuous parameters, making it amenable to state-of-the-art tools like Stan. Moreover, modeling the latent mixture probabilities instead of the discrete assignments admits more precise inferences as a consequence of the Rao-Blackwell theorem. From any perspective the marginalized mixture likelihood is the ideal basis for inference.</p>
</div>
<div id="bayesian-mixture-posteriors" class="section level2">
<h2>Bayesian Mixture Posteriors</h2>
<p>In order to perform Bayesian inference over a mixture model we need to complement the mixture likelihood with prior distributions for both the component parameters, <span class="math inline">\(\boldsymbol{\alpha}\)</span>, and the mixture probabilities, <span class="math inline">\(\boldsymbol{\theta}\)</span>. Assuming that these distributions are independent a priori, <span class="math display">\[
\pi(\boldsymbol{\alpha}, \boldsymbol{\theta})
=
\pi(\boldsymbol{\alpha}) \, \pi(\boldsymbol{\theta}),
\]</span> the subsequent posterior for a single measurement takes the form <span class="math display">\[
\pi(\boldsymbol{\alpha}, \boldsymbol{\theta} \mid y)
\propto
\pi(\boldsymbol{\alpha}) \, \pi(\boldsymbol{\theta})
\sum_{k = 1}^{K} \theta_k \, \pi_{k} ( y \mid \alpha_k ).
\]</span></p>
<p>Similarly, the posterior for multiple measurements becomes <span class="math display">\[
\pi(\boldsymbol{\alpha}, \boldsymbol{\theta} \mid \mathbf{y})
\propto
\pi(\boldsymbol{\alpha}) \, \pi(\boldsymbol{\theta})
\sum_{n = 1}^{N}
\sum_{k = 1}^{K} \theta_k \, \pi_{k} ( y_n \mid \alpha_k ).
\]</span> Additional measurements, however, do not impact the non-identifiability inherent to mixture models. Consequently we will consider only a single measurement in the proceeding section, returning to multiple measurements in the example.</p>
</div>
</div>
<div id="degenerate-mixture-models-and-non-identifiability" class="section level1">
<h1>Degenerate Mixture Models and Non-identifiability</h1>
<p>When making inferences with a mixture model we need to learn each of the component weights, <span class="math inline">\(\theta_k\)</span>, and the component parameters, <span class="math inline">\(\alpha_k\)</span>. This introduces a subtle challenge because if the measurement cannot discriminate between the components then it cannot discriminate between the component parameters.</p>
<p>If the individual component distributions <span class="math inline">\(\pi_{k} (y \mid \alpha_{k})\)</span> are distinct then the unique characteristics of each can be enough to inform the corresponding parameters individually and the mixture model is straightforward to fit. Circumstances become much more dire, however, in the <em>degenerate</em> case when the components are identical, <span class="math inline">\(\pi_{k} (y \mid \alpha_{k}) = \pi (y \mid \alpha_{k})\)</span>. In this case there is a fundamental ambiguity as to which parameters <span class="math inline">\(\alpha_{k}\)</span> are associated with each component in the mixture.</p>
<p>To see this, let <span class="math inline">\(\sigma\)</span> denote a permutation of the indices in our mixture, <span class="math display">\[
\sigma (1, \ldots, K) \mapsto ( \sigma(1), \ldots, \sigma(K)),
\]</span> with <span class="math display">\[
\sigma (\boldsymbol{\alpha}) = \sigma( \alpha_1, \ldots, \alpha_K)
\mapsto ( \alpha_{\sigma(1)}, \ldots, \alpha_{\sigma(K)}).
\]</span> When the component distributions are identical the mixture likelihood is invariant to any permutation of the indices, <span class="math display">\[
\begin{align*}
\pi(y \mid \sigma(\boldsymbol{\alpha}), \sigma(\boldsymbol{\theta}))
&amp;=
\sum_{k = 1}^{K} \theta_{\sigma(k)} \,
\pi_{\sigma(k)} ( y \mid \alpha_{\sigma(k)} )
\\
&amp;=
\sum_{k&#39; = 1}^{K} \theta_{k&#39;} \,
\pi_{k&#39;} ( y \mid \alpha_{k&#39;} )
\\
&amp;=
\pi(y \mid \boldsymbol{\alpha}, \boldsymbol{\theta}).
\end{align*}
\]</span></p>
<p>Moreover, when the priors are <em>exchangeable</em>, <span class="math inline">\(\pi (\sigma(\boldsymbol{\alpha})) = \pi(\boldsymbol{\alpha})\)</span> and <span class="math inline">\(\pi (\sigma(\boldsymbol{\theta})) = \pi(\boldsymbol{\theta})\)</span>, then the posterior will inherit the permutation invariance of the mixture likelihood, <span class="math display">\[
\begin{align*}
\pi(\sigma(\boldsymbol{\alpha}), \sigma(\boldsymbol{\theta}) \mid y)
&amp;\propto
\pi(\sigma(\boldsymbol{\alpha})) \, \pi(\sigma(\boldsymbol{\theta}))
\sum_{k = 1}^{K} \theta_{\sigma(k)} \,
\pi_{\sigma(k)} ( y \mid \alpha_{\sigma(k)} )
\\
&amp;\propto
\pi(\sigma(\boldsymbol{\alpha})) \, \pi(\sigma(\boldsymbol{\theta}))
\sum_{k&#39; = 1}^{K} \theta_{k&#39;} \,
\pi_{k&#39;} ( y \mid \alpha_{k&#39;} )
\\
&amp;\propto
\pi(\boldsymbol{\alpha}) \, \pi(\boldsymbol{\theta})
\sum_{k&#39; = 1}^{K} \theta_{k&#39;} \,
\pi_{k&#39;} ( y \mid \alpha_{k&#39;} )
\\
&amp;=
\pi(\boldsymbol{\alpha}, \boldsymbol{\theta} \mid y).
\end{align*}
\]</span> In this case all of our inferences will be the same regardless of how we label the mixture components with explicit indices.</p>
<p>Because of this labeling degeneracy the posterior distribution will be non-identified. In particular, it will manifest multimodality, with one mode for each of the possible labelings. For a mixture with <span class="math inline">\(K\)</span> identical components there are <span class="math inline">\(K!\)</span> possible labelings and hence any degenerate mixture model will exhibit at least <span class="math inline">\(K!\)</span> modes.</p>
<p>Hence even for a relatively small number of components the posterior distribution will have too many modes for any statistical algorithm to accurately quantify unless the modes collapse into each other. For example, if we applied Markov chain Monte Carlo then any chain would be able to explore one of the modes but it would not be able to transition <em>between</em> the modes, at least not within in any finite running time.</p>
</div>
<div id="identifying-degenerate-bayesian-mixture-models" class="section level1">
<h1>Identifying Degenerate Bayesian Mixture Models</h1>
<p>Even if we had a statistical algorithm that could transition between the degenerate modes and explore the entire mixture posterior, typically there will be too many modes to complete that exploration in any reasonable time. Consequently if we want to accurately fit these models in practice then we need to break the labeling degeneracy and remove the extreme multimodality altogether.</p>
<p>Exactly how we break the labeling degeneracy depends on what prior information we can exploit. In particular, our strategy will be different depending on whether our prior information is exchangeable or not.</p>
<div id="identification-with-non-exchangeable-prior-information" class="section level2">
<h2>Identification with Non-exchangeable Prior Information</h2>
<p>Because the posterior distribution inherits the permutation-invariance of the mixture likelihood only if the priors are exchangeable, one way to immediately obstruct the labeling degeneracy of the mixture posterior is to employ non-exchangeable priors. This approach is especially useful when each component of the likelihood is meant to be responsible for a specific purpose, for example when each component models a known subpopulations with distinct behaviors about which we have prior information. If this principled prior information is strong enough then the prior can suppress all but the one labeling consistent with these responsibilities, ensuring a unimodal mixture posterior distribution.</p>
</div>
<div id="identification-with-exchangeable-prior-information" class="section level2">
<h2>Identification with Exchangeable Prior Information</h2>
<p>When our prior information is exchangeable there is nothing preventing the mixture posterior from becoming multimodal and impractical to fit. When our inferences are also exchangeable, however, we can exploit the symmetry of the labeling degeneracies to simplify the computational problem dramatically.</p>
<p>In this section we’ll study the symmetric geometry induced by labeling degeneracies and show how that symmetry can be used to reduce the multimodal mixture posterior into a unimodal distribution that yields exactly the same inferences while being far easier to fit.</p>
<div id="the-geometry-of-a-degenerate-posterior" class="section level3">
<h3>The Geometry of a Degenerate Posterior</h3>
<p>Each labeling is characterized by the unique assignment of indices to the components in our mixture. Permuting the indices yields a new assignment and hence a new labeling of our mixture model. Consequently a natural way to identify each labeling is to a choose a standard set of indices, <span class="math inline">\(\alpha_1, \ldots, \alpha_K\)</span>, and distinguish each labeling by the permutation that maps to the appropriate indices, <span class="math inline">\(\alpha_{\sigma(1)}, \ldots, \alpha_{\sigma(K)}\)</span>. The standard indices themselves identify a labeling with the trivial permutation that leaves the indices unchanged.</p>
<p>In general it is difficult to utilize these permutations, but if the component parameters, <span class="math inline">\(\alpha_n\)</span>, are scalar then we can exploit their unique <em>ordering</em> to readily identify permutations and hence labelings. For example, if we choose the standard labeling to be the one where the parameter values are ordered, <span class="math inline">\(\alpha_1 \le \ldots \le \alpha_K\)</span>, then any permutation will yield a new ordering of the parameters, <span class="math inline">\(\alpha_{\sigma(1)} \le \ldots \le \alpha_{\sigma(K)}\)</span>, which then identifies another labeling. In other words, we can identify the each labeling by the ordering the parameter values.</p>
<p>This identification also has a welcome geometric interpretation. The region of parameter space satisfying a given ordering constraint, such as <span class="math inline">\(\alpha_1 \le \ldots \le \alpha_K\)</span>, defines a square pyramid with the apex point at zero. The <span class="math inline">\(K\)</span>-dimensional parameter space neatly decomposes into <span class="math inline">\(K!\)</span> of these pyramids, each with a distinct ordering and hence association with a unique labeling.</p>
<p>When the priors are exchangeable the mixture posterior <em>aliases</em> across each of these pyramids in parameter space: if we were given the mixture posterior restricted to one of these pyramids then we could reconstruct the entire mixture distribution by simply rotating that restricted distribution into each of the other <span class="math inline">\(K! - 1\)</span> pyramids. As we do this we also map the mode in the restricted distribution into each pyramid, creating exactly the expected <span class="math inline">\(K!\)</span> multimodality. Moreover, those rotations are exactly given by permuting the parameter indices and reordering the corresponding parameter values.</p>
</div>
<div id="exchangeable-inferences-and-ordering-constraints" class="section level3">
<h3>Exchangeable Inferences and Ordering Constraints</h3>
<p>From a Bayesian perspective, all well-defined inferences are given by expectations of certain functions with respect to our posterior distribution.<br />
Hence if we want to limit ourselves to only those inferences insensitive to the labeling then we have to considering expectations only of those functions that are permutation invariant, <span class="math inline">\(f(\sigma(\boldsymbol{\alpha})) = f(\boldsymbol{\alpha})\)</span>.</p>
<p>Importantly, under this class of functions the symmetry of the degenerate mixture posterior carries over to the expectation values themselves: the expectation taken over each pyramid will yield exactly the same value.<br />
Consequently we should be able to avoid the combinatorial cost of fitting the full mixture model by simply restricting our exploration to a single ordering of the parameters.</p>
<p>Imposing an ordering on parameter space can be taken as a a computational trick, but it can also be interpreted as method of making an exchangeable prior non-exchangeable without affecting the resulting inferences. Given the exchangeable prior <span class="math inline">\(\pi (\boldsymbol{\alpha})\)</span> we define the non-exchangeable prior <span class="math display">\[
\pi&#39; (\boldsymbol{\alpha}) =
\left\{
\begin{array}{ll}
\pi (\boldsymbol{\alpha}), &amp; \alpha_1 \le  \ldots \le \alpha_K \\
0, &amp; \mathrm{else}
\end{array}
\right. ,
\]</span> which limits the mixture posterior, and hence any expectations, to a single ordering. From this perspective <em>all</em> of our strategies for breaking the labeling degeneracy reduce to imposing a non-exchangeable prior.</p>
<p>In this section I will formalize the utility of ordering by proving that the resulting inferences are indeed correct. In order to outline the proof we will first consider a two-component mixture before moving onto the general case.</p>
<div id="two-component-case" class="section level4">
<h4>Two-Component Case</h4>
<p>In the two-component case we have two parameters, <span class="math inline">\(\alpha_1\)</span> and <span class="math inline">\(\alpha_2\)</span>, and two mixture weights, <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2 = 1 - \theta_1\)</span>.</p>
<p>We begin with the desired expectation and decompose it over the two pyramids that arise in the two-dimensional parameter space, <span class="math display">\[
\begin{align*}
\mathbb{E}_{\pi} [ f ]
&amp;=
\int \mathrm{d} \theta_1 \mathrm{d} \theta_2
     \mathrm{d} \alpha_1 \mathrm{d} \alpha_2 \cdot
f (\alpha_1, \alpha_2) \cdot
\pi (\alpha_1 ,\alpha_2, \theta_1 ,\theta_2 \mid y)
\\
&amp;\propto
\int \mathrm{d} \theta_1 \mathrm{d} \theta_2
     \mathrm{d} \alpha_1 \mathrm{d} \alpha_2 \cdot
f (\alpha_1, \alpha_2) \cdot
\pi (\alpha_1 ,\alpha_2) \,
\pi (\theta_1 ,\theta_2) \,
( \theta_1 \pi (y \mid \alpha_1) + \theta_2 \pi (y \mid \alpha_2) )
\\
&amp;\propto \quad
\int_{\alpha_1 &lt; \alpha_2} \mathrm{d} \theta_1 \mathrm{d} \theta_2
     \mathrm{d} \alpha_1 \mathrm{d} \alpha_2 \cdot
f (\alpha_1, \alpha_2) \cdot
\pi (\alpha_1 ,\alpha_2) \,
\pi (\theta_1 ,\theta_2) \,
( \theta_1 \pi (y \mid \alpha_1) + \theta_2 \pi (y \mid \alpha_2) )
\\
&amp;\quad +
\int_{\alpha_2 &lt; \alpha_1} \mathrm{d} \theta_1 \mathrm{d} \theta_2
     \mathrm{d} \alpha_1 \mathrm{d} \alpha_2 \cdot
f (\alpha_1, \alpha_2) \cdot
\pi (\alpha_1 ,\alpha_2) \,
\pi (\theta_1 ,\theta_2) \,
( \theta_1 \pi (y \mid \alpha_1) + \theta_2 \pi (y \mid \alpha_2) ).
\end{align*}
\]</span></p>
<p>We want to manipulate the second term into something that looks like the first, which we can accomplish with a permutation of the parameters, <span class="math inline">\((\alpha_1, \alpha_2) \rightarrow (\beta_2, \beta_1)\)</span> and <span class="math inline">\((\theta_1, \theta_2) \rightarrow (\lambda_2, \lambda_1)\)</span>, that rotates the second pyramid into the first. This gives <span class="math display">\[
\begin{align*}
\mathbb{E}_{\pi} [ f ]
&amp;\propto \quad
\int_{\alpha_1 &lt; \alpha_2} \mathrm{d} \theta_1 \mathrm{d} \theta_2
    \mathrm{d} \alpha_1 \mathrm{d} \alpha_2 \cdot
f (\alpha_1, \alpha_2) \cdot
\pi (\alpha_1 ,\alpha_2) \,
\pi (\theta_1 ,\theta_2) \,
( \theta_1 \pi (y \mid \alpha_1) + \theta_2 \pi (y \mid \alpha_2) )
\\
&amp;\quad +
\int_{\beta_1 &lt; \beta_2} \mathrm{d} \lambda_2 \mathrm{d} \lambda_1
    \mathrm{d} \beta_2 \mathrm{d} \beta_1 \cdot
f (\beta_2, \beta_1) \cdot
\pi (\beta_2, \beta_1) \,
\pi (\lambda_2 ,\lambda_1) \,
( \lambda_2 \pi (y \mid \beta_2) + \lambda_1 \pi (y \mid \beta_1) )
\\
&amp;\propto \quad
\int_{\alpha_1 &lt; \alpha_2} \mathrm{d} \theta_1 \mathrm{d} \theta_2
    \mathrm{d} \alpha_1 \mathrm{d} \alpha_2 \cdot
f (\alpha_1, \alpha_2) \cdot
\pi (\alpha_1 ,\alpha_2) \,
\pi (\theta_1 ,\theta_2) \,
( \theta_1 \pi (y \mid \alpha_1) + \theta_2 \pi (y \mid \alpha_2) )
\\
&amp;\quad +
\int_{\beta_1 &lt; \beta_2} \mathrm{d} \lambda_1 \mathrm{d} \lambda_2
    \mathrm{d} \beta_1 \mathrm{d} \beta_2 \cdot
f (\beta_2, \beta_1) \cdot
\pi (\beta_2, \beta_1) \,
\pi (\lambda_2 ,\lambda_1) \,
(  \lambda_1 \pi (y \mid \beta_1) + \lambda_2 \pi (y \mid \beta_2) ).
\end{align*}
\]</span></p>
<p>Now we exploit the permutation-invariance of <span class="math inline">\(f\)</span> and the exchangeability of the priors to massage the second term to be equivalent to the first, <span class="math display">\[
\begin{align*}
\mathbb{E}_{\pi} [ f ]
&amp;\propto \quad
\int_{\alpha_1 &lt; \alpha_2} \mathrm{d} \theta_1 \mathrm{d} \theta_2
    \mathrm{d} \alpha_1 \mathrm{d} \alpha_2 \cdot
f (\alpha_1, \alpha_2) \cdot
\pi (\alpha_1 ,\alpha_2) \,
\pi (\theta_1 ,\theta_2) \,
( \theta_1 \pi (y \mid \alpha_1) + \theta_2 \pi (y \mid \alpha_2) )
\\
&amp;\quad +
\int_{\beta_1 &lt; \beta_2} \mathrm{d} \lambda_1 \mathrm{d} \lambda_2
    \mathrm{d} \beta_1 \mathrm{d} \beta_2 \cdot
f (\beta_1, \beta_2) \cdot
\pi (\beta_1, \beta_2) \,
\pi (\lambda_1 ,\lambda_2) \,
(  \lambda_1 \pi (y \mid \beta_1) + \lambda_2 \pi (y \mid \beta_2) )
\\
&amp;\propto
2 \int_{\alpha_1 &lt; \alpha_2} \mathrm{d} \theta_1 \mathrm{d} \theta_2
    \mathrm{d} \alpha_1 \mathrm{d} \alpha_2 \cdot
f (\alpha_1, \alpha_2) \cdot
\pi (\alpha_1 ,\alpha_2) \,
\pi (\theta_1 ,\theta_2) \,
( \theta_1 \pi (y \mid \alpha_1) + \theta_2 \pi (y \mid \alpha_2) )
\\
&amp;\propto
\int_{\alpha_1 &lt; \alpha_2} \mathrm{d} \theta_1 \mathrm{d} \theta_2
    \mathrm{d} \alpha_1 \mathrm{d} \alpha_2 \cdot
f (\alpha_1, \alpha_2) \cdot
2 \pi (\alpha_1 ,\alpha_2) \,
\pi (\theta_1 ,\theta_2) \,
2 ( \theta_1 \pi (y \mid \alpha_1) + \theta_2 \pi (y \mid \alpha_2) )
\\
&amp;=
\int_{\alpha_1 &lt; \alpha_2} \mathrm{d} \theta_1 \mathrm{d} \theta_2
    \mathrm{d} \alpha_1 \mathrm{d} \alpha_2 \cdot
f (\alpha_1, \alpha_2) \cdot
\pi&#39; (\alpha_1, \alpha_2, \theta_1, \theta_2 \mid y).
\end{align*}
\]</span></p>
<p><span class="math inline">\(\pi&#39; (\alpha_1, \alpha_2, \theta_1, \theta_2)\)</span>, however, is exactly the mixture posterior density restricted to the pyramid defined by the standard ordering, so we finally have <span class="math display">\[
\mathbb{E}_{\pi} [ f ] = \mathbb{E}_{\pi&#39;} [ f ].
\]</span> In words, taking an expectation over the pyramid defined by the standard ordering yields the same value as the expectation taken over the entire parameter space. Only the distribution over that one pyramid is no longer multimodal!</p>
</div>
<div id="general-case" class="section level4">
<h4>General Case</h4>
<p>The general case follows almost exactly once we use permutations of the standard ordering to identify each pyramid of the <span class="math inline">\(K!\)</span> pyramids. Writing <span class="math inline">\(\Sigma&#39;\)</span> as the set of all <span class="math inline">\(K! - 1\)</span> label permutations except for the trivial permutation, the desired expectation decomposes as <span class="math display">\[
\begin{align*}
\mathbb{E}_{\pi} [ f ]
&amp;=
\int \prod_{k = 1}^{K} \mathrm{d} \theta_k \mathrm{d} \alpha_k \cdot
f (\boldsymbol{\alpha}) \cdot
\pi (\boldsymbol{\alpha}, \boldsymbol{\theta} \mid y)
\\
&amp;\propto
\int \prod_{k = 1}^{K} \mathrm{d} \theta_k \mathrm{d} \alpha_k \cdot
f (\boldsymbol{\alpha}) \cdot
\pi (\boldsymbol{\alpha}) \, \pi(\boldsymbol{\theta}) \,
\sum_{k = 1}^{K} \theta_k \, \pi (y \mid \alpha_k)
\\
&amp;\propto \quad
\int_{\alpha_1 &lt; \ldots &lt; \alpha_K}
\prod_{k = 1}^{K} \mathrm{d} \theta_k \mathrm{d} \alpha_k \cdot
f (\boldsymbol{\alpha}) \cdot
\pi (\boldsymbol{\alpha}) \, \pi(\boldsymbol{\theta}) \,
\sum_{k = 1}^{K} \theta_k \, \pi (y \mid \alpha_k)
\\
&amp;\quad + \sum_{\sigma \in \Sigma&#39;}
\int_{\alpha_{\sigma(1)} &lt; \ldots &lt; \alpha_{\sigma(K)}}
\prod_{k = 1}^{K} \mathrm{d} \theta_k \mathrm{d} \alpha_k \cdot
f (\boldsymbol{\alpha}) \cdot
\pi (\boldsymbol{\alpha}) \, \pi(\boldsymbol{\theta}) \,
\sum_{k = 1}^{K} \theta_k \, \pi (y \mid \alpha_k).
\end{align*}
\]</span></p>
<p>In the permuted terms we apply the transformations <span class="math display">\[
\sigma( \boldsymbol{\alpha} )
\mapsto \boldsymbol{\beta}, \,
\sigma( \boldsymbol{\theta} )
\mapsto \boldsymbol{\lambda}
\]</span> to give <span class="math display">\[
\begin{align*}
\mathbb{E}_{\pi} [ f ]
&amp;\propto \quad
\int_{\alpha_1 &lt; \ldots &lt; \alpha_K}
\prod_{k = 1}^{K} \mathrm{d} \theta_k \mathrm{d} \alpha_k \cdot
f (\boldsymbol{\alpha}) \cdot
\pi (\boldsymbol{\alpha}) \, \pi(\boldsymbol{\theta}) \,
\sum_{k = 1}^{K} \theta_k \, \pi (y \mid \alpha_k)
\\
&amp;\quad + \sum_{\sigma \in \Sigma&#39;}
\int_{\beta_1 &lt; \ldots &lt; \beta_K}
\prod_{k = 1}^{K}
\mathrm{d} \lambda_{\sigma^{-1}(k)} \mathrm{d} \beta_{\sigma^{-1}(k)} \cdot
f ( \sigma^{-1}(\boldsymbol{\beta}) ) \cdot
\pi ( \sigma^{-1}(\boldsymbol{\beta}) ) \,
\pi ( \sigma^{-1}(\boldsymbol{\lambda}) ) \,
\sum_{k = 1}^{K} \lambda_{\sigma^{-1}(k)} \, \pi (y \mid \beta_{\sigma^{-1}(k)})
\\
&amp;\propto \quad
\int_{\alpha_1 &lt; \ldots &lt; \alpha_K}
\prod_{k = 1}^{K} \mathrm{d} \theta_k \mathrm{d} \alpha_k \cdot
f (\boldsymbol{\alpha}) \cdot
\pi (\boldsymbol{\alpha}) \, \pi(\boldsymbol{\theta}) \,
\sum_{k = 1}^{K} \theta_k \, \pi (y \mid \alpha_k)
\\
&amp;\quad + \sum_{\sigma \in \Sigma&#39;}
\int_{\beta_1 &lt; \ldots &lt; \beta_K}
\prod_{k&#39; = 1}^{K}
\mathrm{d} \lambda_{k&#39;} \mathrm{d} \beta_{k&#39;} \cdot
f ( \sigma^{-1}(\boldsymbol{\beta}) ) \cdot
\pi ( \sigma^{-1}(\boldsymbol{\beta}) ) \,
\pi ( \sigma^{-1}(\boldsymbol{\lambda}) ) \,
\sum_{k&#39; = 1}^{K} \lambda_{k&#39;} \, \pi (y \mid \beta_{k&#39;}).
\end{align*}
\]</span> We now exploit the permutation-invariance of <span class="math inline">\(f\)</span> and the exchangeability of the priors to give <span class="math display">\[
\begin{align*}
\mathbb{E}_{\pi} [ f ]
&amp;\propto \quad
\int_{\alpha_1 &lt; \ldots &lt; \alpha_K}
\prod_{k = 1}^{K} \mathrm{d} \theta_k \mathrm{d} \alpha_k \cdot
f (\boldsymbol{\alpha}) \cdot
\pi (\boldsymbol{\alpha}) \, \pi(\boldsymbol{\theta}) \,
\sum_{k = 1}^{K} \theta_k \, \pi (y \mid \alpha_k)
\\
&amp;\quad + \sum_{\sigma \in \Sigma&#39;}
\int_{\beta_1 &lt; \ldots &lt; \beta_K}
\prod_{k&#39; = 1}^{K}
\mathrm{d} \lambda_{k&#39;} \mathrm{d} \beta_{k&#39;} \cdot
f ( \boldsymbol{\beta} ) \cdot
\pi ( \boldsymbol{\beta} ) \, \pi ( \boldsymbol{\lambda} ) \,
\sum_{k&#39; = 1}^{K} \lambda_{k&#39;} \, \pi (y \mid \beta_{k&#39;})
\\
&amp;\propto K!
\int_{\alpha_1 &lt; \ldots &lt; \alpha_K}
\prod_{k = 1}^{K} \mathrm{d} \theta_k \mathrm{d} \alpha_k \cdot
f (\boldsymbol{\alpha}) \cdot
\pi (\boldsymbol{\alpha}) \, \pi(\boldsymbol{\theta}) \,
\sum_{k = 1}^{K} \theta_k \, \pi (y \mid \alpha_k)
\\
&amp;\propto
\int_{\alpha_1 &lt; \ldots &lt; \alpha_K}
\prod_{k = 1}^{K} \mathrm{d} \theta_k \mathrm{d} \alpha_k \cdot
f (\boldsymbol{\alpha}) \cdot
K! \, \pi (\boldsymbol{\alpha}) \, \pi(\boldsymbol{\theta}) \,
\sum_{k = 1}^{K} \theta_k \, \pi (y \mid \alpha_k)
\\
&amp;= \int_{\alpha_1 &lt; \ldots &lt; \alpha_N}
\prod_{n = 1}^{K} \mathrm{d} \theta_n \mathrm{d} \alpha_n \cdot
f (\boldsymbol{\alpha}) \cdot
\pi&#39; (\boldsymbol{\alpha}, \boldsymbol{\theta} \mid y).
\end{align*}
\]</span></p>
<p>Once again <span class="math display">\[
\pi&#39; (\boldsymbol{\alpha}, \boldsymbol{\theta} \mid y)
\propto
K! \, \pi (\boldsymbol{\alpha}) \, \pi(\boldsymbol{\theta}) \,
\sum_{k = 1}^{K} \theta_k \, \pi (y \mid \alpha_k)
\]</span> is exactly the mixture posterior density confined to the pyramid given by the standard ordering. Consequently, in general we can exactly recover the desired expectation by integrating over only the region of parameter space satisfying the chosen ordering constraint.</p>
<p>While the ordering is limited to scalar parameters, it can still prove useful when the component distributions are multivariate. Although we cannot order the multivariate parameters themselves, ordering any one of the parameters is sufficient to break the labeling degeneracy for the entire mixture.</p>
</div>
</div>
</div>
</div>
<div id="a-bayesian-mixture-model-example" class="section level1">
<h1>A Bayesian Mixture Model Example</h1>
<p>To illustrate the pathologies of Bayesian mixture models, and their potential resolutions, let’s consider a relatively simple example where the likelihood is given by a mixture of two Gaussians, <span class="math display">\[
\pi(y_1, \ldots, y_N \mid \mu_1, \sigma_1, \mu_2, \sigma_2, \theta_1, \theta_2)
=
\sum_{n = 1}^{N}
\theta_1 \mathcal{N} (y_n \mid \mu_1, \sigma_1)
+ \theta_2 \mathcal{N} (y_n \mid \mu_2, \sigma_2).
\]</span> Note that the mixture is applied to each datum individually – our model assumes that each measurement is drawn from one of the components independently as opposed to the entire dataset being drawn from one of the components as a whole.</p>
<p>We first define the component data generating processes to be well-separated relative to their standard deviations,</p>
<pre class="r"><code>mu &lt;- c(-2.75, 2.75);
sigma &lt;- c(1, 1);
lambda &lt;- 0.4</code></pre>
<p>Then we simulate some data from the mixture likelihood by following its generative structure, first drawing assignments for each measurement and then drawing the measurements themselves from the corresponding Gaussian,</p>
<pre class="r"><code>set.seed(689934)

N &lt;- 1000
z &lt;- rbinom(N, 1, lambda) + 1;
y &lt;- rnorm(N, mu[z], sigma[z]);</code></pre>
<pre class="r"><code>library(rstan)
rstan_options(auto_write = TRUE)

stan_rdump(c(&quot;N&quot;, &quot;y&quot;), file=&quot;mix.data.R&quot;)</code></pre>
<p>Let’s now consider a Bayesian fit of this model, first with labeling degeneracies and then with various attempted resolutions.</p>
<div id="a-degenerate-implementation" class="section level2">
<h2>A Degenerate Implementation</h2>
<p>As discussed above, in order to ensure that the labeling degeneracies persist in the posterior distribution we need exchangeable priors. We can, for example, accomplish this by assigning the identical priors to the Gaussian parameters, <span class="math display">\[
\mu_1, \mu_2 \sim \mathcal{N} (0, 2), \,
\sigma_1, \sigma_2 \sim \text{Half-}\mathcal{N} (0, 2),
\]</span> and a symmetric Beta distribution to the mixture weight, <span class="math display">\[
\theta_1 \sim \text{Beta} (5, 5).
\]</span></p>
<pre class="r"><code>writeLines(readLines(&quot;gauss_mix.stan&quot;))</code></pre>
<pre><code>data {
 int&lt;lower = 0&gt; N;
 vector[N] y;
}

parameters {
  vector[2] mu;
  real&lt;lower=0&gt; sigma[2];
  real&lt;lower=0, upper=1&gt; theta;
}

model {
 sigma ~ normal(0, 2);
 mu ~ normal(0, 2);
 theta ~ beta(5, 5);
 for (n in 1:N)
   target += log_mix(theta,
                     normal_lpdf(y[n] | mu[1], sigma[1]),
                     normal_lpdf(y[n] | mu[2], sigma[2]));
}</code></pre>
<p>Equivalently we could also have defined <span class="math inline">\(\theta\)</span> as a two-dimensional simplex with a <span class="math inline">\(\text{Dirichlet}(5, 5)\)</span> prior which would yield the same exact model.</p>
<p>Aware of the labeling degeneracy, let’s go ahead and fit this Bayesian mixture model in Stan,</p>
<pre class="r"><code>input_data &lt;- read_rdump(&quot;mix.data.R&quot;)

degenerate_fit &lt;- stan(file=&#39;gauss_mix.stan&#39;, data=input_data,
                       chains=4, seed=483892929, refresh=2000)</code></pre>
<pre><code>
SAMPLING FOR MODEL &#39;gauss_mix&#39; NOW (CHAIN 1).

Chain 1, Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1, Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1, Iteration: 2000 / 2000 [100%]  (Sampling)
 Elapsed Time: 1.38577 seconds (Warm-up)
               0.973725 seconds (Sampling)
               2.3595 seconds (Total)


SAMPLING FOR MODEL &#39;gauss_mix&#39; NOW (CHAIN 2).

Chain 2, Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2, Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2, Iteration: 2000 / 2000 [100%]  (Sampling)
 Elapsed Time: 1.30787 seconds (Warm-up)
               0.909254 seconds (Sampling)
               2.21712 seconds (Total)


SAMPLING FOR MODEL &#39;gauss_mix&#39; NOW (CHAIN 3).

Chain 3, Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3, Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3, Iteration: 2000 / 2000 [100%]  (Sampling)
 Elapsed Time: 1.33762 seconds (Warm-up)
               1.2385 seconds (Sampling)
               2.57612 seconds (Total)


SAMPLING FOR MODEL &#39;gauss_mix&#39; NOW (CHAIN 4).

Chain 4, Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 4, Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4, Iteration: 2000 / 2000 [100%]  (Sampling)
 Elapsed Time: 1.43549 seconds (Warm-up)
               0.895336 seconds (Sampling)
               2.33083 seconds (Total)</code></pre>
<p>The split Rhat is atrocious, indicating that the chains are not exploring the same regions of parameter space.</p>
<pre class="r"><code>print(degenerate_fit)</code></pre>
<pre><code>Inference for Stan model: gauss_mix.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

             mean se_mean   sd     2.5%      25%      50%      75%
mu[1]       -1.33    1.72 2.43    -2.81    -2.75    -2.72    -1.26
mu[2]        1.47    1.72 2.43    -2.79     1.36     2.85     2.89
sigma[1]     1.03    0.00 0.03     0.96     1.00     1.03     1.05
sigma[2]     1.02    0.00 0.04     0.95     1.00     1.02     1.05
theta        0.56    0.07 0.11     0.36     0.53     0.62     0.63
lp__     -2108.57    0.03 1.55 -2112.46 -2109.43 -2108.28 -2107.43
            97.5% n_eff  Rhat
mu[1]        2.94     2 56.48
mu[2]        2.96     2 51.11
sigma[1]     1.09  4000  1.00
sigma[2]     1.10  4000  1.00
theta        0.65     2  7.52
lp__     -2106.51  2050  1.00

Samples were drawn using NUTS(diag_e) at Thu Mar  2 15:36:24 2017.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>Indeed this is to be expected as the individual chains find and then explore one of the two degenerate modes independently of the others,</p>
<pre class="r"><code>c_light_trans &lt;- c(&quot;#DCBCBCBF&quot;)
c_light_highlight_trans &lt;- c(&quot;#C79999BF&quot;)
c_mid_trans &lt;- c(&quot;#B97C7CBF&quot;)
c_mid_highlight_trans &lt;- c(&quot;#A25050BF&quot;)
c_dark_trans &lt;- c(&quot;#8F2727BF&quot;)
c_dark_highlight_trans &lt;- c(&quot;#7C0000BF&quot;)</code></pre>
<pre class="r"><code>params1 &lt;- as.data.frame(extract(degenerate_fit, permuted=FALSE)[,1,])
params2 &lt;- as.data.frame(extract(degenerate_fit, permuted=FALSE)[,2,])
params3 &lt;- as.data.frame(extract(degenerate_fit, permuted=FALSE)[,3,])
params4 &lt;- as.data.frame(extract(degenerate_fit, permuted=FALSE)[,4,])

par(mar = c(4, 4, 0.5, 0.5))
plot(params1$&quot;mu[1]&quot;, params1$&quot;mu[2]&quot;, col=c_dark_highlight_trans, pch=16, cex=0.8,
     xlab=&quot;mu1&quot;, xlim=c(-3, 3), ylab=&quot;mu2&quot;, ylim=c(-3, 3))
points(params2$&quot;mu[1]&quot;, params2$&quot;mu[2]&quot;, col=c_dark_trans, pch=16, cex=0.8)
points(params3$&quot;mu[1]&quot;, params3$&quot;mu[2]&quot;, col=c_mid_highlight_trans, pch=16, cex=0.8)
points(params4$&quot;mu[1]&quot;, params4$&quot;mu[2]&quot;, col=c_mid_trans, pch=16, cex=0.8)
lines(0.08*(1:100) - 4, 0.08*(1:100) - 4, col=&quot;grey&quot;, lw=2)
legend(&quot;topright&quot;, c(&quot;Chain 1&quot;, &quot;Chain 2&quot;, &quot;Chain 3&quot;, &quot;Chain 4&quot;),
       fill=c(c_dark_highlight_trans, c_dark_trans,
              c_mid_highlight_trans, c_mid_trans), box.lty=0, inset=0.0005)</code></pre>
<p><img src="identifying_mixture_models_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>This degenerate example is a particularly nice demonstration of the importance of running multiple chains in any MCMC analysis. If we had run just one chain then we would have had no indication of the multimodality in our the posterior and the incompleteness of our fits!</p>
</div>
<div id="breaking-the-labeling-degeneracy-with-non-exchangeable-priors" class="section level2">
<h2>Breaking the Labeling Degeneracy with Non-exchangeable Priors</h2>
<p>Our first potential resolution of the labeling degeneracy is to tweak the priors to no longer be exchangeable. With no reason to expect that the standard deviations will vary between the two components, we will instead adjust the priors for the means to strongly favor <span class="math inline">\(\mu_1\)</span> positive and <span class="math inline">\(\mu_2\)</span> negative,</p>
<pre class="r"><code>writeLines(readLines(&quot;gauss_mix_asym_prior.stan&quot;))</code></pre>
<pre><code>data {
 int&lt;lower = 0&gt; N;
 vector[N] y;
}

parameters {
  vector[2] mu;
  real&lt;lower=0&gt; sigma[2];
  real&lt;lower=0, upper=1&gt; theta;
}

model {
 sigma ~ normal(0, 2);
 mu[1] ~ normal(4, 0.5);
 mu[2] ~ normal(-4, 0.5);
 theta ~ beta(5, 5);
 for (n in 1:N)
   target += log_mix(theta,
                     normal_lpdf(y[n] | mu[1], sigma[1]),
                     normal_lpdf(y[n] | mu[2], sigma[2]));
}</code></pre>
<p>Running in Stan</p>
<pre class="r"><code>asym_fit &lt;- stan(file=&#39;gauss_mix_asym_prior.stan&#39;, data=input_data,
                 chains=4, seed=483892929, refresh=2000)</code></pre>
<pre><code>
SAMPLING FOR MODEL &#39;gauss_mix_asym_prior&#39; NOW (CHAIN 1).

Chain 1, Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1, Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1, Iteration: 2000 / 2000 [100%]  (Sampling)
 Elapsed Time: 1.32051 seconds (Warm-up)
               0.892697 seconds (Sampling)
               2.21321 seconds (Total)


SAMPLING FOR MODEL &#39;gauss_mix_asym_prior&#39; NOW (CHAIN 2).

Chain 2, Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2, Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2, Iteration: 2000 / 2000 [100%]  (Sampling)
 Elapsed Time: 1.37814 seconds (Warm-up)
               1.60082 seconds (Sampling)
               2.97896 seconds (Total)


SAMPLING FOR MODEL &#39;gauss_mix_asym_prior&#39; NOW (CHAIN 3).

Chain 3, Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3, Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3, Iteration: 2000 / 2000 [100%]  (Sampling)
 Elapsed Time: 1.34297 seconds (Warm-up)
               0.927972 seconds (Sampling)
               2.27095 seconds (Total)


SAMPLING FOR MODEL &#39;gauss_mix_asym_prior&#39; NOW (CHAIN 4).

Chain 4, Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 4, Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4, Iteration: 2000 / 2000 [100%]  (Sampling)
 Elapsed Time: 1.40858 seconds (Warm-up)
               0.901724 seconds (Sampling)
               2.3103 seconds (Total)</code></pre>
<p>we see that split Rhat still looks terrible,</p>
<pre class="r"><code>print(asym_fit)</code></pre>
<pre><code>Inference for Stan model: gauss_mix_asym_prior.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

             mean se_mean    sd     2.5%      25%      50%      75%
mu[1]        1.49    1.71  2.42    -2.75     1.39     2.86     2.91
mu[2]       -1.36    1.69  2.40    -2.82    -2.76    -2.72    -1.29
sigma[1]     1.02    0.00  0.04     0.95     1.00     1.02     1.05
sigma[2]     1.03    0.00  0.03     0.97     1.01     1.03     1.05
theta        0.44    0.07  0.11     0.35     0.37     0.39     0.47
lp__     -2156.80   54.45 77.03 -2292.21 -2163.47 -2112.71 -2111.44
            97.5% n_eff  Rhat
mu[1]        2.98     2 49.94
mu[2]        2.86     2 57.85
sigma[1]     1.10  4000  1.00
sigma[2]     1.10  4000  1.00
theta        0.64     2  7.29
lp__     -2110.31     2 51.79

Samples were drawn using NUTS(diag_e) at Thu Mar  2 15:36:35 2017.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>Investigating the fit output we see that despite our strong, asymmetric prior the posterior distribution is still multimodal and the disfavored mode is still capturing chains,</p>
<pre class="r"><code>params1 &lt;- as.data.frame(extract(asym_fit, permuted=FALSE)[,1,])
params2 &lt;- as.data.frame(extract(asym_fit, permuted=FALSE)[,2,])
params3 &lt;- as.data.frame(extract(asym_fit, permuted=FALSE)[,3,])
params4 &lt;- as.data.frame(extract(asym_fit, permuted=FALSE)[,4,])

par(mar = c(4, 4, 0.5, 0.5))
plot(params1$&quot;mu[1]&quot;, params1$&quot;mu[2]&quot;, col=c_dark_highlight_trans, pch=16, cex=0.8,
     xlab=&quot;mu1&quot;, xlim=c(-3, 3), ylab=&quot;mu2&quot;, ylim=c(-3, 3))
points(params2$&quot;mu[1]&quot;, params2$&quot;mu[2]&quot;, col=c_dark_trans, pch=16, cex=0.8)
points(params3$&quot;mu[1]&quot;, params3$&quot;mu[2]&quot;, col=c_mid_highlight_trans, pch=16, cex=0.8)
points(params4$&quot;mu[1]&quot;, params4$&quot;mu[2]&quot;, col=c_mid_trans, pch=16, cex=0.8)
lines(0.08*(1:100) - 4, 0.08*(1:100) - 4, col=&quot;grey&quot;, lw=2)
legend(&quot;topright&quot;, c(&quot;Chain 1&quot;, &quot;Chain 2&quot;, &quot;Chain 3&quot;, &quot;Chain 4&quot;),
       fill=c(c_dark_highlight_trans, c_dark_trans,
              c_mid_highlight_trans, c_mid_trans), box.lty=0, inset=0.0005)</code></pre>
<p><img src="identifying_mixture_models_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>This example clearly demonstrates the subtle challenge of trying to resolve labeling degeneracy with most non-exchangeable prior distributions. When there are many data the mixture likelihood will be very informative and it can easily overwhelm even strong prior information. The posterior will no longer be symmetric between the degenerate modes, but the disfavored modes will still have sufficiently significant posterior mass to require exploration in the fit.</p>
<p>Reducing the amount data, however, reduces the influence of the likelihood and makes it easier to corral the mixture components in principled directions. With a smaller data set,</p>
<pre class="r"><code>N &lt;- 100
z &lt;- rbinom(N, 1, lambda) + 1;
y &lt;- rnorm(N, mu[z], sigma[z]);

stan_rdump(c(&quot;N&quot;, &quot;y&quot;), file=&quot;mix_low.data.R&quot;)
input_low_data &lt;- read_rdump(&quot;mix_low.data.R&quot;)

asym_fit &lt;- stan(file=&#39;gauss_mix_asym_prior.stan&#39;, data=input_low_data,
                 chains=4, seed=483892929, refresh=2000)</code></pre>
<pre><code>
SAMPLING FOR MODEL &#39;gauss_mix_asym_prior&#39; NOW (CHAIN 1).

Chain 1, Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1, Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1, Iteration: 2000 / 2000 [100%]  (Sampling)
 Elapsed Time: 0.163203 seconds (Warm-up)
               0.256404 seconds (Sampling)
               0.419607 seconds (Total)


SAMPLING FOR MODEL &#39;gauss_mix_asym_prior&#39; NOW (CHAIN 2).

Chain 2, Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2, Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2, Iteration: 2000 / 2000 [100%]  (Sampling)
 Elapsed Time: 0.161927 seconds (Warm-up)
               0.107424 seconds (Sampling)
               0.269351 seconds (Total)


SAMPLING FOR MODEL &#39;gauss_mix_asym_prior&#39; NOW (CHAIN 3).

Chain 3, Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3, Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3, Iteration: 2000 / 2000 [100%]  (Sampling)
 Elapsed Time: 0.163485 seconds (Warm-up)
               0.169538 seconds (Sampling)
               0.333023 seconds (Total)


SAMPLING FOR MODEL &#39;gauss_mix_asym_prior&#39; NOW (CHAIN 4).

Chain 4, Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 4, Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4, Iteration: 2000 / 2000 [100%]  (Sampling)
 Elapsed Time: 0.158586 seconds (Warm-up)
               0.123968 seconds (Sampling)
               0.282554 seconds (Total)</code></pre>
<p>we achieve a much better fit,</p>
<pre class="r"><code>print(asym_fit)</code></pre>
<pre><code>Inference for Stan model: gauss_mix_asym_prior.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

            mean se_mean   sd    2.5%     25%     50%     75%   97.5%
mu[1]       2.80    0.00 0.18    2.45    2.67    2.79    2.91    3.16
mu[2]      -2.69    0.00 0.12   -2.93   -2.77   -2.69   -2.60   -2.45
sigma[1]    1.01    0.00 0.16    0.76    0.90    0.99    1.10    1.37
sigma[2]    1.03    0.00 0.10    0.86    0.96    1.02    1.09    1.25
theta       0.33    0.00 0.04    0.25    0.30    0.33    0.36    0.42
lp__     -218.35    0.04 1.63 -222.57 -219.15 -218.02 -217.16 -216.22
         n_eff Rhat
mu[1]     4000    1
mu[2]     4000    1
sigma[1]  4000    1
sigma[2]  4000    1
theta     4000    1
lp__      2106    1

Samples were drawn using NUTS(diag_e) at Thu Mar  2 15:36:37 2017.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>All of our chains converge to the favored mode and our inference become well-behaved,</p>
<pre class="r"><code>params1 &lt;- as.data.frame(extract(asym_fit, permuted=FALSE)[,1,])
params2 &lt;- as.data.frame(extract(asym_fit, permuted=FALSE)[,2,])
params3 &lt;- as.data.frame(extract(asym_fit, permuted=FALSE)[,3,])
params4 &lt;- as.data.frame(extract(asym_fit, permuted=FALSE)[,4,])

par(mar = c(4, 4, 0.5, 0.5))
plot(params1$&quot;mu[1]&quot;, params1$&quot;mu[2]&quot;, col=c_dark_highlight_trans, pch=16, cex=0.8,
     xlab=&quot;mu1&quot;, xlim=c(-3, 3), ylab=&quot;mu2&quot;, ylim=c(-3, 3))
points(params2$&quot;mu[1]&quot;, params2$&quot;mu[2]&quot;, col=c_dark_trans, pch=16, cex=0.8)
points(params3$&quot;mu[1]&quot;, params3$&quot;mu[2]&quot;, col=c_mid_highlight_trans, pch=16, cex=0.8)
points(params4$&quot;mu[1]&quot;, params4$&quot;mu[2]&quot;, col=c_mid_trans, pch=16, cex=0.8)
lines(0.08*(1:100) - 4, 0.08*(1:100) - 4, col=&quot;grey&quot;, lw=2)
legend(&quot;topright&quot;, c(&quot;Chain 1&quot;, &quot;Chain 2&quot;, &quot;Chain 3&quot;, &quot;Chain 4&quot;),
       fill=c(c_dark_highlight_trans, c_dark_trans,
              c_mid_highlight_trans, c_mid_trans), box.lty=0, inset=0.0005)</code></pre>
<p><img src="identifying_mixture_models_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>Still, extreme care must be taken when using this approach to ensure that the priors are strong enough to suppress the labeling degeneracy. In particular, running multiple chains is critical to identifying insufficiently strong priors.</p>
</div>
<div id="breaking-the-labeling-degeneracy-by-enforcing-an-ordering" class="section level2">
<h2>Breaking the Labeling Degeneracy by Enforcing an Ordering</h2>
<p>If we consider only inferences insensitive to the labelings then we can also keep the exchangeable priors but impose an ordering on one of the component parameters. Alternatively, we can consider this as making the exchangeable prior non-exchangeable by restricting it to a subset of parameter space. Following a similar logic that motivated tweaking the component means above, we’ll order the means here. Fortunately, imposing such an order in Stan could not be easier – as we have to do is employ the <code>ordered</code> type,</p>
<pre class="r"><code>writeLines(readLines(&quot;gauss_mix_ordered_prior.stan&quot;))</code></pre>
<pre><code>data {
 int&lt;lower = 0&gt; N;
 vector[N] y;
}

parameters {
  ordered[2] mu;
  real&lt;lower=0&gt; sigma[2];
  real&lt;lower=0, upper=1&gt; theta;
}

model {
 sigma ~ normal(0, 2);
 mu ~ normal(0, 2);
 theta ~ beta(5, 5);
 for (n in 1:N)
   target += log_mix(theta,
                     normal_lpdf(y[n] | mu[1], sigma[1]),
                     normal_lpdf(y[n] | mu[2], sigma[2]));
}</code></pre>
<p>Running the model in Stan</p>
<pre class="r"><code>ordered_fit &lt;- stan(file=&#39;gauss_mix_ordered_prior.stan&#39;, data=input_data,
                    chains=4, seed=483892929, refresh=2000)</code></pre>
<pre><code>
SAMPLING FOR MODEL &#39;gauss_mix_ordered_prior&#39; NOW (CHAIN 1).

Chain 1, Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1, Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1, Iteration: 2000 / 2000 [100%]  (Sampling)
 Elapsed Time: 1.67068 seconds (Warm-up)
               1.58276 seconds (Sampling)
               3.25344 seconds (Total)</code></pre>
<pre><code>The following numerical problems occured the indicated number of times on chain 1</code></pre>
<pre><code>                                                                                        count
Exception thrown at line 17: normal_log: Location parameter is inf, but must be finite!     1</code></pre>
<pre><code>When a numerical problem occurs, the Hamiltonian proposal gets rejected.</code></pre>
<pre><code>See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected</code></pre>
<pre><code>If the number in the &#39;count&#39; column is small, do not ask about this message on stan-users.</code></pre>
<pre><code>
SAMPLING FOR MODEL &#39;gauss_mix_ordered_prior&#39; NOW (CHAIN 2).

Chain 2, Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2, Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2, Iteration: 2000 / 2000 [100%]  (Sampling)
 Elapsed Time: 1.6663 seconds (Warm-up)
               1.49428 seconds (Sampling)
               3.16058 seconds (Total)</code></pre>
<pre><code>The following numerical problems occured the indicated number of times on chain 2</code></pre>
<pre><code>                                                                                        count
Exception thrown at line 17: normal_log: Location parameter is inf, but must be finite!     8</code></pre>
<pre><code>When a numerical problem occurs, the Hamiltonian proposal gets rejected.</code></pre>
<pre><code>See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected</code></pre>
<pre><code>If the number in the &#39;count&#39; column is small, do not ask about this message on stan-users.</code></pre>
<pre><code>
SAMPLING FOR MODEL &#39;gauss_mix_ordered_prior&#39; NOW (CHAIN 3).

Chain 3, Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3, Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3, Iteration: 2000 / 2000 [100%]  (Sampling)
 Elapsed Time: 1.86612 seconds (Warm-up)
               1.45769 seconds (Sampling)
               3.32381 seconds (Total)</code></pre>
<pre><code>The following numerical problems occured the indicated number of times on chain 3</code></pre>
<pre><code>                                                                                count
Exception thrown at line 17: normal_log: Scale parameter is 0, but must be &gt; 0!     3</code></pre>
<pre><code>When a numerical problem occurs, the Hamiltonian proposal gets rejected.</code></pre>
<pre><code>See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected</code></pre>
<pre><code>If the number in the &#39;count&#39; column is small, do not ask about this message on stan-users.</code></pre>
<pre><code>
SAMPLING FOR MODEL &#39;gauss_mix_ordered_prior&#39; NOW (CHAIN 4).

Chain 4, Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 4, Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4, Iteration: 2000 / 2000 [100%]  (Sampling)
 Elapsed Time: 1.85206 seconds (Warm-up)
               1.46471 seconds (Sampling)
               3.31677 seconds (Total)</code></pre>
<pre><code>The following numerical problems occured the indicated number of times on chain 4</code></pre>
<pre><code>                                                                                count
Exception thrown at line 17: normal_log: Scale parameter is 0, but must be &gt; 0!     1</code></pre>
<pre><code>When a numerical problem occurs, the Hamiltonian proposal gets rejected.</code></pre>
<pre><code>See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected</code></pre>
<pre><code>If the number in the &#39;count&#39; column is small, do not ask about this message on stan-users.</code></pre>
<p>we immediately recover a better fit,</p>
<pre class="r"><code>print(ordered_fit)</code></pre>
<pre><code>Inference for Stan model: gauss_mix_ordered_prior.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

             mean se_mean   sd     2.5%      25%      50%      75%
mu[1]       -2.73    0.00 0.04    -2.82    -2.76    -2.73    -2.70
mu[2]        2.87    0.00 0.05     2.77     2.83     2.87     2.90
sigma[1]     1.03    0.00 0.03     0.97     1.01     1.03     1.05
sigma[2]     1.02    0.00 0.04     0.94     1.00     1.02     1.05
theta        0.62    0.00 0.02     0.59     0.61     0.62     0.63
lp__     -2106.85    0.03 1.56 -2110.61 -2107.68 -2106.50 -2105.69
            97.5% n_eff Rhat
mu[1]       -2.65  3276    1
mu[2]        2.98  4000    1
sigma[1]     1.09  4000    1
sigma[2]     1.11  4000    1
theta        0.65  4000    1
lp__     -2104.75  2173    1

Samples were drawn using NUTS(diag_e) at Thu Mar  2 15:36:51 2017.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>with all of the chains exploring the mode satisfying the standard ordering <span class="math display">\[\mu_1 \le \mu_2,\]</span></p>
<pre class="r"><code>params1 &lt;- as.data.frame(extract(ordered_fit, permuted=FALSE)[,1,])
params2 &lt;- as.data.frame(extract(ordered_fit, permuted=FALSE)[,2,])
params3 &lt;- as.data.frame(extract(ordered_fit, permuted=FALSE)[,3,])
params4 &lt;- as.data.frame(extract(ordered_fit, permuted=FALSE)[,4,])

par(mar = c(4, 4, 0.5, 0.5))
plot(params1$&quot;mu[1]&quot;, params1$&quot;mu[2]&quot;, col=c_dark_highlight_trans, pch=16, cex=0.8,
     xlab=&quot;mu1&quot;, xlim=c(-3, 3), ylab=&quot;mu2&quot;, ylim=c(-3, 3))
points(params2$&quot;mu[1]&quot;, params2$&quot;mu[2]&quot;, col=c_dark_trans, pch=16, cex=0.8)
points(params3$&quot;mu[1]&quot;, params3$&quot;mu[2]&quot;, col=c_mid_highlight_trans, pch=16, cex=0.8)
points(params4$&quot;mu[1]&quot;, params4$&quot;mu[2]&quot;, col=c_mid_trans, pch=16, cex=0.8)
lines(0.08*(1:100) - 4, 0.08*(1:100) - 4, col=&quot;grey&quot;, lw=2)
legend(&quot;topright&quot;, c(&quot;Chain 1&quot;, &quot;Chain 2&quot;, &quot;Chain 3&quot;, &quot;Chain 4&quot;),
       fill=c(c_dark_highlight_trans, c_dark_trans,
              c_mid_highlight_trans, c_mid_trans), box.lty=0, inset=0.0005)</code></pre>
<p><img src="identifying_mixture_models_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>Contrast the immediate success achieved by ordering with the subtleties required to get a directly non-exchangeable prior to yield a good fit.</p>
</div>
</div>
<div id="singular-components-and-computational-issues" class="section level1">
<h1>Singular Components and Computational Issues</h1>
<p>Unfortunately, there is one more pathology hidden in mixture models that can seriously compromise the accuracy of a fit. If two of the mixture components overlap then the resulting posterior manifests a particularly nasty geometry that is difficult to explore. This overlap is especially common when there are more components in the mixture than needed to capture the structure in the data, and excess components are forced to collapse into the necessary components.</p>
<p>To see this let’s simulate a measurement where the two mixture components are located less than one standard deviation away from each other,</p>
<pre class="r"><code>N &lt;- 1000
mu &lt;- c(-0.75, 0.75);
sigma &lt;- c(1, 1);
lambda &lt;- 0.4
z &lt;- rbinom(N, 1, lambda) + 1;
y &lt;- rnorm(N, mu[z], sigma[z]);

stan_rdump(c(&quot;N&quot;, &quot;y&quot;), file=&quot;collapse_mix.data.R&quot;)</code></pre>
<p>Before fitting the joint model over the component parameters and the mixture weights, let’s try to fit conditioned on a single mixture weight. Note that we will be using just a single chain in this section as the posteriors will all be unimodal, but in practice one should always multiple chains as we will not have such a guarantee.</p>
<pre class="r"><code>writeLines(readLines(&quot;gauss_mix_given_theta.stan&quot;))</code></pre>
<pre><code>data {
 int&lt;lower = 0&gt; N;
 vector[N] y;
 real&lt;lower=0, upper=1&gt; theta;
}

parameters {
  vector[2] mu;
  real&lt;lower=0&gt; sigma[2];
}

model {
 sigma ~ normal(0, 2);
 mu ~ normal(0, 2);
 for (n in 1:N)
   target += log_mix(theta,
                     normal_lpdf(y[n] | mu[1], sigma[1]),
                     normal_lpdf(y[n] | mu[2], sigma[2]));
}</code></pre>
<pre class="r"><code>theta &lt;- 0.25
stan_rdump(c(&quot;N&quot;, &quot;y&quot;, &quot;theta&quot;), file=&quot;collapse_mix.data.R&quot;)

input_data &lt;- read_rdump(&quot;collapse_mix.data.R&quot;)

singular_fit &lt;- stan(file=&#39;gauss_mix_given_theta.stan&#39;, data=input_data,
                     chains=1, iter=11000, warmup=1000, seed=483892929,
                     refresh=11000)</code></pre>
<pre><code>
SAMPLING FOR MODEL &#39;gauss_mix_given_theta&#39; NOW (CHAIN 1).

Chain 1, Iteration:     1 / 11000 [  0%]  (Warmup)
Chain 1, Iteration:  1001 / 11000 [  9%]  (Sampling)
Chain 1, Iteration: 11000 / 11000 [100%]  (Sampling)
 Elapsed Time: 2.41239 seconds (Warm-up)
               40.2729 seconds (Sampling)
               42.6853 seconds (Total)</code></pre>
<pre><code>The following numerical problems occured the indicated number of times on chain 1</code></pre>
<pre><code>                                                                                count
Exception thrown at line 16: normal_log: Scale parameter is 0, but must be &gt; 0!     2</code></pre>
<pre><code>When a numerical problem occurs, the Hamiltonian proposal gets rejected.</code></pre>
<pre><code>See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected</code></pre>
<pre><code>If the number in the &#39;count&#39; column is small, do not ask about this message on stan-users.</code></pre>
<p>Because the components are so close together they can move around to yield similar fits to the data, and the multimodal non-identifiability from the labeling degeneracy collapses into a strong continuous non-identifiability,</p>
<pre class="r"><code>c_light &lt;- c(&quot;#DCBCBC&quot;)
c_light_highlight &lt;- c(&quot;#C79999&quot;)
c_mid &lt;- c(&quot;#B97C7C&quot;)
c_mid_highlight &lt;- c(&quot;#A25050&quot;)
c_dark &lt;- c(&quot;#8F2727&quot;)
c_dark_highlight &lt;- c(&quot;#7C0000&quot;)</code></pre>
<pre class="r"><code>params25 &lt;- as.data.frame(extract(singular_fit, permuted=FALSE)[,1,])

par(mar = c(4, 4, 0.5, 0.5))
plot(params25$&quot;mu[1]&quot;, params25$&quot;mu[2]&quot;, col=c_dark_highlight, pch=16, cex=0.8,
     xlab=&quot;mu1&quot;, xlim=c(-3, 3), ylab=&quot;mu2&quot;, ylim=c(-3, 3))
lines(0.08*(1:100) - 4, 0.08*(1:100) - 4, col=&quot;grey&quot;, lw=2)</code></pre>
<p><img src="identifying_mixture_models_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>Interestingly, as we change the mixture weight the non-identifiability rotates around parameter space,</p>
<pre class="r"><code>theta &lt;- 0.5
stan_rdump(c(&quot;N&quot;, &quot;y&quot;, &quot;theta&quot;), file=&quot;collapse_mix.data.R&quot;)

input_data &lt;- read_rdump(&quot;collapse_mix.data.R&quot;)

singular_fit &lt;- stan(file=&#39;gauss_mix_given_theta.stan&#39;, data=input_data,
                     chains=1, iter=11000, warmup=1000, seed=483892929,
                     refresh=11000)</code></pre>
<pre><code>
SAMPLING FOR MODEL &#39;gauss_mix_given_theta&#39; NOW (CHAIN 1).

Chain 1, Iteration:     1 / 11000 [  0%]  (Warmup)
Chain 1, Iteration:  1001 / 11000 [  9%]  (Sampling)
Chain 1, Iteration: 11000 / 11000 [100%]  (Sampling)
 Elapsed Time: 2.35405 seconds (Warm-up)
               20.4307 seconds (Sampling)
               22.7848 seconds (Total)</code></pre>
<pre><code>The following numerical problems occured the indicated number of times on chain 1</code></pre>
<pre><code>                                                                                count
Exception thrown at line 16: normal_log: Scale parameter is 0, but must be &gt; 0!     3</code></pre>
<pre><code>When a numerical problem occurs, the Hamiltonian proposal gets rejected.</code></pre>
<pre><code>See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected</code></pre>
<pre><code>If the number in the &#39;count&#39; column is small, do not ask about this message on stan-users.</code></pre>
<pre class="r"><code>params50 &lt;- as.data.frame(extract(singular_fit, permuted=FALSE)[,1,])</code></pre>
<pre class="r"><code>theta &lt;- 0.75
stan_rdump(c(&quot;N&quot;, &quot;y&quot;, &quot;theta&quot;), file=&quot;collapse_mix.data.R&quot;)

input_data &lt;- read_rdump(&quot;collapse_mix.data.R&quot;)

singular_fit &lt;- stan(file=&#39;gauss_mix_given_theta.stan&#39;, data=input_data,
                     chains=1, iter=11000, warmup=1000, seed=483892929,
                     refresh=11000)</code></pre>
<pre><code>
SAMPLING FOR MODEL &#39;gauss_mix_given_theta&#39; NOW (CHAIN 1).

Chain 1, Iteration:     1 / 11000 [  0%]  (Warmup)
Chain 1, Iteration:  1001 / 11000 [  9%]  (Sampling)
Chain 1, Iteration: 11000 / 11000 [100%]  (Sampling)
 Elapsed Time: 1.69313 seconds (Warm-up)
               15.4523 seconds (Sampling)
               17.1454 seconds (Total)</code></pre>
<pre><code>The following numerical problems occured the indicated number of times on chain 1</code></pre>
<pre><code>                                                                                count
Exception thrown at line 16: normal_log: Scale parameter is 0, but must be &gt; 0!     2</code></pre>
<pre><code>When a numerical problem occurs, the Hamiltonian proposal gets rejected.</code></pre>
<pre><code>See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected</code></pre>
<pre><code>If the number in the &#39;count&#39; column is small, do not ask about this message on stan-users.</code></pre>
<pre class="r"><code>params75 &lt;- as.data.frame(extract(singular_fit, permuted=FALSE)[,1,])</code></pre>
<pre class="r"><code>par(mar = c(4, 4, 0.5, 0.5))
plot(params25$&quot;mu[1]&quot;, params25$&quot;mu[2]&quot;, col=c_dark_highlight, pch=16, cex=0.8,
     xlab=&quot;mu1&quot;, xlim=c(-3, 3), ylab=&quot;mu2&quot;, ylim=c(-3, 3))
points(params50$&quot;mu[1]&quot;, params50$&quot;mu[2]&quot;, col=c_mid_highlight, pch=16, cex=0.8)
points(params75$&quot;mu[1]&quot;, params75$&quot;mu[2]&quot;, col=c_light_highlight, pch=16, cex=0.8)

lines(0.08*(1:100) - 4, 0.08*(1:100) - 4, col=&quot;grey&quot;, lw=2)

legend(&quot;topleft&quot;, c(&quot;Theta = 0.25&quot;, &quot;Theta = 0.5&quot;, &quot;Theta = 0.75&quot;),
       fill=c(c_dark_highlight, c_mid_highlight, c_light_highlight), bty=&quot;n&quot;)</code></pre>
<p><img src="identifying_mixture_models_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<p>These conditional posteriors are not ideal, but they can be adequately fit with the state-of-the-art Hamiltonian Monte Carlo sampler in Stan. Unfortunately, when we try to fit the mixture weight jointly with the component parameters the conditional pathologies aggregate into something worse.</p>
<pre class="r"><code>singular_fit &lt;- stan(file=&#39;gauss_mix.stan&#39;, data=input_data,
                     chains=1, iter=11000, warmup=1000, seed=483892929,
                     refresh=11000)</code></pre>
<pre><code>
SAMPLING FOR MODEL &#39;gauss_mix&#39; NOW (CHAIN 1).

Chain 1, Iteration:     1 / 11000 [  0%]  (Warmup)
Chain 1, Iteration:  1001 / 11000 [  9%]  (Sampling)
Chain 1, Iteration: 11000 / 11000 [100%]  (Sampling)
 Elapsed Time: 4.04861 seconds (Warm-up)
               45.3196 seconds (Sampling)
               49.3682 seconds (Total)</code></pre>
<pre><code>The following numerical problems occured the indicated number of times on chain 1</code></pre>
<pre><code>                                                                                count
Exception thrown at line 17: normal_log: Scale parameter is 0, but must be &gt; 0!     3</code></pre>
<pre><code>When a numerical problem occurs, the Hamiltonian proposal gets rejected.</code></pre>
<pre><code>See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected</code></pre>
<pre><code>If the number in the &#39;count&#39; column is small, do not ask about this message on stan-users.</code></pre>
<p>The problem is that the collapsing components do not inform the mixture weights particularly well, and the posterior is changed little from the prior,</p>
<pre class="r"><code>params1 &lt;- as.data.frame(extract(singular_fit, permuted=FALSE)[,1,])

breaks=(0:50) / 50
prior_hist &lt;- hist(rbeta(10000, 5, 5), breaks=breaks, plot=FALSE)
post_hist &lt;- hist(params1$theta, breaks=breaks, plot=FALSE)

par(mar = c(4, 4, 0.5, 0.5))
plot(prior_hist, col=c_light_trans, border=c_light_highlight_trans,
     main=&quot;&quot;, xlab=&quot;theta&quot;, yaxt=&#39;n&#39;, ann=FALSE)
plot(post_hist, col=c_dark_trans, border=c_dark_highlight_trans, add=T)
legend(&quot;topright&quot;, c(&quot;Posterior&quot;, &quot;Prior&quot;),
       fill=c(c_dark_trans, c_light_trans), bty=&quot;n&quot;)</code></pre>
<p><img src="identifying_mixture_models_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<p>Consequently the joint posterior simply aggregates the conditional posteriors together, resulting in a “bowtie” geometry,</p>
<pre class="r"><code>par(mar = c(4, 4, 0.5, 0.5))
plot(params1$&quot;mu[1]&quot;, params1$&quot;mu[2]&quot;, col=c_dark_highlight, pch=16, cex=0.8,
     xlab=&quot;mu1&quot;, xlim=c(-3, 3), ylab=&quot;mu2&quot;, ylim=c(-3, 3))
lines(0.08*(1:100) - 4, 0.08*(1:100) - 4, col=&quot;grey&quot;, lw=2)</code></pre>
<p><img src="identifying_mixture_models_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<p>The sharp corners of the bowtie slow exploration, resulting in relatively few effective samples per iteration for the component means,</p>
<pre class="r"><code>print(singular_fit)</code></pre>
<pre><code>Inference for Stan model: gauss_mix.
1 chains, each with iter=11000; warmup=1000; thin=1; 
post-warmup draws per chain=10000, total post-warmup draws=10000.

             mean se_mean   sd     2.5%      25%      50%      75%
mu[1]       -0.08    0.15 0.74    -1.33    -0.90     0.22     0.51
mu[2]       -0.45    0.15 0.73    -1.36    -1.06    -0.80     0.29
sigma[1]     1.06    0.02 0.16     0.72     0.95     1.08     1.18
sigma[2]     1.00    0.02 0.16     0.69     0.89     1.00     1.13
theta        0.52    0.02 0.16     0.20     0.40     0.52     0.64
lp__     -1671.51    0.04 1.84 -1676.03 -1672.50 -1671.13 -1670.16
            97.5% n_eff Rhat
mu[1]        0.93    24 1.04
mu[2]        0.86    24 1.04
sigma[1]     1.32    52 1.02
sigma[2]     1.30    47 1.02
theta        0.82    84 1.01
lp__     -1669.07  2257 1.00

Samples were drawn using NUTS(diag_e) at Thu Mar 16 22:16:28 2017.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>While the bowtie geometry slows exploration for this particular model, however, it does not bias it – there are no divergences or other failing diagnostics,</p>
<pre class="r"><code>sum(get_sampler_params(singular_fit, inc_warmup=FALSE)[[1]][,&#39;divergent__&#39;])</code></pre>
<pre><code>[1] 0</code></pre>
<p>Consequently we can readily overcome the slower exploration by running longer or, even better, running multiple chains. Still, care must be taken in practice to ensure that the posterior geometry does not become too pathological for Stan to accurately fit.</p>
<p>Moreover, imposing a parameter ordering simply cuts the bowtie in half, maintaining the potentially pathological geometry,</p>
<pre class="r"><code>singular_fit &lt;- stan(file=&#39;gauss_mix_ordered_prior.stan&#39;, data=input_data,
                     chains=1, iter=11000, warmup=1000, seed=483892929,
                     refresh=11000)</code></pre>
<pre><code>
SAMPLING FOR MODEL &#39;gauss_mix_ordered_prior&#39; NOW (CHAIN 1).

Chain 1, Iteration:     1 / 11000 [  0%]  (Warmup)
Chain 1, Iteration:  1001 / 11000 [  9%]  (Sampling)
Chain 1, Iteration: 11000 / 11000 [100%]  (Sampling)
 Elapsed Time: 4.8613 seconds (Warm-up)
               58.0077 seconds (Sampling)
               62.869 seconds (Total)</code></pre>
<pre><code>The following numerical problems occured the indicated number of times on chain 1</code></pre>
<pre><code>                                                                                count
Exception thrown at line 17: normal_log: Scale parameter is 0, but must be &gt; 0!     3</code></pre>
<pre><code>When a numerical problem occurs, the Hamiltonian proposal gets rejected.</code></pre>
<pre><code>See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected</code></pre>
<pre><code>If the number in the &#39;count&#39; column is small, do not ask about this message on stan-users.</code></pre>
<pre class="r"><code>params1 &lt;- as.data.frame(extract(singular_fit, permuted=FALSE)[,1,])

par(mar = c(4, 4, 0.5, 0.5))
plot(params1$&quot;mu[1]&quot;, params1$&quot;mu[2]&quot;, col=c_dark_highlight, pch=16, cex=0.8,
     xlab=&quot;mu1&quot;, xlim=c(-3, 3), ylab=&quot;mu2&quot;, ylim=c(-3, 3))
lines(0.08*(1:100) - 4, 0.08*(1:100) - 4, col=&quot;grey&quot;, lw=2)</code></pre>
<p><img src="identifying_mixture_models_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<p>In order to avoid the bowtie geometry entirely we would need to ensure that the components are sufficiently separated. If this separation is not induced by the likelihood then it could be enforced with a prescient choice of prior, in particular one that “repels” the components from each other without sacrificing exchangeability. The development of such repulsive priors is an active topic of research.</p>
</div>
<div id="discussion" class="section level1">
<h1>Discussion</h1>
<p>The labeling non-identifiability inherent to degenerate mixture models yields multimodal distributions that even the most start-of-the-art computational algorithms cannot fit. In order to ensure accurate fits we need to break the labeling degeneracy in some way.</p>
<p>From the perspective of Bayesian mixture models we can break this degeneracy by complementing the mixture likelihood with non-exchangeable prior information. A principled prior that assigns each mixture component to a specific responsibility will break the degeneracy, but unless that prior is incredibly strong the modes corresponding to disfavored assignments will maintain enough posterior mass to require exploration. Alternatively, a non-exchangeable prior that enforces a standard parameter ordering exactly removes the labeling degeneracy without compromising inferences.</p>
<p>Even with the labeling degeneracy removed, however, Bayesian mixture models can still be troublesome to fit. As components begin to collapse into each other, for example, the posterior geometry becomes more and more pathological. Furthermore, mixture models often exhibit multimodalities beyond just those induced by the labeling degeneracy. Resolving the degeneracy does not eliminate these tangible modes and the computational issues they beget.</p>
<p>As with so many modeling techniques, mixture models can be powerful in practice provided that they are employed with great care.</p>
</div>
<div id="acknowledgements" class="section level1">
<h1>Acknowledgements</h1>
<p>I thank Bob Carpenter and Aki Vehtari for careful readings of earlier drafts of this case study and many helpful comments.</p>
</div>
<div id="original-computing-environment" class="section level1">
<h1>Original Computing Environment</h1>
<pre class="r"><code>writeLines(readLines(file.path(Sys.getenv(&quot;HOME&quot;), &quot;.R/Makevars&quot;)))</code></pre>
<pre><code>#CFLAGS =              -O3 -Wall -pipe -pedantic -std=gnu99 -march=native 
#CXXFLAGS =            -O3 -Wall -pipe -Wno-unused -pedantic -march=native </code></pre>
<pre class="r"><code>devtools::session_info(&quot;rstan&quot;)</code></pre>
<pre><code>Session info --------------------------------------------------------------</code></pre>
<pre><code> setting  value                       
 version  R version 3.3.3 (2017-03-06)
 system   x86_64, linux-gnu           
 ui       X11                         
 language (EN)                        
 collate  en_US.UTF-8                 
 tz       America/Los_Angeles         
 date     2017-06-27                  </code></pre>
<pre><code>Packages ------------------------------------------------------------------</code></pre>
<pre><code> package      * version   date       source        
 assertthat     0.1       2013-12-06 CRAN (R 3.3.0)
 BH             1.62.0-1  2016-11-19 CRAN (R 3.3.3)
 colorspace     1.2-6     2015-03-11 CRAN (R 3.3.0)
 dichromat      2.0-0     2013-01-24 CRAN (R 3.3.0)
 digest         0.6.11    2017-01-03 CRAN (R 3.3.2)
 ggplot2      * 2.2.1     2016-12-30 CRAN (R 3.3.2)
 gridExtra      2.2.1     2016-02-29 CRAN (R 3.3.0)
 gtable         0.2.0     2016-02-26 CRAN (R 3.3.0)
 inline         0.3.14    2015-04-13 CRAN (R 3.3.3)
 labeling       0.3       2014-08-23 CRAN (R 3.3.0)
 lattice        0.20-34   2016-09-06 CRAN (R 3.3.1)
 lazyeval       0.2.0     2016-06-12 CRAN (R 3.3.0)
 magrittr       1.5       2014-11-22 CRAN (R 3.3.0)
 MASS           7.3-45    2015-11-10 CRAN (R 3.2.5)
 Matrix         1.2-8     2017-01-20 CRAN (R 3.3.2)
 munsell        0.4.3     2016-02-13 CRAN (R 3.3.0)
 plyr           1.8.4     2016-06-08 CRAN (R 3.3.0)
 RColorBrewer   1.1-2     2014-12-07 CRAN (R 3.3.0)
 Rcpp           0.12.11   2017-05-22 CRAN (R 3.3.3)
 RcppEigen      0.3.3.3.0 2017-05-01 CRAN (R 3.3.3)
 reshape2       1.4.1     2014-12-06 CRAN (R 3.3.0)
 rlang          0.1.1     2017-05-18 CRAN (R 3.3.3)
 rstan        * 2.15.1    2017-04-19 CRAN (R 3.3.3)
 scales         0.4.1     2016-11-09 CRAN (R 3.3.2)
 StanHeaders  * 2.15.0-1  2017-04-19 CRAN (R 3.3.3)
 stringi        1.1.1     2016-05-27 CRAN (R 3.3.0)
 stringr        1.1.0     2016-08-19 CRAN (R 3.3.0)
 tibble         1.3.1     2017-05-17 CRAN (R 3.3.3)</code></pre>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
